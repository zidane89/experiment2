{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0 \n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self.N += 1 \n",
    "        power = sample[0][0]\n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2\n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N)\n",
    "        \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    next_power_states = np.array([0 if val[3] is None else (val[3][0] - memory.power_mean) / memory.power_std  \n",
    "                        for val in batch])\n",
    "    \n",
    "    states[:, 0] = (states[:, 0] - memory.power_mean) / memory.power_std \n",
    "    next_states[:, 0] = next_power_states\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 3\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -1062.6645356103486 Explore P: 0.9217 SOC: 0.8224 Cumulative_SOC_deviation: 99.9983 Fuel Consumption: 62.6818\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -980.5046587964488 Explore P: 0.8970 SOC: 0.8015 Cumulative_SOC_deviation: 91.9617 Fuel Consumption: 60.8878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -1151.9033397297962 Explore P: 0.8730 SOC: 0.8362 Cumulative_SOC_deviation: 108.8115 Fuel Consumption: 63.7881\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -1029.7290343150316 Explore P: 0.8496 SOC: 0.8227 Cumulative_SOC_deviation: 96.7137 Fuel Consumption: 62.5921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -865.1668391631677 Explore P: 0.8269 SOC: 0.7847 Cumulative_SOC_deviation: 80.5827 Fuel Consumption: 59.3401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -942.3829822302937 Explore P: 0.8048 SOC: 0.8033 Cumulative_SOC_deviation: 88.1689 Fuel Consumption: 60.6942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -1014.7650098590743 Explore P: 0.7832 SOC: 0.8071 Cumulative_SOC_deviation: 95.3669 Fuel Consumption: 61.0961\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -1091.5828028432811 Explore P: 0.7623 SOC: 0.8279 Cumulative_SOC_deviation: 102.8749 Fuel Consumption: 62.8333\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -935.5398551639245 Explore P: 0.7419 SOC: 0.7923 Cumulative_SOC_deviation: 87.5609 Fuel Consumption: 59.9305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -1023.4729563130551 Explore P: 0.7221 SOC: 0.8038 Cumulative_SOC_deviation: 96.2657 Fuel Consumption: 60.8158\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -991.6312763919428 Explore P: 0.7028 SOC: 0.7987 Cumulative_SOC_deviation: 93.1077 Fuel Consumption: 60.5538\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -1088.5076482803834 Explore P: 0.6840 SOC: 0.8282 Cumulative_SOC_deviation: 102.5460 Fuel Consumption: 63.0472\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -1017.9510233422745 Explore P: 0.6658 SOC: 0.8151 Cumulative_SOC_deviation: 95.6120 Fuel Consumption: 61.8314\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -874.7797341922309 Explore P: 0.6480 SOC: 0.7811 Cumulative_SOC_deviation: 81.5858 Fuel Consumption: 58.9220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -924.6863372085382 Explore P: 0.6307 SOC: 0.8035 Cumulative_SOC_deviation: 86.3967 Fuel Consumption: 60.7195\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -1204.3398522020436 Explore P: 0.6139 SOC: 0.8815 Cumulative_SOC_deviation: 113.6790 Fuel Consumption: 67.5499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -1112.3716468513542 Explore P: 0.5976 SOC: 0.8650 Cumulative_SOC_deviation: 104.6210 Fuel Consumption: 66.1619\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -1319.1630155943787 Explore P: 0.5816 SOC: 0.9010 Cumulative_SOC_deviation: 124.9755 Fuel Consumption: 69.4083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -1312.7488785246917 Explore P: 0.5662 SOC: 0.8956 Cumulative_SOC_deviation: 124.3902 Fuel Consumption: 68.8473\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -1400.8350454295469 Explore P: 0.5511 SOC: 0.9168 Cumulative_SOC_deviation: 133.0042 Fuel Consumption: 70.7933\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -1285.3399121121647 Explore P: 0.5364 SOC: 0.8966 Cumulative_SOC_deviation: 121.6254 Fuel Consumption: 69.0860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -693.3639135939303 Explore P: 0.5222 SOC: 0.7485 Cumulative_SOC_deviation: 63.6572 Fuel Consumption: 56.7920\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -572.8115963879449 Explore P: 0.5083 SOC: 0.7125 Cumulative_SOC_deviation: 51.8792 Fuel Consumption: 54.0194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -569.3009386939739 Explore P: 0.4948 SOC: 0.7083 Cumulative_SOC_deviation: 51.5619 Fuel Consumption: 53.6818\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -567.8676307006934 Explore P: 0.4817 SOC: 0.7123 Cumulative_SOC_deviation: 51.4030 Fuel Consumption: 53.8379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -542.9221789736945 Explore P: 0.4689 SOC: 0.6987 Cumulative_SOC_deviation: 49.0066 Fuel Consumption: 52.8558\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -995.8284601709167 Explore P: 0.4565 SOC: 0.8270 Cumulative_SOC_deviation: 93.2460 Fuel Consumption: 63.3680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -1314.8179228855925 Explore P: 0.4444 SOC: 0.9058 Cumulative_SOC_deviation: 124.4969 Fuel Consumption: 69.8487\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -1343.1657036067302 Explore P: 0.4326 SOC: 0.8857 Cumulative_SOC_deviation: 127.5157 Fuel Consumption: 68.0086\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -1482.514181364393 Explore P: 0.4212 SOC: 0.9426 Cumulative_SOC_deviation: 140.9693 Fuel Consumption: 72.8210\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -994.8573820543552 Explore P: 0.4100 SOC: 0.8479 Cumulative_SOC_deviation: 93.0004 Fuel Consumption: 64.8538\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -1270.0990879664723 Explore P: 0.3992 SOC: 0.8938 Cumulative_SOC_deviation: 120.1672 Fuel Consumption: 68.4272\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -1065.844453832772 Explore P: 0.3887 SOC: 0.8487 Cumulative_SOC_deviation: 100.1167 Fuel Consumption: 64.6770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -922.5663134052013 Explore P: 0.3784 SOC: 0.8086 Cumulative_SOC_deviation: 86.1249 Fuel Consumption: 61.3170\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -1067.8515626324331 Explore P: 0.3684 SOC: 0.8568 Cumulative_SOC_deviation: 100.2615 Fuel Consumption: 65.2365\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -793.3951766805152 Explore P: 0.3587 SOC: 0.7931 Cumulative_SOC_deviation: 73.3340 Fuel Consumption: 60.0556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -1033.9379425820478 Explore P: 0.3493 SOC: 0.8488 Cumulative_SOC_deviation: 96.9348 Fuel Consumption: 64.5898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -841.30887161898 Explore P: 0.3401 SOC: 0.8035 Cumulative_SOC_deviation: 78.0401 Fuel Consumption: 60.9080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -548.3934269998673 Explore P: 0.3311 SOC: 0.6838 Cumulative_SOC_deviation: 49.7121 Fuel Consumption: 51.2726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -566.1602329946952 Explore P: 0.3224 SOC: 0.6291 Cumulative_SOC_deviation: 51.8740 Fuel Consumption: 47.4200\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -582.0207519526581 Explore P: 0.3140 SOC: 0.6999 Cumulative_SOC_deviation: 52.9404 Fuel Consumption: 52.6166\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -616.9645215743582 Explore P: 0.3057 SOC: 0.7232 Cumulative_SOC_deviation: 56.2922 Fuel Consumption: 54.0427\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -462.59967232696033 Explore P: 0.2977 SOC: 0.6727 Cumulative_SOC_deviation: 41.2735 Fuel Consumption: 49.8648\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -620.3617068490959 Explore P: 0.2899 SOC: 0.7029 Cumulative_SOC_deviation: 56.8394 Fuel Consumption: 51.9679\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -701.0578802341653 Explore P: 0.2824 SOC: 0.7254 Cumulative_SOC_deviation: 64.7052 Fuel Consumption: 54.0062\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -714.2974657206951 Explore P: 0.2750 SOC: 0.7252 Cumulative_SOC_deviation: 66.0149 Fuel Consumption: 54.1486\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -699.2799103237907 Explore P: 0.2678 SOC: 0.7335 Cumulative_SOC_deviation: 64.4401 Fuel Consumption: 54.8794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -1228.8197012056653 Explore P: 0.2608 SOC: 0.8995 Cumulative_SOC_deviation: 115.9997 Fuel Consumption: 68.8230\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -818.8596119700317 Explore P: 0.2540 SOC: 0.7692 Cumulative_SOC_deviation: 76.0769 Fuel Consumption: 58.0908\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -986.1790542123113 Explore P: 0.2474 SOC: 0.7980 Cumulative_SOC_deviation: 92.5706 Fuel Consumption: 60.4732\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -1221.449435991637 Explore P: 0.2410 SOC: 0.8390 Cumulative_SOC_deviation: 115.7446 Fuel Consumption: 64.0030\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -1011.529080104011 Explore P: 0.2347 SOC: 0.8046 Cumulative_SOC_deviation: 95.0403 Fuel Consumption: 61.1265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -681.0825897436753 Explore P: 0.2286 SOC: 0.7282 Cumulative_SOC_deviation: 62.6009 Fuel Consumption: 55.0740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -664.2293875521311 Explore P: 0.2227 SOC: 0.7455 Cumulative_SOC_deviation: 60.7933 Fuel Consumption: 56.2963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -640.2745858218944 Explore P: 0.2170 SOC: 0.7089 Cumulative_SOC_deviation: 58.6811 Fuel Consumption: 53.4639\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -649.4895120145941 Explore P: 0.2114 SOC: 0.7323 Cumulative_SOC_deviation: 59.3998 Fuel Consumption: 55.4916\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -711.0497071143027 Explore P: 0.2059 SOC: 0.7424 Cumulative_SOC_deviation: 65.4694 Fuel Consumption: 56.3556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -943.4585748575996 Explore P: 0.2006 SOC: 0.7859 Cumulative_SOC_deviation: 88.3521 Fuel Consumption: 59.9376\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -652.4617071976124 Explore P: 0.1954 SOC: 0.7354 Cumulative_SOC_deviation: 59.6854 Fuel Consumption: 55.6081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -668.4376495690498 Explore P: 0.1904 SOC: 0.7353 Cumulative_SOC_deviation: 61.2867 Fuel Consumption: 55.5711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -842.9171762652227 Explore P: 0.1855 SOC: 0.7459 Cumulative_SOC_deviation: 78.6500 Fuel Consumption: 56.4175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -687.6763280188509 Explore P: 0.1808 SOC: 0.7341 Cumulative_SOC_deviation: 63.2200 Fuel Consumption: 55.4759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -755.6792854939295 Explore P: 0.1761 SOC: 0.7491 Cumulative_SOC_deviation: 69.9222 Fuel Consumption: 56.4575\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -601.0367512235038 Explore P: 0.1716 SOC: 0.7102 Cumulative_SOC_deviation: 54.7456 Fuel Consumption: 53.5805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -540.3895119645307 Explore P: 0.1673 SOC: 0.6957 Cumulative_SOC_deviation: 48.8009 Fuel Consumption: 52.3809\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -624.2955165409124 Explore P: 0.1630 SOC: 0.7115 Cumulative_SOC_deviation: 57.0656 Fuel Consumption: 53.6393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -541.4462906878591 Explore P: 0.1589 SOC: 0.6832 Cumulative_SOC_deviation: 48.9992 Fuel Consumption: 51.4540\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -594.5168997013657 Explore P: 0.1548 SOC: 0.6946 Cumulative_SOC_deviation: 54.2173 Fuel Consumption: 52.3434\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -804.891257050736 Explore P: 0.1509 SOC: 0.7440 Cumulative_SOC_deviation: 74.8448 Fuel Consumption: 56.4433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -695.7393739607165 Explore P: 0.1471 SOC: 0.7258 Cumulative_SOC_deviation: 64.0811 Fuel Consumption: 54.9285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -811.4276776713062 Explore P: 0.1434 SOC: 0.7432 Cumulative_SOC_deviation: 75.4989 Fuel Consumption: 56.4382\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -561.8726332789344 Explore P: 0.1398 SOC: 0.7033 Cumulative_SOC_deviation: 50.9106 Fuel Consumption: 52.7666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -573.0874865662802 Explore P: 0.1362 SOC: 0.7070 Cumulative_SOC_deviation: 51.9819 Fuel Consumption: 53.2683\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -766.544160261168 Explore P: 0.1328 SOC: 0.7476 Cumulative_SOC_deviation: 71.0304 Fuel Consumption: 56.2401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -579.5945598936959 Explore P: 0.1295 SOC: 0.7141 Cumulative_SOC_deviation: 52.5867 Fuel Consumption: 53.7275\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -893.2914730241516 Explore P: 0.1263 SOC: 0.7830 Cumulative_SOC_deviation: 83.4219 Fuel Consumption: 59.0720\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -762.5134102255599 Explore P: 0.1231 SOC: 0.7533 Cumulative_SOC_deviation: 70.5673 Fuel Consumption: 56.8406\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -803.0104139962448 Explore P: 0.1200 SOC: 0.7528 Cumulative_SOC_deviation: 74.6431 Fuel Consumption: 56.5791\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -981.149592905612 Explore P: 0.1171 SOC: 0.7946 Cumulative_SOC_deviation: 92.0928 Fuel Consumption: 60.2214\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -1251.7418911942923 Explore P: 0.1142 SOC: 0.8382 Cumulative_SOC_deviation: 118.8019 Fuel Consumption: 63.7227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -1174.5648122122916 Explore P: 0.1113 SOC: 0.8190 Cumulative_SOC_deviation: 111.2338 Fuel Consumption: 62.2267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -1212.3311000217502 Explore P: 0.1086 SOC: 0.8139 Cumulative_SOC_deviation: 115.0754 Fuel Consumption: 61.5770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -1193.7316995758247 Explore P: 0.1059 SOC: 0.7980 Cumulative_SOC_deviation: 113.3488 Fuel Consumption: 60.2437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -1456.822221902013 Explore P: 0.1033 SOC: 0.8573 Cumulative_SOC_deviation: 139.2042 Fuel Consumption: 64.7799\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -724.0117058848194 Explore P: 0.1008 SOC: 0.7331 Cumulative_SOC_deviation: 66.8719 Fuel Consumption: 55.2929\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -700.6178846472349 Explore P: 0.0983 SOC: 0.7204 Cumulative_SOC_deviation: 64.6781 Fuel Consumption: 53.8364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -819.9338616543862 Explore P: 0.0960 SOC: 0.7777 Cumulative_SOC_deviation: 76.1554 Fuel Consumption: 58.3802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -786.0728440673234 Explore P: 0.0936 SOC: 0.7429 Cumulative_SOC_deviation: 73.0789 Fuel Consumption: 55.2843\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -697.5365460971398 Explore P: 0.0914 SOC: 0.7188 Cumulative_SOC_deviation: 64.4260 Fuel Consumption: 53.2764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -963.5369039847587 Explore P: 0.0892 SOC: 0.7710 Cumulative_SOC_deviation: 90.5598 Fuel Consumption: 57.9388\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -1143.8458773624457 Explore P: 0.0870 SOC: 0.7871 Cumulative_SOC_deviation: 108.4545 Fuel Consumption: 59.3010\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -1128.2554468933529 Explore P: 0.0849 SOC: 0.7851 Cumulative_SOC_deviation: 106.9067 Fuel Consumption: 59.1880\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -1100.4127881776978 Explore P: 0.0829 SOC: 0.7774 Cumulative_SOC_deviation: 104.2107 Fuel Consumption: 58.3062\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -850.6418864688227 Explore P: 0.0809 SOC: 0.7612 Cumulative_SOC_deviation: 79.3583 Fuel Consumption: 57.0585\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -759.0578322864491 Explore P: 0.0790 SOC: 0.7430 Cumulative_SOC_deviation: 70.3381 Fuel Consumption: 55.6765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -989.106172227547 Explore P: 0.0771 SOC: 0.7853 Cumulative_SOC_deviation: 92.9982 Fuel Consumption: 59.1242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -990.1585355050947 Explore P: 0.0753 SOC: 0.7825 Cumulative_SOC_deviation: 93.0973 Fuel Consumption: 59.1855\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -624.0477012218979 Explore P: 0.0735 SOC: 0.6927 Cumulative_SOC_deviation: 57.1909 Fuel Consumption: 52.1392\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -780.328199967982 Explore P: 0.0718 SOC: 0.7430 Cumulative_SOC_deviation: 72.4570 Fuel Consumption: 55.7586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -691.8118035221073 Explore P: 0.0701 SOC: 0.7217 Cumulative_SOC_deviation: 63.7679 Fuel Consumption: 54.1324\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -516.7850747338978 Explore P: 0.0685 SOC: 0.6810 Cumulative_SOC_deviation: 46.5970 Fuel Consumption: 50.8155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -660.6950649412432 Explore P: 0.0669 SOC: 0.6894 Cumulative_SOC_deviation: 60.8764 Fuel Consumption: 51.9314\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -908.8360527610826 Explore P: 0.0654 SOC: 0.7507 Cumulative_SOC_deviation: 85.2155 Fuel Consumption: 56.6810\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -740.122018688242 Explore P: 0.0639 SOC: 0.7365 Cumulative_SOC_deviation: 68.4810 Fuel Consumption: 55.3117\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -1050.5029877043646 Explore P: 0.0624 SOC: 0.7674 Cumulative_SOC_deviation: 99.2234 Fuel Consumption: 58.2687\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -1093.8162336404916 Explore P: 0.0610 SOC: 0.7667 Cumulative_SOC_deviation: 103.5549 Fuel Consumption: 58.2677\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -1176.0719444258539 Explore P: 0.0596 SOC: 0.7799 Cumulative_SOC_deviation: 111.6770 Fuel Consumption: 59.3021\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -1393.7741879127252 Explore P: 0.0583 SOC: 0.8040 Cumulative_SOC_deviation: 133.2992 Fuel Consumption: 60.7819\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -1151.9531965501992 Explore P: 0.0570 SOC: 0.7967 Cumulative_SOC_deviation: 109.1807 Fuel Consumption: 60.1458\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -993.9596447346745 Explore P: 0.0557 SOC: 0.7759 Cumulative_SOC_deviation: 93.5763 Fuel Consumption: 58.1963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -931.5332382026727 Explore P: 0.0545 SOC: 0.7572 Cumulative_SOC_deviation: 87.4609 Fuel Consumption: 56.9242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -546.2520134625513 Explore P: 0.0533 SOC: 0.6917 Cumulative_SOC_deviation: 49.4334 Fuel Consumption: 51.9182\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -778.4864557056478 Explore P: 0.0521 SOC: 0.7307 Cumulative_SOC_deviation: 72.3259 Fuel Consumption: 55.2275\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -721.8480188234931 Explore P: 0.0510 SOC: 0.7250 Cumulative_SOC_deviation: 66.6994 Fuel Consumption: 54.8535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -795.7144821496255 Explore P: 0.0498 SOC: 0.7307 Cumulative_SOC_deviation: 74.0495 Fuel Consumption: 55.2194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -605.2375271373318 Explore P: 0.0488 SOC: 0.6977 Cumulative_SOC_deviation: 55.2504 Fuel Consumption: 52.7337\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -996.8493042099503 Explore P: 0.0477 SOC: 0.7711 Cumulative_SOC_deviation: 93.8244 Fuel Consumption: 58.6052\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -681.7803856713832 Explore P: 0.0467 SOC: 0.6903 Cumulative_SOC_deviation: 62.9570 Fuel Consumption: 52.2107\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -651.5734914751406 Explore P: 0.0457 SOC: 0.6491 Cumulative_SOC_deviation: 60.2400 Fuel Consumption: 49.1730\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -615.013354333709 Explore P: 0.0447 SOC: 0.6367 Cumulative_SOC_deviation: 56.6757 Fuel Consumption: 48.2562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -659.2096678420082 Explore P: 0.0438 SOC: 0.6328 Cumulative_SOC_deviation: 61.1368 Fuel Consumption: 47.8412\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -713.718465256925 Explore P: 0.0429 SOC: 0.6333 Cumulative_SOC_deviation: 66.5864 Fuel Consumption: 47.8549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -714.6554374809151 Explore P: 0.0420 SOC: 0.6341 Cumulative_SOC_deviation: 66.6591 Fuel Consumption: 48.0647\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -615.9211967120483 Explore P: 0.0411 SOC: 0.6425 Cumulative_SOC_deviation: 56.7077 Fuel Consumption: 48.8446\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -706.4634177452704 Explore P: 0.0403 SOC: 0.6568 Cumulative_SOC_deviation: 65.6533 Fuel Consumption: 49.9301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -615.6341257177394 Explore P: 0.0395 SOC: 0.6715 Cumulative_SOC_deviation: 56.4640 Fuel Consumption: 50.9944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -785.4035173902295 Explore P: 0.0387 SOC: 0.6603 Cumulative_SOC_deviation: 73.5569 Fuel Consumption: 49.8344\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -674.9465437770423 Explore P: 0.0379 SOC: 0.6693 Cumulative_SOC_deviation: 62.4094 Fuel Consumption: 50.8530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -722.9729311654354 Explore P: 0.0371 SOC: 0.6650 Cumulative_SOC_deviation: 67.2619 Fuel Consumption: 50.3543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -646.062170224386 Explore P: 0.0364 SOC: 0.6656 Cumulative_SOC_deviation: 59.5856 Fuel Consumption: 50.2058\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -710.9901254031261 Explore P: 0.0357 SOC: 0.6634 Cumulative_SOC_deviation: 66.0705 Fuel Consumption: 50.2854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -664.4499306544793 Explore P: 0.0350 SOC: 0.6838 Cumulative_SOC_deviation: 61.2460 Fuel Consumption: 51.9902\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -782.1519468115274 Explore P: 0.0343 SOC: 0.7344 Cumulative_SOC_deviation: 72.6392 Fuel Consumption: 55.7602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -628.1683251934387 Explore P: 0.0336 SOC: 0.6966 Cumulative_SOC_deviation: 57.5603 Fuel Consumption: 52.5649\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -589.3733827273887 Explore P: 0.0330 SOC: 0.6749 Cumulative_SOC_deviation: 53.8189 Fuel Consumption: 51.1844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -620.5998740926128 Explore P: 0.0324 SOC: 0.6902 Cumulative_SOC_deviation: 56.8267 Fuel Consumption: 52.3328\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -679.7005545964722 Explore P: 0.0318 SOC: 0.6562 Cumulative_SOC_deviation: 62.9980 Fuel Consumption: 49.7203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -801.0541599144519 Explore P: 0.0312 SOC: 0.6156 Cumulative_SOC_deviation: 75.4400 Fuel Consumption: 46.6538\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -628.4528289296373 Explore P: 0.0306 SOC: 0.6228 Cumulative_SOC_deviation: 58.1598 Fuel Consumption: 46.8550\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -881.031549756008 Explore P: 0.0301 SOC: 0.6278 Cumulative_SOC_deviation: 83.3513 Fuel Consumption: 47.5189\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -807.450009908213 Explore P: 0.0295 SOC: 0.6646 Cumulative_SOC_deviation: 75.7184 Fuel Consumption: 50.2661\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -767.5689426046945 Explore P: 0.0290 SOC: 0.6352 Cumulative_SOC_deviation: 71.9568 Fuel Consumption: 48.0008\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -770.4059453260699 Explore P: 0.0285 SOC: 0.6927 Cumulative_SOC_deviation: 71.7918 Fuel Consumption: 52.4879\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -591.2675875142357 Explore P: 0.0280 SOC: 0.6885 Cumulative_SOC_deviation: 53.9488 Fuel Consumption: 51.7799\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -613.2883837801436 Explore P: 0.0275 SOC: 0.6695 Cumulative_SOC_deviation: 56.2908 Fuel Consumption: 50.3802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -619.2910786618793 Explore P: 0.0270 SOC: 0.6856 Cumulative_SOC_deviation: 56.7792 Fuel Consumption: 51.4991\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -464.17562956852635 Explore P: 0.0265 SOC: 0.6438 Cumulative_SOC_deviation: 41.5946 Fuel Consumption: 48.2299\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -532.3677087117188 Explore P: 0.0261 SOC: 0.6865 Cumulative_SOC_deviation: 48.0777 Fuel Consumption: 51.5909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -529.0422249051932 Explore P: 0.0257 SOC: 0.6676 Cumulative_SOC_deviation: 47.8676 Fuel Consumption: 50.3665\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -629.7062540158634 Explore P: 0.0252 SOC: 0.6883 Cumulative_SOC_deviation: 57.7866 Fuel Consumption: 51.8402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -608.5347745953304 Explore P: 0.0248 SOC: 0.7184 Cumulative_SOC_deviation: 55.4792 Fuel Consumption: 53.7432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -596.5737204364257 Explore P: 0.0244 SOC: 0.6995 Cumulative_SOC_deviation: 54.4219 Fuel Consumption: 52.3546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -514.6875412548285 Explore P: 0.0240 SOC: 0.6619 Cumulative_SOC_deviation: 46.5127 Fuel Consumption: 49.5607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -608.3841883950888 Explore P: 0.0237 SOC: 0.6784 Cumulative_SOC_deviation: 55.7720 Fuel Consumption: 50.6640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -524.3868739673119 Explore P: 0.0233 SOC: 0.6758 Cumulative_SOC_deviation: 47.3626 Fuel Consumption: 50.7613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -683.431826235606 Explore P: 0.0229 SOC: 0.6510 Cumulative_SOC_deviation: 63.4442 Fuel Consumption: 48.9896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -684.1027877727934 Explore P: 0.0226 SOC: 0.6401 Cumulative_SOC_deviation: 63.5687 Fuel Consumption: 48.4159\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -666.5968857667506 Explore P: 0.0222 SOC: 0.6514 Cumulative_SOC_deviation: 61.7280 Fuel Consumption: 49.3169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -780.4377570825819 Explore P: 0.0219 SOC: 0.6466 Cumulative_SOC_deviation: 73.1522 Fuel Consumption: 48.9156\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -907.3102699296056 Explore P: 0.0216 SOC: 0.6280 Cumulative_SOC_deviation: 85.9667 Fuel Consumption: 47.6430\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -1250.2637441749064 Explore P: 0.0213 SOC: 0.5754 Cumulative_SOC_deviation: 120.6398 Fuel Consumption: 43.8655\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -1589.2804824437153 Explore P: 0.0210 SOC: 0.5471 Cumulative_SOC_deviation: 154.7837 Fuel Consumption: 41.4436\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -1020.335384561308 Explore P: 0.0207 SOC: 0.6240 Cumulative_SOC_deviation: 97.3335 Fuel Consumption: 47.0004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -643.0765658972991 Explore P: 0.0204 SOC: 0.6710 Cumulative_SOC_deviation: 59.2987 Fuel Consumption: 50.0898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -761.2555338976665 Explore P: 0.0201 SOC: 0.6444 Cumulative_SOC_deviation: 71.2989 Fuel Consumption: 48.2669\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -842.6455895924967 Explore P: 0.0198 SOC: 0.6558 Cumulative_SOC_deviation: 79.3532 Fuel Consumption: 49.1137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -613.1745323084788 Explore P: 0.0196 SOC: 0.6780 Cumulative_SOC_deviation: 56.2380 Fuel Consumption: 50.7947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -661.9383603787306 Explore P: 0.0193 SOC: 0.6840 Cumulative_SOC_deviation: 61.0759 Fuel Consumption: 51.1794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -581.0630259999951 Explore P: 0.0190 SOC: 0.6739 Cumulative_SOC_deviation: 53.1046 Fuel Consumption: 50.0173\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -481.13132331948003 Explore P: 0.0188 SOC: 0.6657 Cumulative_SOC_deviation: 43.1559 Fuel Consumption: 49.5718\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -449.857037452757 Explore P: 0.0186 SOC: 0.6643 Cumulative_SOC_deviation: 40.0316 Fuel Consumption: 49.5409\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -602.9504596495127 Explore P: 0.0183 SOC: 0.6931 Cumulative_SOC_deviation: 55.1144 Fuel Consumption: 51.8068\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -497.3816671911691 Explore P: 0.0181 SOC: 0.6781 Cumulative_SOC_deviation: 44.6900 Fuel Consumption: 50.4820\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -657.0439344418586 Explore P: 0.0179 SOC: 0.6961 Cumulative_SOC_deviation: 60.5236 Fuel Consumption: 51.8083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -611.3973765417713 Explore P: 0.0177 SOC: 0.7010 Cumulative_SOC_deviation: 55.9156 Fuel Consumption: 52.2416\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -922.4502847089938 Explore P: 0.0175 SOC: 0.7639 Cumulative_SOC_deviation: 86.5277 Fuel Consumption: 57.1736\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -1005.7696294575646 Explore P: 0.0173 SOC: 0.7848 Cumulative_SOC_deviation: 94.7013 Fuel Consumption: 58.7562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -913.5348222719251 Explore P: 0.0171 SOC: 0.7593 Cumulative_SOC_deviation: 85.6848 Fuel Consumption: 56.6871\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -996.029348581231 Explore P: 0.0169 SOC: 0.7803 Cumulative_SOC_deviation: 93.7694 Fuel Consumption: 58.3351\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -1197.8889824548812 Explore P: 0.0167 SOC: 0.7967 Cumulative_SOC_deviation: 113.8242 Fuel Consumption: 59.6472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -960.0907923698337 Explore P: 0.0165 SOC: 0.7918 Cumulative_SOC_deviation: 90.0507 Fuel Consumption: 59.5833\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -790.9428884259227 Explore P: 0.0163 SOC: 0.7284 Cumulative_SOC_deviation: 73.6269 Fuel Consumption: 54.6741\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -633.2513399946346 Explore P: 0.0162 SOC: 0.6667 Cumulative_SOC_deviation: 58.3208 Fuel Consumption: 50.0432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -499.85854642012913 Explore P: 0.0160 SOC: 0.6546 Cumulative_SOC_deviation: 45.1134 Fuel Consumption: 48.7250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -567.0514977979618 Explore P: 0.0158 SOC: 0.6722 Cumulative_SOC_deviation: 51.6852 Fuel Consumption: 50.1997\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -532.4778801675373 Explore P: 0.0157 SOC: 0.6596 Cumulative_SOC_deviation: 48.3371 Fuel Consumption: 49.1066\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -439.20316965898184 Explore P: 0.0155 SOC: 0.6444 Cumulative_SOC_deviation: 39.1279 Fuel Consumption: 47.9238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -400.0783274828512 Explore P: 0.0154 SOC: 0.6490 Cumulative_SOC_deviation: 35.1773 Fuel Consumption: 48.3054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -418.4986585180978 Explore P: 0.0152 SOC: 0.6493 Cumulative_SOC_deviation: 37.0107 Fuel Consumption: 48.3921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -355.9832553105699 Explore P: 0.0151 SOC: 0.6335 Cumulative_SOC_deviation: 30.8794 Fuel Consumption: 47.1895\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -351.9258105176196 Explore P: 0.0149 SOC: 0.6400 Cumulative_SOC_deviation: 30.4140 Fuel Consumption: 47.7859\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -672.8461181335277 Explore P: 0.0148 SOC: 0.6660 Cumulative_SOC_deviation: 62.2550 Fuel Consumption: 50.2965\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -619.17125282722 Explore P: 0.0147 SOC: 0.6530 Cumulative_SOC_deviation: 56.9624 Fuel Consumption: 49.5475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -957.6035351288634 Explore P: 0.0146 SOC: 0.7579 Cumulative_SOC_deviation: 90.0131 Fuel Consumption: 57.4726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -966.048648761337 Explore P: 0.0144 SOC: 0.7843 Cumulative_SOC_deviation: 90.6490 Fuel Consumption: 59.5590\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -796.4707382435653 Explore P: 0.0143 SOC: 0.6811 Cumulative_SOC_deviation: 74.5226 Fuel Consumption: 51.2447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -682.7814560861666 Explore P: 0.0142 SOC: 0.6504 Cumulative_SOC_deviation: 63.3756 Fuel Consumption: 49.0255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -575.9387888723925 Explore P: 0.0141 SOC: 0.6363 Cumulative_SOC_deviation: 52.8171 Fuel Consumption: 47.7682\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4b3ac91aa511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2794\u001b[0m     return gca().plot(\n\u001b[0;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2796\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1665\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1900\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \"\"\"\n\u001b[1;32m-> 1924\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \"\"\"\n\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1388\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAJDCAYAAACPEUSwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZC0lEQVR4nO3dX6it513g8e+viVGotYI5A5I/JmA6NVOEOodMh15YaWdIepHcdCSBopXQczNRZixCRKkSr6wMBSH+yWCpFmyMvdCDRDKgFUVMySmdCU1K4BCd5hChsdbclDZm5pmLvadsd0+yV07W2qfd5/OBA+t917PW/t087J1v3netWWsFAAAAwJXtDZd7AAAAAAAuP5EIAAAAAJEIAAAAAJEIAAAAgEQiAAAAABKJAAAAAGiDSDQzH5uZL83M51/h+ZmZX5+Z8zPz5Mz8yPbHBAAAAGCXNrmS6OPV7a/y/B3VLfv/zlS/+frHAgAAAOA4HRmJ1lp/Wf3jqyy5q/q9tefx6ntn5vu3NSAAAAAAu7eNzyS6rnruwPGF/XMAAAAAfJu4egvvMRc5ty66cOZMe7ek9cY3vvHfvvWtb93CjwcAAACg6rOf/ew/rLVOXcprtxGJLlQ3HDi+vnr+YgvXWg9VD1WdPn16nTt3bgs/HgAAAICqmfnfl/rabdxudrb6if1vOXtH9eJa6++38L4AAAAAHJMjrySamU9W76qunZkL1S9V31G11vqt6tHqvdX56qvVT+1qWAAAAAB248hItNa654jnV/WftzYRAAAAAMduG7ebAQAAAPBtTiQCAAAAQCQCAAAAQCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAoA0j0czcPjPPzMz5mbn/Is/fODOfnpnPzcyTM/Pe7Y8KAAAAwK4cGYlm5qrqweqO6tbqnpm59dCyX6weWWu9vbq7+o1tDwoAAADA7mxyJdFt1fm11rNrrZeqh6u7Dq1Z1ffsP35z9fz2RgQAAABg167eYM111XMHji9U/+7Qml+u/sfM/HT1xuo9W5kOAAAAgGOxyZVEc5Fz69DxPdXH11rXV++tPjEz3/TeM3NmZs7NzLkXXnjhtU8LAAAAwE5sEokuVDccOL6+b76d7N7qkaq11t9U31Vde/iN1loPrbVOr7VOnzp16tImBgAAAGDrNolET1S3zMzNM3NNex9MffbQmi9W766amR9qLxK5VAgAAADg28SRkWit9XJ1X/VY9YX2vsXsqZl5YGbu3F/2oeqDM/O/qk9WH1hrHb4lDQAAAIBvUZt8cHVrrUerRw+d+/CBx09X79zuaAAAAAAcl01uNwMAAADghBOJAAAAABCJAAAAABCJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAGjDSDQzt8/MMzNzfmbuf4U1Pz4zT8/MUzPz+9sdEwAAAIBduvqoBTNzVfVg9R+qC9UTM3N2rfX0gTW3VD9fvXOt9ZWZ+Ve7GhgAAACA7dvkSqLbqvNrrWfXWi9VD1d3HVrzwerBtdZXqtZaX9rumAAAAADs0iaR6LrquQPHF/bPHfSW6i0z89cz8/jM3L6tAQEAAADYvSNvN6vmIufWRd7nlupd1fXVX83M29Za//Qv3mjmTHWm6sYbb3zNwwIAAACwG5tcSXShuuHA8fXV8xdZ88drrX9ea/1t9Ux70ehfWGs9tNY6vdY6ferUqUudGQAAAIAt2yQSPVHdMjM3z8w11d3V2UNr/qj6saqZuba928+e3eagAAAAAOzOkZForfVydV/1WPWF6pG11lMz88DM3Lm/7LHqyzPzdPXp6ufWWl/e1dAAAAAAbNesdfjjhY7H6dOn17lz5y7LzwYAAAA4iWbms2ut05fy2k1uNwMAAADghBOJAAAAABCJAAAAABCJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAANowEs3M7TPzzMycn5n7X2Xd+2Zmzczp7Y0IAAAAwK4dGYlm5qrqweqO6tbqnpm59SLr3lT9TPWZbQ8JAAAAwG5tciXRbdX5tdaza62Xqoeruy6y7leqj1Rf2+J8AAAAAByDTSLRddVzB44v7J/7hpl5e3XDWutPtjgbAAAAAMdkk0g0Fzm3vvHkzBuqj1YfOvKNZs7MzLmZOffCCy9sPiUAAAAAO7VJJLpQ3XDg+Prq+QPHb6reVv3FzPxd9Y7q7MU+vHqt9dBa6/Ra6/SpU6cufWoAAAAAtmqTSPREdcvM3Dwz11R3V2f//5NrrRfXWteutW5aa91UPV7dudY6t5OJAQAAANi6IyPRWuvl6r7qseoL1SNrradm5oGZuXPXAwIAAACwe1dvsmit9Wj16KFzH36Fte96/WMBAAAAcJw2ud0MAAAAgBNOJAIAAABAJAIAAABAJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAACgDSPRzNw+M8/MzPmZuf8iz//szDw9M0/OzJ/NzA9sf1QAAAAAduXISDQzV1UPVndUt1b3zMyth5Z9rjq91vrh6lPVR7Y9KAAAAAC7s8mVRLdV59daz661Xqoeru46uGCt9em11lf3Dx+vrt/umAAAAADs0iaR6LrquQPHF/bPvZJ7qz99PUMBAAAAcLyu3mDNXOTcuujCmfdXp6sffYXnz1Rnqm688cYNRwQAAABg1za5kuhCdcOB4+ur5w8vmpn3VL9Q3bnW+vrF3mit9dBa6/Ra6/SpU6cuZV4AAAAAdmCTSPREdcvM3Dwz11R3V2cPLpiZt1e/3V4g+tL2xwQAAABgl46MRGutl6v7qseqL1SPrLWempkHZubO/WW/Vn139Ycz8z9n5uwrvB0AAAAA34I2+Uyi1lqPVo8eOvfhA4/fs+W5AAAAADhGm9xuBgAAAMAJJxIBAAAAIBIBAAAAIBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAA0IaRaGZun5lnZub8zNx/kee/c2b+YP/5z8zMTdseFAAAAIDdOTISzcxV1YPVHdWt1T0zc+uhZfdWX1lr/WD10epXtz0oAAAAALuzyZVEt1Xn11rPrrVeqh6u7jq05q7qd/cff6p698zM9sYEAAAAYJc2iUTXVc8dOL6wf+6ia9ZaL1cvVt+3jQEBAAAA2L2rN1hzsSuC1iWsaWbOVGf2D78+M5/f4OcD23Vt9Q+Xewi4Atl7cPnYf3B52HtwefzrS33hJpHoQnXDgePrq+dfYc2Fmbm6enP1j4ffaK31UPVQ1cycW2udvpShgUtn78HlYe/B5WP/weVh78HlMTPnLvW1m9xu9kR1y8zcPDPXVHdXZw+tOVv95P7j91V/vtb6piuJAAAAAPjWdOSVRGutl2fmvuqx6qrqY2utp2bmgercWuts9TvVJ2bmfHtXEN29y6EBAAAA2K5NbjdrrfVo9eihcx8+8Phr1X96jT/7ode4HtgOew8uD3sPLh/7Dy4Pew8uj0vee+OuMAAAAAA2+UwiAAAAAE64nUeimbl9Zp6ZmfMzc/9Fnv/OmfmD/ec/MzM37XomuBJssPd+dmaenpknZ+bPZuYHLseccNIctfcOrHvfzKyZ8a0vsAWb7L2Z+fH9331PzczvH/eMcFJt8HfnjTPz6Zn53P7fnu+9HHPCSTIzH5uZL83M51/h+ZmZX9/fl0/OzI9s8r47jUQzc1X1YHVHdWt1z8zcemjZvdVX1lo/WH20+tVdzgRXgg333ueq02utH64+VX3keKeEk2fDvdfMvKn6meozxzshnEyb7L2ZuaX6+eqda61/U/2XYx8UTqANf/f9YvXIWuvt7X3J0W8c75RwIn28uv1Vnr+jumX/35nqNzd5011fSXRbdX6t9exa66Xq4equQ2vuqn53//GnqnfPzOx4Ljjpjtx7a61Pr7W+un/4eHX9Mc8IJ9Emv/eqfqW9MPu14xwOTrBN9t4HqwfXWl+pWmt96ZhnhJNqk/23qu/Zf/zm6vljnA9OpLXWX7b37fKv5K7q99aex6vvnZnvP+p9dx2JrqueO3B8Yf/cRdestV6uXqy+b8dzwUm3yd476N7qT3c6EVwZjtx7M/P26oa11p8c52Bwwm3ye+8t1Vtm5q9n5vGZebX/+wpsbpP998vV+2fmQnvfmv3TxzMaXNFe638TVnX1zsbZc7Ergg5/ndoma4DXZuN9NTPvr05XP7rTieDK8Kp7b2be0N6t1R84roHgCrHJ772r27vk/l3tXT37VzPztrXWP+14NjjpNtl/91QfX2v9t5n599Un9vff/939eHDFuqTWsusriS5UNxw4vr5vvrTwG2tm5ur2Lj98tUumgKNtsveamfdUv1Ddudb6+jHNBifZUXvvTdXbqr+Ymb+r3lGd9eHV8Lpt+jfnH6+1/nmt9bfVM+1FI+D12WT/3Vs9UrXW+pvqu6prj2U6uHJt9N+Eh+06Ej1R3TIzN8/MNe19SNnZQ2vOVj+5//h91Z+vtVxJBK/PkXtv/5aX324vEPlcBtiOV917a60X11rXrrVuWmvd1N7ngd251jp3ecaFE2OTvzn/qPqxqpm5tr3bz5491inhZNpk/32xenfVzPxQe5HohWOdEq48Z6uf2P+Ws3dUL661/v6oF+30drO11sszc1/1WHVV9bG11lMz80B1bq11tvqd9i43PN/eFUR373ImuBJsuPd+rfru6g/3Pyv+i2utOy/b0HACbLj3gC3bcO89Vv3HmXm6+j/Vz621vnz5poaTYcP996Hqv8/Mf23vdpcPuDAAXp+Z+WR7t1Bfu/95X79UfUfVWuu32vv8r/dW56uvVj+10fvamwAAAADs+nYzAAAAAL4NiEQAAAAAiEQAAAAAiEQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAAFT/Dw9GvECmVdVfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"DDQN3_3.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(3 > 2) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
