{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -1046.9451366949152 Explore P: 0.9217 SOC: 0.8149 Cumulative_SOC_deviation: 98.4998 Fuel Consumption: 61.9469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -879.9504298457555 Explore P: 0.8970 SOC: 0.7843 Cumulative_SOC_deviation: 82.0401 Fuel Consumption: 59.5494\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -895.2696870857686 Explore P: 0.8730 SOC: 0.7735 Cumulative_SOC_deviation: 83.6604 Fuel Consumption: 58.6660\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -819.9255908876036 Explore P: 0.8496 SOC: 0.7503 Cumulative_SOC_deviation: 76.3109 Fuel Consumption: 56.8163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -779.2048743257744 Explore P: 0.8269 SOC: 0.7180 Cumulative_SOC_deviation: 72.5057 Fuel Consumption: 54.1476\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -755.9659464524658 Explore P: 0.8048 SOC: 0.7329 Cumulative_SOC_deviation: 70.0552 Fuel Consumption: 55.4140\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -702.4115641743355 Explore P: 0.7832 SOC: 0.7245 Cumulative_SOC_deviation: 64.7772 Fuel Consumption: 54.6392\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -849.1448358903859 Explore P: 0.7623 SOC: 0.7324 Cumulative_SOC_deviation: 79.3785 Fuel Consumption: 55.3603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -862.2669271938756 Explore P: 0.7419 SOC: 0.7491 Cumulative_SOC_deviation: 80.5535 Fuel Consumption: 56.7322\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -750.9325995236254 Explore P: 0.7221 SOC: 0.7334 Cumulative_SOC_deviation: 69.5582 Fuel Consumption: 55.3502\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -820.7145095551468 Explore P: 0.7028 SOC: 0.7047 Cumulative_SOC_deviation: 76.7672 Fuel Consumption: 53.0428\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -687.2861476319842 Explore P: 0.6840 SOC: 0.6947 Cumulative_SOC_deviation: 63.4840 Fuel Consumption: 52.4458\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -752.3318125268031 Explore P: 0.6658 SOC: 0.6850 Cumulative_SOC_deviation: 70.0749 Fuel Consumption: 51.5828\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -714.3107822905272 Explore P: 0.6480 SOC: 0.7013 Cumulative_SOC_deviation: 66.1422 Fuel Consumption: 52.8892\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -727.4527740645489 Explore P: 0.6307 SOC: 0.6617 Cumulative_SOC_deviation: 67.7800 Fuel Consumption: 49.6524\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -859.8791778565924 Explore P: 0.6139 SOC: 0.6449 Cumulative_SOC_deviation: 81.1550 Fuel Consumption: 48.3292\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -751.9794428715508 Explore P: 0.5976 SOC: 0.6663 Cumulative_SOC_deviation: 70.1966 Fuel Consumption: 50.0138\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -825.5276410795959 Explore P: 0.5816 SOC: 0.6508 Cumulative_SOC_deviation: 77.6520 Fuel Consumption: 49.0078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -675.577466232117 Explore P: 0.5662 SOC: 0.6662 Cumulative_SOC_deviation: 62.5154 Fuel Consumption: 50.4232\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -830.5764013169544 Explore P: 0.5511 SOC: 0.6554 Cumulative_SOC_deviation: 78.1209 Fuel Consumption: 49.3676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -743.5916085259296 Explore P: 0.5364 SOC: 0.7106 Cumulative_SOC_deviation: 68.9442 Fuel Consumption: 54.1496\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -712.7814879684133 Explore P: 0.5222 SOC: 0.6448 Cumulative_SOC_deviation: 66.4019 Fuel Consumption: 48.7625\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -738.8930044947246 Explore P: 0.5083 SOC: 0.6852 Cumulative_SOC_deviation: 68.7108 Fuel Consumption: 51.7850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -839.0402786385778 Explore P: 0.4948 SOC: 0.6690 Cumulative_SOC_deviation: 78.8295 Fuel Consumption: 50.7450\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -737.8809043990476 Explore P: 0.4817 SOC: 0.6356 Cumulative_SOC_deviation: 68.9667 Fuel Consumption: 48.2137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -1048.577079494953 Explore P: 0.4689 SOC: 0.6052 Cumulative_SOC_deviation: 100.3123 Fuel Consumption: 45.4542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -743.6228705825894 Explore P: 0.4565 SOC: 0.6092 Cumulative_SOC_deviation: 69.7506 Fuel Consumption: 46.1171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -995.6745470053224 Explore P: 0.4444 SOC: 0.5857 Cumulative_SOC_deviation: 95.1671 Fuel Consumption: 44.0038\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -1085.6253063423571 Explore P: 0.4326 SOC: 0.5772 Cumulative_SOC_deviation: 104.2167 Fuel Consumption: 43.4580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -1043.0457580929885 Explore P: 0.4212 SOC: 0.5865 Cumulative_SOC_deviation: 99.8715 Fuel Consumption: 44.3312\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -1014.4111459409776 Explore P: 0.4100 SOC: 0.5796 Cumulative_SOC_deviation: 97.0359 Fuel Consumption: 44.0520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -969.3583134823526 Explore P: 0.3992 SOC: 0.5847 Cumulative_SOC_deviation: 92.5191 Fuel Consumption: 44.1675\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -1051.676124264761 Explore P: 0.3887 SOC: 0.6391 Cumulative_SOC_deviation: 100.3288 Fuel Consumption: 48.3880\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -1108.7448358639788 Explore P: 0.3784 SOC: 0.6122 Cumulative_SOC_deviation: 106.2313 Fuel Consumption: 46.4323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -838.2156045690404 Explore P: 0.3684 SOC: 0.6014 Cumulative_SOC_deviation: 79.2641 Fuel Consumption: 45.5743\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -771.0007285027659 Explore P: 0.3587 SOC: 0.6586 Cumulative_SOC_deviation: 72.1328 Fuel Consumption: 49.6727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -726.9915822077853 Explore P: 0.3493 SOC: 0.7043 Cumulative_SOC_deviation: 67.3289 Fuel Consumption: 53.7026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -824.8123447419929 Explore P: 0.3401 SOC: 0.6473 Cumulative_SOC_deviation: 77.6117 Fuel Consumption: 48.6956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -609.2107624656998 Explore P: 0.3311 SOC: 0.6946 Cumulative_SOC_deviation: 55.6533 Fuel Consumption: 52.6779\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -661.1465562356891 Explore P: 0.3224 SOC: 0.6888 Cumulative_SOC_deviation: 60.9101 Fuel Consumption: 52.0460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -737.3709678733514 Explore P: 0.3140 SOC: 0.6918 Cumulative_SOC_deviation: 68.5207 Fuel Consumption: 52.1640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -889.8529685551639 Explore P: 0.3057 SOC: 0.7371 Cumulative_SOC_deviation: 83.4281 Fuel Consumption: 55.5721\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -643.6001427369201 Explore P: 0.2977 SOC: 0.7021 Cumulative_SOC_deviation: 59.0884 Fuel Consumption: 52.7164\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -529.6314392468519 Explore P: 0.2899 SOC: 0.6383 Cumulative_SOC_deviation: 48.1787 Fuel Consumption: 47.8447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -630.7643199283833 Explore P: 0.2824 SOC: 0.6068 Cumulative_SOC_deviation: 58.5439 Fuel Consumption: 45.3255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -666.6021678861409 Explore P: 0.2750 SOC: 0.6491 Cumulative_SOC_deviation: 61.8082 Fuel Consumption: 48.5203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -599.5454360310051 Explore P: 0.2678 SOC: 0.6097 Cumulative_SOC_deviation: 55.3882 Fuel Consumption: 45.6630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -630.8900408153189 Explore P: 0.2608 SOC: 0.6078 Cumulative_SOC_deviation: 58.5413 Fuel Consumption: 45.4770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -728.7904488176912 Explore P: 0.2540 SOC: 0.7145 Cumulative_SOC_deviation: 67.5299 Fuel Consumption: 53.4913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -928.8698913757223 Explore P: 0.2474 SOC: 0.7813 Cumulative_SOC_deviation: 86.9143 Fuel Consumption: 59.7272\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -731.5497457583257 Explore P: 0.2410 SOC: 0.6703 Cumulative_SOC_deviation: 68.1087 Fuel Consumption: 50.4623\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -690.7042399764032 Explore P: 0.2347 SOC: 0.7381 Cumulative_SOC_deviation: 63.4501 Fuel Consumption: 56.2031\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -769.488503425382 Explore P: 0.2286 SOC: 0.6858 Cumulative_SOC_deviation: 71.7697 Fuel Consumption: 51.7915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -652.3897778165889 Explore P: 0.2227 SOC: 0.6914 Cumulative_SOC_deviation: 60.0347 Fuel Consumption: 52.0429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -702.9987583005185 Explore P: 0.2170 SOC: 0.6572 Cumulative_SOC_deviation: 65.3466 Fuel Consumption: 49.5323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -696.100365276893 Explore P: 0.2114 SOC: 0.7254 Cumulative_SOC_deviation: 64.1046 Fuel Consumption: 55.0547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -700.3940299157698 Explore P: 0.2059 SOC: 0.6375 Cumulative_SOC_deviation: 65.2484 Fuel Consumption: 47.9096\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -655.5378268054599 Explore P: 0.2006 SOC: 0.6498 Cumulative_SOC_deviation: 60.6452 Fuel Consumption: 49.0858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -543.4434080067749 Explore P: 0.1954 SOC: 0.6731 Cumulative_SOC_deviation: 49.2733 Fuel Consumption: 50.7106\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -686.9133820781663 Explore P: 0.1904 SOC: 0.7772 Cumulative_SOC_deviation: 62.7855 Fuel Consumption: 59.0583\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -503.9382288505231 Explore P: 0.1855 SOC: 0.6427 Cumulative_SOC_deviation: 45.5595 Fuel Consumption: 48.3429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -589.1131018300163 Explore P: 0.1808 SOC: 0.6505 Cumulative_SOC_deviation: 54.0432 Fuel Consumption: 48.6814\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -673.1147525921267 Explore P: 0.1761 SOC: 0.6958 Cumulative_SOC_deviation: 62.0812 Fuel Consumption: 52.3024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -388.23515254116626 Explore P: 0.1716 SOC: 0.6480 Cumulative_SOC_deviation: 33.9694 Fuel Consumption: 48.5410\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -924.8782700768788 Explore P: 0.1673 SOC: 0.6960 Cumulative_SOC_deviation: 87.2408 Fuel Consumption: 52.4706\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -542.9312387057286 Explore P: 0.1630 SOC: 0.6609 Cumulative_SOC_deviation: 49.3034 Fuel Consumption: 49.8977\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -607.0682176645221 Explore P: 0.1589 SOC: 0.6325 Cumulative_SOC_deviation: 55.9707 Fuel Consumption: 47.3613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -704.4683208755441 Explore P: 0.1548 SOC: 0.7487 Cumulative_SOC_deviation: 64.8003 Fuel Consumption: 56.4656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -410.6654310490311 Explore P: 0.1509 SOC: 0.6105 Cumulative_SOC_deviation: 36.4912 Fuel Consumption: 45.7532\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -413.1854410366154 Explore P: 0.1471 SOC: 0.6367 Cumulative_SOC_deviation: 36.5647 Fuel Consumption: 47.5381\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -529.9159677569429 Explore P: 0.1434 SOC: 0.6579 Cumulative_SOC_deviation: 48.0792 Fuel Consumption: 49.1238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -504.06427046394185 Explore P: 0.1398 SOC: 0.6320 Cumulative_SOC_deviation: 45.6887 Fuel Consumption: 47.1773\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -531.7608379604468 Explore P: 0.1362 SOC: 0.6308 Cumulative_SOC_deviation: 48.4664 Fuel Consumption: 47.0972\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -487.9747738197759 Explore P: 0.1328 SOC: 0.6351 Cumulative_SOC_deviation: 44.0624 Fuel Consumption: 47.3511\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -514.1516309013178 Explore P: 0.1295 SOC: 0.6418 Cumulative_SOC_deviation: 46.6530 Fuel Consumption: 47.6217\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -448.1675417467351 Explore P: 0.1263 SOC: 0.6605 Cumulative_SOC_deviation: 39.8990 Fuel Consumption: 49.1771\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -424.59879989906113 Explore P: 0.1231 SOC: 0.6291 Cumulative_SOC_deviation: 37.7923 Fuel Consumption: 46.6756\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -450.6559927805067 Explore P: 0.1200 SOC: 0.6328 Cumulative_SOC_deviation: 40.3649 Fuel Consumption: 47.0070\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -496.23138678270834 Explore P: 0.1171 SOC: 0.6167 Cumulative_SOC_deviation: 45.0516 Fuel Consumption: 45.7152\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -450.6962029723591 Explore P: 0.1142 SOC: 0.6247 Cumulative_SOC_deviation: 40.4134 Fuel Consumption: 46.5621\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -559.6984320272046 Explore P: 0.1113 SOC: 0.6426 Cumulative_SOC_deviation: 51.1587 Fuel Consumption: 48.1118\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -514.6417434541352 Explore P: 0.1086 SOC: 0.6116 Cumulative_SOC_deviation: 46.9262 Fuel Consumption: 45.3797\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -548.1531120398881 Explore P: 0.1059 SOC: 0.6365 Cumulative_SOC_deviation: 50.0423 Fuel Consumption: 47.7302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -526.1277340200958 Explore P: 0.1033 SOC: 0.6833 Cumulative_SOC_deviation: 47.5049 Fuel Consumption: 51.0790\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -553.079894356808 Explore P: 0.1008 SOC: 0.7097 Cumulative_SOC_deviation: 50.0024 Fuel Consumption: 53.0555\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -487.710849213338 Explore P: 0.0983 SOC: 0.6381 Cumulative_SOC_deviation: 44.0208 Fuel Consumption: 47.5027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -476.70100012026376 Explore P: 0.0960 SOC: 0.6310 Cumulative_SOC_deviation: 42.9809 Fuel Consumption: 46.8920\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -461.3508006049922 Explore P: 0.0936 SOC: 0.6463 Cumulative_SOC_deviation: 41.3288 Fuel Consumption: 48.0632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -552.0258518235112 Explore P: 0.0914 SOC: 0.6645 Cumulative_SOC_deviation: 50.1885 Fuel Consumption: 50.1410\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -383.5221907439334 Explore P: 0.0892 SOC: 0.6365 Cumulative_SOC_deviation: 33.6183 Fuel Consumption: 47.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -501.3416404466421 Explore P: 0.0870 SOC: 0.6929 Cumulative_SOC_deviation: 44.8994 Fuel Consumption: 52.3475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -336.2274290770162 Explore P: 0.0849 SOC: 0.6138 Cumulative_SOC_deviation: 29.0679 Fuel Consumption: 45.5480\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -476.15728699398386 Explore P: 0.0829 SOC: 0.6254 Cumulative_SOC_deviation: 42.9462 Fuel Consumption: 46.6953\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -913.450060041403 Explore P: 0.0809 SOC: 0.8464 Cumulative_SOC_deviation: 84.8082 Fuel Consumption: 65.3677\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -455.0672201453655 Explore P: 0.0790 SOC: 0.6168 Cumulative_SOC_deviation: 40.9536 Fuel Consumption: 45.5308\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -471.03759857734286 Explore P: 0.0771 SOC: 0.6225 Cumulative_SOC_deviation: 42.4731 Fuel Consumption: 46.3061\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -569.7535379934734 Explore P: 0.0753 SOC: 0.6261 Cumulative_SOC_deviation: 52.2885 Fuel Consumption: 46.8682\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -394.4827392498176 Explore P: 0.0735 SOC: 0.6245 Cumulative_SOC_deviation: 34.8219 Fuel Consumption: 46.2636\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -415.73315212471766 Explore P: 0.0718 SOC: 0.6258 Cumulative_SOC_deviation: 36.9388 Fuel Consumption: 46.3452\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -425.19217385353744 Explore P: 0.0701 SOC: 0.6318 Cumulative_SOC_deviation: 37.8032 Fuel Consumption: 47.1601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -459.7715829416233 Explore P: 0.0685 SOC: 0.6471 Cumulative_SOC_deviation: 41.1526 Fuel Consumption: 48.2456\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -361.1064115444171 Explore P: 0.0669 SOC: 0.6289 Cumulative_SOC_deviation: 31.4604 Fuel Consumption: 46.5028\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -340.5198003237332 Explore P: 0.0654 SOC: 0.6208 Cumulative_SOC_deviation: 29.4468 Fuel Consumption: 46.0522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -396.49259555151434 Explore P: 0.0639 SOC: 0.6314 Cumulative_SOC_deviation: 34.9539 Fuel Consumption: 46.9533\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -309.35605983500267 Explore P: 0.0624 SOC: 0.6419 Cumulative_SOC_deviation: 26.2182 Fuel Consumption: 47.1738\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -358.8680057974186 Explore P: 0.0610 SOC: 0.6120 Cumulative_SOC_deviation: 31.3615 Fuel Consumption: 45.2530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -404.44172334505487 Explore P: 0.0596 SOC: 0.6493 Cumulative_SOC_deviation: 35.6348 Fuel Consumption: 48.0941\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -378.4768723341018 Explore P: 0.0583 SOC: 0.6159 Cumulative_SOC_deviation: 33.2933 Fuel Consumption: 45.5434\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -508.8625104403181 Explore P: 0.0570 SOC: 0.7078 Cumulative_SOC_deviation: 45.5953 Fuel Consumption: 52.9095\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -356.51431855142994 Explore P: 0.0557 SOC: 0.6378 Cumulative_SOC_deviation: 30.9116 Fuel Consumption: 47.3983\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -324.4053049075559 Explore P: 0.0545 SOC: 0.6201 Cumulative_SOC_deviation: 27.8385 Fuel Consumption: 46.0198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -410.78050252073166 Explore P: 0.0533 SOC: 0.6356 Cumulative_SOC_deviation: 36.3382 Fuel Consumption: 47.3983\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -364.48877187295295 Explore P: 0.0521 SOC: 0.6437 Cumulative_SOC_deviation: 31.6740 Fuel Consumption: 47.7484\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -436.12392337668354 Explore P: 0.0510 SOC: 0.6293 Cumulative_SOC_deviation: 38.9628 Fuel Consumption: 46.4957\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -332.68470764916094 Explore P: 0.0498 SOC: 0.6411 Cumulative_SOC_deviation: 28.5151 Fuel Consumption: 47.5341\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -295.7049153674499 Explore P: 0.0488 SOC: 0.6195 Cumulative_SOC_deviation: 24.9664 Fuel Consumption: 46.0411\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -346.1625695781949 Explore P: 0.0477 SOC: 0.6237 Cumulative_SOC_deviation: 29.9329 Fuel Consumption: 46.8332\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -377.4438220533732 Explore P: 0.0467 SOC: 0.6148 Cumulative_SOC_deviation: 33.1733 Fuel Consumption: 45.7107\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -435.0075348708141 Explore P: 0.0457 SOC: 0.6085 Cumulative_SOC_deviation: 38.9532 Fuel Consumption: 45.4760\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -386.74731093863915 Explore P: 0.0447 SOC: 0.6218 Cumulative_SOC_deviation: 34.0093 Fuel Consumption: 46.6548\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -311.134506006946 Explore P: 0.0438 SOC: 0.6115 Cumulative_SOC_deviation: 26.6013 Fuel Consumption: 45.1218\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -462.64102184120406 Explore P: 0.0429 SOC: 0.6370 Cumulative_SOC_deviation: 41.5345 Fuel Consumption: 47.2964\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -393.38737745073615 Explore P: 0.0420 SOC: 0.6228 Cumulative_SOC_deviation: 34.7453 Fuel Consumption: 45.9347\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -345.0871914142483 Explore P: 0.0411 SOC: 0.6155 Cumulative_SOC_deviation: 29.9644 Fuel Consumption: 45.4436\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -454.7303612949407 Explore P: 0.0403 SOC: 0.6169 Cumulative_SOC_deviation: 40.8878 Fuel Consumption: 45.8526\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -311.89635542659533 Explore P: 0.0395 SOC: 0.6052 Cumulative_SOC_deviation: 26.7250 Fuel Consumption: 44.6459\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -346.1371128533087 Explore P: 0.0387 SOC: 0.6263 Cumulative_SOC_deviation: 29.9649 Fuel Consumption: 46.4881\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -379.48984350095105 Explore P: 0.0379 SOC: 0.6105 Cumulative_SOC_deviation: 33.4119 Fuel Consumption: 45.3711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -305.73186822018073 Explore P: 0.0371 SOC: 0.6116 Cumulative_SOC_deviation: 26.0623 Fuel Consumption: 45.1086\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -290.90276486254345 Explore P: 0.0364 SOC: 0.6102 Cumulative_SOC_deviation: 24.5597 Fuel Consumption: 45.3063\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -383.36181584401794 Explore P: 0.0357 SOC: 0.6113 Cumulative_SOC_deviation: 33.7957 Fuel Consumption: 45.4046\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -420.1672606706551 Explore P: 0.0350 SOC: 0.6196 Cumulative_SOC_deviation: 37.4073 Fuel Consumption: 46.0943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -371.9373088787564 Explore P: 0.0343 SOC: 0.6410 Cumulative_SOC_deviation: 32.4257 Fuel Consumption: 47.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -344.71281873834295 Explore P: 0.0336 SOC: 0.6188 Cumulative_SOC_deviation: 29.8878 Fuel Consumption: 45.8348\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -465.612725221063 Explore P: 0.0330 SOC: 0.6463 Cumulative_SOC_deviation: 41.7574 Fuel Consumption: 48.0388\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -338.07479630914474 Explore P: 0.0324 SOC: 0.6153 Cumulative_SOC_deviation: 29.2689 Fuel Consumption: 45.3853\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -339.58643021813003 Explore P: 0.0318 SOC: 0.6187 Cumulative_SOC_deviation: 29.4093 Fuel Consumption: 45.4933\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -379.2179069553769 Explore P: 0.0312 SOC: 0.6385 Cumulative_SOC_deviation: 33.2275 Fuel Consumption: 46.9432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -296.64188326456457 Explore P: 0.0306 SOC: 0.6134 Cumulative_SOC_deviation: 25.1509 Fuel Consumption: 45.1324\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -283.2049462814439 Explore P: 0.0301 SOC: 0.6121 Cumulative_SOC_deviation: 23.8083 Fuel Consumption: 45.1223\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -287.9698721006876 Explore P: 0.0295 SOC: 0.6132 Cumulative_SOC_deviation: 24.2908 Fuel Consumption: 45.0620\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -323.54315826431446 Explore P: 0.0290 SOC: 0.6075 Cumulative_SOC_deviation: 27.8632 Fuel Consumption: 44.9110\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -325.2938502267118 Explore P: 0.0285 SOC: 0.6074 Cumulative_SOC_deviation: 28.0357 Fuel Consumption: 44.9368\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -365.1296075226438 Explore P: 0.0280 SOC: 0.6327 Cumulative_SOC_deviation: 31.8260 Fuel Consumption: 46.8697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -302.259318138002 Explore P: 0.0275 SOC: 0.6076 Cumulative_SOC_deviation: 25.7325 Fuel Consumption: 44.9343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -336.6774296284692 Explore P: 0.0270 SOC: 0.6113 Cumulative_SOC_deviation: 29.1467 Fuel Consumption: 45.2100\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -325.02083493110905 Explore P: 0.0265 SOC: 0.6141 Cumulative_SOC_deviation: 27.9489 Fuel Consumption: 45.5323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -359.9649674851207 Explore P: 0.0261 SOC: 0.6123 Cumulative_SOC_deviation: 31.4566 Fuel Consumption: 45.3985\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -332.1947101333946 Explore P: 0.0257 SOC: 0.6123 Cumulative_SOC_deviation: 28.6981 Fuel Consumption: 45.2135\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -337.71843376435373 Explore P: 0.0252 SOC: 0.6121 Cumulative_SOC_deviation: 29.2439 Fuel Consumption: 45.2799\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -288.6913200676162 Explore P: 0.0248 SOC: 0.6136 Cumulative_SOC_deviation: 24.3365 Fuel Consumption: 45.3265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -324.80549091259974 Explore P: 0.0244 SOC: 0.6137 Cumulative_SOC_deviation: 27.9296 Fuel Consumption: 45.5095\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -348.55300646235946 Explore P: 0.0240 SOC: 0.6184 Cumulative_SOC_deviation: 30.2517 Fuel Consumption: 46.0360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -359.89236120488164 Explore P: 0.0237 SOC: 0.6176 Cumulative_SOC_deviation: 31.3864 Fuel Consumption: 46.0284\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -422.0562358571291 Explore P: 0.0233 SOC: 0.6152 Cumulative_SOC_deviation: 37.6317 Fuel Consumption: 45.7396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -326.3670091287882 Explore P: 0.0229 SOC: 0.6143 Cumulative_SOC_deviation: 28.0783 Fuel Consumption: 45.5840\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -319.1624929878036 Explore P: 0.0226 SOC: 0.6036 Cumulative_SOC_deviation: 27.4665 Fuel Consumption: 44.4979\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -422.94130654195607 Explore P: 0.0222 SOC: 0.6127 Cumulative_SOC_deviation: 37.7141 Fuel Consumption: 45.7999\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -447.0707318090602 Explore P: 0.0219 SOC: 0.6255 Cumulative_SOC_deviation: 40.0521 Fuel Consumption: 46.5499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -319.1920189221081 Explore P: 0.0216 SOC: 0.6127 Cumulative_SOC_deviation: 27.3983 Fuel Consumption: 45.2095\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -407.0866911317205 Explore P: 0.0213 SOC: 0.6227 Cumulative_SOC_deviation: 36.0641 Fuel Consumption: 46.4455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -387.2541136767821 Explore P: 0.0210 SOC: 0.6180 Cumulative_SOC_deviation: 34.1346 Fuel Consumption: 45.9083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -353.5872284031233 Explore P: 0.0207 SOC: 0.6154 Cumulative_SOC_deviation: 30.7820 Fuel Consumption: 45.7669\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -404.5594908469154 Explore P: 0.0204 SOC: 0.6132 Cumulative_SOC_deviation: 35.8918 Fuel Consumption: 45.6412\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -325.32044533914024 Explore P: 0.0201 SOC: 0.6097 Cumulative_SOC_deviation: 28.0181 Fuel Consumption: 45.1390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -316.02521584772535 Explore P: 0.0198 SOC: 0.6138 Cumulative_SOC_deviation: 27.0751 Fuel Consumption: 45.2743\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -282.20234985747584 Explore P: 0.0196 SOC: 0.6105 Cumulative_SOC_deviation: 23.7136 Fuel Consumption: 45.0660\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -323.6836894363491 Explore P: 0.0193 SOC: 0.6097 Cumulative_SOC_deviation: 27.8769 Fuel Consumption: 44.9145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -334.0476191095403 Explore P: 0.0190 SOC: 0.6074 Cumulative_SOC_deviation: 28.9228 Fuel Consumption: 44.8192\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -305.2555317364455 Explore P: 0.0188 SOC: 0.6079 Cumulative_SOC_deviation: 26.0430 Fuel Consumption: 44.8258\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -295.21621703981566 Explore P: 0.0186 SOC: 0.6184 Cumulative_SOC_deviation: 24.9487 Fuel Consumption: 45.7294\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -323.4823356085279 Explore P: 0.0183 SOC: 0.6077 Cumulative_SOC_deviation: 27.8491 Fuel Consumption: 44.9910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -305.8287089951072 Explore P: 0.0181 SOC: 0.6144 Cumulative_SOC_deviation: 26.0467 Fuel Consumption: 45.3615\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -352.0806090576 Explore P: 0.0179 SOC: 0.6063 Cumulative_SOC_deviation: 30.7141 Fuel Consumption: 44.9399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -322.9981702997429 Explore P: 0.0177 SOC: 0.6108 Cumulative_SOC_deviation: 27.7941 Fuel Consumption: 45.0569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -317.64967494530384 Explore P: 0.0175 SOC: 0.6105 Cumulative_SOC_deviation: 27.2316 Fuel Consumption: 45.3341\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -294.4779505466253 Explore P: 0.0173 SOC: 0.6111 Cumulative_SOC_deviation: 24.9508 Fuel Consumption: 44.9698\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -329.57719153230306 Explore P: 0.0171 SOC: 0.6126 Cumulative_SOC_deviation: 28.4581 Fuel Consumption: 44.9966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -360.2494149563629 Explore P: 0.0169 SOC: 0.6177 Cumulative_SOC_deviation: 31.4516 Fuel Consumption: 45.7330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -280.9348001316835 Explore P: 0.0167 SOC: 0.6144 Cumulative_SOC_deviation: 23.5633 Fuel Consumption: 45.3017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -348.08406532820436 Explore P: 0.0165 SOC: 0.6096 Cumulative_SOC_deviation: 30.3187 Fuel Consumption: 44.8973\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -345.0161335627402 Explore P: 0.0163 SOC: 0.6164 Cumulative_SOC_deviation: 29.9283 Fuel Consumption: 45.7330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -321.7641369273057 Explore P: 0.0162 SOC: 0.6081 Cumulative_SOC_deviation: 27.6637 Fuel Consumption: 45.1274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -375.1319193094701 Explore P: 0.0160 SOC: 0.6120 Cumulative_SOC_deviation: 32.9625 Fuel Consumption: 45.5064\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -385.72317766733516 Explore P: 0.0158 SOC: 0.6096 Cumulative_SOC_deviation: 34.0495 Fuel Consumption: 45.2282\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -328.05491696772026 Explore P: 0.0157 SOC: 0.6068 Cumulative_SOC_deviation: 28.3162 Fuel Consumption: 44.8932\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -276.0999960490682 Explore P: 0.0155 SOC: 0.6062 Cumulative_SOC_deviation: 23.1435 Fuel Consumption: 44.6652\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -275.0150399633474 Explore P: 0.0154 SOC: 0.6061 Cumulative_SOC_deviation: 23.0417 Fuel Consumption: 44.5978\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -301.37329829475846 Explore P: 0.0152 SOC: 0.6054 Cumulative_SOC_deviation: 25.6722 Fuel Consumption: 44.6510\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -312.0842527607948 Explore P: 0.0151 SOC: 0.6102 Cumulative_SOC_deviation: 26.6999 Fuel Consumption: 45.0853\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -285.23426191612157 Explore P: 0.0149 SOC: 0.6096 Cumulative_SOC_deviation: 24.0308 Fuel Consumption: 44.9262\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -328.86312349016873 Explore P: 0.0148 SOC: 0.6167 Cumulative_SOC_deviation: 28.3188 Fuel Consumption: 45.6752\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -291.595837881199 Explore P: 0.0147 SOC: 0.6106 Cumulative_SOC_deviation: 24.6584 Fuel Consumption: 45.0123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -279.3523637494775 Explore P: 0.0146 SOC: 0.6050 Cumulative_SOC_deviation: 23.4625 Fuel Consumption: 44.7270\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -258.64947658500125 Explore P: 0.0144 SOC: 0.6088 Cumulative_SOC_deviation: 21.3795 Fuel Consumption: 44.8547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -248.64712358387135 Explore P: 0.0143 SOC: 0.6057 Cumulative_SOC_deviation: 20.3835 Fuel Consumption: 44.8121\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -281.1015112286678 Explore P: 0.0142 SOC: 0.6068 Cumulative_SOC_deviation: 23.6323 Fuel Consumption: 44.7787\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -290.2701599939949 Explore P: 0.0141 SOC: 0.6069 Cumulative_SOC_deviation: 24.5241 Fuel Consumption: 45.0296\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
