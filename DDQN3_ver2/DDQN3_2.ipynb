{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -982.9904969155 Explore P: 0.9217 SOC: 0.8091 Cumulative_SOC_deviation: 92.1441 Fuel Consumption: 61.5496\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -906.9265028542512 Explore P: 0.8970 SOC: 0.7882 Cumulative_SOC_deviation: 84.7211 Fuel Consumption: 59.7156\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -1071.0817207581233 Explore P: 0.8730 SOC: 0.8311 Cumulative_SOC_deviation: 100.7847 Fuel Consumption: 63.2347\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -1001.6821630767378 Explore P: 0.8496 SOC: 0.8075 Cumulative_SOC_deviation: 94.0215 Fuel Consumption: 61.4670\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -1022.8323811290156 Explore P: 0.8269 SOC: 0.8132 Cumulative_SOC_deviation: 96.0947 Fuel Consumption: 61.8856\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -984.4462749579889 Explore P: 0.8048 SOC: 0.7982 Cumulative_SOC_deviation: 92.3785 Fuel Consumption: 60.6612\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -927.37816018352 Explore P: 0.7832 SOC: 0.7815 Cumulative_SOC_deviation: 86.8020 Fuel Consumption: 59.3578\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -885.2893482837186 Explore P: 0.7623 SOC: 0.7751 Cumulative_SOC_deviation: 82.6585 Fuel Consumption: 58.7041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -906.296308097352 Explore P: 0.7419 SOC: 0.7698 Cumulative_SOC_deviation: 84.8133 Fuel Consumption: 58.1633\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -846.3286420342868 Explore P: 0.7221 SOC: 0.7724 Cumulative_SOC_deviation: 78.7963 Fuel Consumption: 58.3660\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -943.5159155677567 Explore P: 0.7028 SOC: 0.8061 Cumulative_SOC_deviation: 88.2375 Fuel Consumption: 61.1412\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -1048.3282596007077 Explore P: 0.6840 SOC: 0.8182 Cumulative_SOC_deviation: 98.6026 Fuel Consumption: 62.3027\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -980.9242644476592 Explore P: 0.6658 SOC: 0.8095 Cumulative_SOC_deviation: 91.9516 Fuel Consumption: 61.4082\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -884.6438093632987 Explore P: 0.6480 SOC: 0.7644 Cumulative_SOC_deviation: 82.6791 Fuel Consumption: 57.8527\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -866.2501667430357 Explore P: 0.6307 SOC: 0.7618 Cumulative_SOC_deviation: 80.8801 Fuel Consumption: 57.4488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -892.4945056645031 Explore P: 0.6139 SOC: 0.7727 Cumulative_SOC_deviation: 83.4131 Fuel Consumption: 58.3635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -815.8292264161256 Explore P: 0.5976 SOC: 0.7479 Cumulative_SOC_deviation: 75.9279 Fuel Consumption: 56.5502\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -868.476788339925 Explore P: 0.5816 SOC: 0.7864 Cumulative_SOC_deviation: 80.8822 Fuel Consumption: 59.6548\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -774.3374044389309 Explore P: 0.5662 SOC: 0.7212 Cumulative_SOC_deviation: 72.0127 Fuel Consumption: 54.2104\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -835.506219614468 Explore P: 0.5511 SOC: 0.7583 Cumulative_SOC_deviation: 77.8354 Fuel Consumption: 57.1518\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -784.4722163789827 Explore P: 0.5364 SOC: 0.7258 Cumulative_SOC_deviation: 72.9682 Fuel Consumption: 54.7907\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -845.0084395646014 Explore P: 0.5222 SOC: 0.7587 Cumulative_SOC_deviation: 78.7570 Fuel Consumption: 57.4381\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -813.8089671956366 Explore P: 0.5083 SOC: 0.7497 Cumulative_SOC_deviation: 75.7191 Fuel Consumption: 56.6181\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -855.9064277087775 Explore P: 0.4948 SOC: 0.7808 Cumulative_SOC_deviation: 79.6793 Fuel Consumption: 59.1130\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -1153.269430535649 Explore P: 0.4817 SOC: 0.8450 Cumulative_SOC_deviation: 108.8679 Fuel Consumption: 64.5908\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -890.4517479561913 Explore P: 0.4689 SOC: 0.7142 Cumulative_SOC_deviation: 83.6554 Fuel Consumption: 53.8977\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -805.9026373279879 Explore P: 0.4565 SOC: 0.7658 Cumulative_SOC_deviation: 74.7771 Fuel Consumption: 58.1314\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -807.5980103871609 Explore P: 0.4444 SOC: 0.7292 Cumulative_SOC_deviation: 75.2523 Fuel Consumption: 55.0750\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -783.8772622090696 Explore P: 0.4326 SOC: 0.7276 Cumulative_SOC_deviation: 72.9142 Fuel Consumption: 54.7349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -905.0457206879021 Explore P: 0.4212 SOC: 0.7720 Cumulative_SOC_deviation: 84.6605 Fuel Consumption: 58.4410\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -1012.975476766907 Explore P: 0.4100 SOC: 0.7977 Cumulative_SOC_deviation: 95.2215 Fuel Consumption: 60.7601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -1156.330612653703 Explore P: 0.3992 SOC: 0.8382 Cumulative_SOC_deviation: 109.2500 Fuel Consumption: 63.8307\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -903.3831646085579 Explore P: 0.3887 SOC: 0.7661 Cumulative_SOC_deviation: 84.5126 Fuel Consumption: 58.2576\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -931.5359020799962 Explore P: 0.3784 SOC: 0.7630 Cumulative_SOC_deviation: 87.3885 Fuel Consumption: 57.6505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -1292.3235950833136 Explore P: 0.3684 SOC: 0.8697 Cumulative_SOC_deviation: 122.5381 Fuel Consumption: 66.9428\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -1145.3650715153901 Explore P: 0.3587 SOC: 0.8177 Cumulative_SOC_deviation: 108.2786 Fuel Consumption: 62.5794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -1036.4469229855106 Explore P: 0.3493 SOC: 0.8136 Cumulative_SOC_deviation: 97.4243 Fuel Consumption: 62.2044\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -1382.1899015749475 Explore P: 0.3401 SOC: 0.8758 Cumulative_SOC_deviation: 131.4458 Fuel Consumption: 67.7324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -868.0061359741658 Explore P: 0.3311 SOC: 0.7268 Cumulative_SOC_deviation: 81.3535 Fuel Consumption: 54.4714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -895.1486939104486 Explore P: 0.3224 SOC: 0.7912 Cumulative_SOC_deviation: 83.5177 Fuel Consumption: 59.9715\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -970.0221575099534 Explore P: 0.3140 SOC: 0.8083 Cumulative_SOC_deviation: 90.8553 Fuel Consumption: 61.4696\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -673.3787080812323 Explore P: 0.3057 SOC: 0.7164 Cumulative_SOC_deviation: 61.9541 Fuel Consumption: 53.8379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -810.300132248252 Explore P: 0.2977 SOC: 0.6737 Cumulative_SOC_deviation: 76.0117 Fuel Consumption: 50.1830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -883.7883395795512 Explore P: 0.2899 SOC: 0.6323 Cumulative_SOC_deviation: 83.6819 Fuel Consumption: 46.9695\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -870.0386942189967 Explore P: 0.2824 SOC: 0.6508 Cumulative_SOC_deviation: 82.1761 Fuel Consumption: 48.2780\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -1004.0832746050888 Explore P: 0.2750 SOC: 0.7873 Cumulative_SOC_deviation: 94.3814 Fuel Consumption: 60.2690\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -989.2158141419596 Explore P: 0.2678 SOC: 0.7998 Cumulative_SOC_deviation: 92.8384 Fuel Consumption: 60.8320\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -903.9336409994974 Explore P: 0.2608 SOC: 0.7524 Cumulative_SOC_deviation: 84.7180 Fuel Consumption: 56.7534\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -822.6910825888715 Explore P: 0.2540 SOC: 0.7268 Cumulative_SOC_deviation: 76.8104 Fuel Consumption: 54.5875\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -1415.2406521559196 Explore P: 0.2474 SOC: 0.8803 Cumulative_SOC_deviation: 134.7280 Fuel Consumption: 67.9609\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -1048.9804353474262 Explore P: 0.2410 SOC: 0.8045 Cumulative_SOC_deviation: 98.7774 Fuel Consumption: 61.2065\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -1071.9137125676357 Explore P: 0.2347 SOC: 0.8094 Cumulative_SOC_deviation: 101.0383 Fuel Consumption: 61.5304\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -1085.629265305418 Explore P: 0.2286 SOC: 0.8238 Cumulative_SOC_deviation: 102.3167 Fuel Consumption: 62.4618\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -800.6443780154242 Explore P: 0.2227 SOC: 0.7781 Cumulative_SOC_deviation: 74.1984 Fuel Consumption: 58.6600\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -967.0237377464606 Explore P: 0.2170 SOC: 0.7904 Cumulative_SOC_deviation: 90.6707 Fuel Consumption: 60.3171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -1500.4693456121192 Explore P: 0.2114 SOC: 0.9153 Cumulative_SOC_deviation: 142.9328 Fuel Consumption: 71.1415\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -1094.9482530422404 Explore P: 0.2059 SOC: 0.8493 Cumulative_SOC_deviation: 102.9112 Fuel Consumption: 65.8360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -724.3189227356348 Explore P: 0.2006 SOC: 0.6623 Cumulative_SOC_deviation: 67.5006 Fuel Consumption: 49.3129\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -1020.5210560843401 Explore P: 0.1954 SOC: 0.7259 Cumulative_SOC_deviation: 96.5461 Fuel Consumption: 55.0603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -839.0672578769764 Explore P: 0.1904 SOC: 0.7733 Cumulative_SOC_deviation: 78.0552 Fuel Consumption: 58.5155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -853.1633587023126 Explore P: 0.1855 SOC: 0.7811 Cumulative_SOC_deviation: 79.4088 Fuel Consumption: 59.0755\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -926.088948488857 Explore P: 0.1808 SOC: 0.7801 Cumulative_SOC_deviation: 86.6851 Fuel Consumption: 59.2377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -768.4420152001607 Explore P: 0.1761 SOC: 0.6837 Cumulative_SOC_deviation: 71.7836 Fuel Consumption: 50.6062\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -795.9108776971048 Explore P: 0.1716 SOC: 0.6331 Cumulative_SOC_deviation: 74.9333 Fuel Consumption: 46.5783\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -702.3362602944161 Explore P: 0.1673 SOC: 0.6765 Cumulative_SOC_deviation: 65.1984 Fuel Consumption: 50.3523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -1140.0968994043853 Explore P: 0.1630 SOC: 0.6182 Cumulative_SOC_deviation: 109.4303 Fuel Consumption: 45.7938\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -986.9030262556679 Explore P: 0.1589 SOC: 0.7340 Cumulative_SOC_deviation: 93.1435 Fuel Consumption: 55.4682\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -1244.627107089742 Explore P: 0.1548 SOC: 0.8591 Cumulative_SOC_deviation: 117.8488 Fuel Consumption: 66.1396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -581.6188929679868 Explore P: 0.1509 SOC: 0.6642 Cumulative_SOC_deviation: 53.2524 Fuel Consumption: 49.0950\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -794.679132094224 Explore P: 0.1471 SOC: 0.7325 Cumulative_SOC_deviation: 74.0183 Fuel Consumption: 54.4957\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -899.0972300626877 Explore P: 0.1434 SOC: 0.7663 Cumulative_SOC_deviation: 84.1729 Fuel Consumption: 57.3682\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -1109.3466371964116 Explore P: 0.1398 SOC: 0.8272 Cumulative_SOC_deviation: 104.6168 Fuel Consumption: 63.1789\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -1278.2909567486222 Explore P: 0.1362 SOC: 0.8900 Cumulative_SOC_deviation: 120.9423 Fuel Consumption: 68.8676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -897.7992255097424 Explore P: 0.1328 SOC: 0.7745 Cumulative_SOC_deviation: 83.9691 Fuel Consumption: 58.1086\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -1014.1684347873138 Explore P: 0.1295 SOC: 0.7005 Cumulative_SOC_deviation: 96.1792 Fuel Consumption: 52.3769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -1040.3855140801559 Explore P: 0.1263 SOC: 0.8029 Cumulative_SOC_deviation: 97.8966 Fuel Consumption: 61.4199\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -953.5198466421718 Explore P: 0.1231 SOC: 0.7885 Cumulative_SOC_deviation: 89.3973 Fuel Consumption: 59.5468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -692.0673120659268 Explore P: 0.1200 SOC: 0.7309 Cumulative_SOC_deviation: 63.7484 Fuel Consumption: 54.5829\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -723.9180311201114 Explore P: 0.1171 SOC: 0.7311 Cumulative_SOC_deviation: 66.9187 Fuel Consumption: 54.7309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -1225.754771219669 Explore P: 0.1142 SOC: 0.8771 Cumulative_SOC_deviation: 115.8276 Fuel Consumption: 67.4790\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -659.5798785164743 Explore P: 0.1113 SOC: 0.6858 Cumulative_SOC_deviation: 60.8805 Fuel Consumption: 50.7744\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -806.0647712213203 Explore P: 0.1086 SOC: 0.6544 Cumulative_SOC_deviation: 75.7650 Fuel Consumption: 48.4144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -838.7964592370172 Explore P: 0.1059 SOC: 0.6287 Cumulative_SOC_deviation: 79.2463 Fuel Consumption: 46.3335\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -912.593351058826 Explore P: 0.1033 SOC: 0.6516 Cumulative_SOC_deviation: 86.4527 Fuel Consumption: 48.0662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -935.7403509380167 Explore P: 0.1008 SOC: 0.6526 Cumulative_SOC_deviation: 88.7601 Fuel Consumption: 48.1392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -792.5149076858864 Explore P: 0.0983 SOC: 0.7191 Cumulative_SOC_deviation: 73.8836 Fuel Consumption: 53.6793\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -910.798755615284 Explore P: 0.0960 SOC: 0.7309 Cumulative_SOC_deviation: 85.5658 Fuel Consumption: 55.1404\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -887.6031499435312 Explore P: 0.0936 SOC: 0.7696 Cumulative_SOC_deviation: 82.8993 Fuel Consumption: 58.6103\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -1047.5339330610614 Explore P: 0.0914 SOC: 0.8026 Cumulative_SOC_deviation: 98.6226 Fuel Consumption: 61.3079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -909.7101337386171 Explore P: 0.0892 SOC: 0.7523 Cumulative_SOC_deviation: 85.2858 Fuel Consumption: 56.8518\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -947.8934219764167 Explore P: 0.0870 SOC: 0.7815 Cumulative_SOC_deviation: 88.8241 Fuel Consumption: 59.6522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -980.8227512207216 Explore P: 0.0849 SOC: 0.7785 Cumulative_SOC_deviation: 92.1682 Fuel Consumption: 59.1409\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -708.891507539792 Explore P: 0.0829 SOC: 0.7081 Cumulative_SOC_deviation: 65.6247 Fuel Consumption: 52.6445\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -695.2430116401207 Explore P: 0.0809 SOC: 0.6726 Cumulative_SOC_deviation: 64.5444 Fuel Consumption: 49.7994\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -1018.45880441513 Explore P: 0.0790 SOC: 0.5898 Cumulative_SOC_deviation: 97.5320 Fuel Consumption: 43.1393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -866.0146148271442 Explore P: 0.0771 SOC: 0.6840 Cumulative_SOC_deviation: 81.5139 Fuel Consumption: 50.8753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -1006.1358600125013 Explore P: 0.0753 SOC: 0.7980 Cumulative_SOC_deviation: 94.4681 Fuel Consumption: 61.4549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -1127.013205701058 Explore P: 0.0735 SOC: 0.8322 Cumulative_SOC_deviation: 106.3252 Fuel Consumption: 63.7607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -805.0917536419458 Explore P: 0.0718 SOC: 0.7548 Cumulative_SOC_deviation: 74.8561 Fuel Consumption: 56.5310\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -933.6955079788672 Explore P: 0.0701 SOC: 0.7676 Cumulative_SOC_deviation: 87.5891 Fuel Consumption: 57.8045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -804.5267010886183 Explore P: 0.0685 SOC: 0.7288 Cumulative_SOC_deviation: 74.9680 Fuel Consumption: 54.8469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -818.6373612966033 Explore P: 0.0669 SOC: 0.7560 Cumulative_SOC_deviation: 76.1923 Fuel Consumption: 56.7144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -1048.305572031916 Explore P: 0.0654 SOC: 0.8034 Cumulative_SOC_deviation: 98.6772 Fuel Consumption: 61.5334\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -910.3571330740499 Explore P: 0.0639 SOC: 0.7539 Cumulative_SOC_deviation: 85.3244 Fuel Consumption: 57.1128\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -731.6407057756645 Explore P: 0.0624 SOC: 0.7293 Cumulative_SOC_deviation: 67.7527 Fuel Consumption: 54.1141\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -818.2424595081392 Explore P: 0.0610 SOC: 0.6326 Cumulative_SOC_deviation: 77.1869 Fuel Consumption: 46.3735\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -836.0600792072016 Explore P: 0.0596 SOC: 0.6955 Cumulative_SOC_deviation: 78.4277 Fuel Consumption: 51.7834\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -768.3089988404791 Explore P: 0.0583 SOC: 0.6570 Cumulative_SOC_deviation: 71.9632 Fuel Consumption: 48.6769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -859.1352841606214 Explore P: 0.0570 SOC: 0.6564 Cumulative_SOC_deviation: 81.0670 Fuel Consumption: 48.4650\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -718.4140111829421 Explore P: 0.0557 SOC: 0.6976 Cumulative_SOC_deviation: 66.6313 Fuel Consumption: 52.1007\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -1048.1759754109237 Explore P: 0.0545 SOC: 0.8250 Cumulative_SOC_deviation: 98.4938 Fuel Consumption: 63.2382\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -940.8365286155515 Explore P: 0.0533 SOC: 0.7934 Cumulative_SOC_deviation: 88.0542 Fuel Consumption: 60.2943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -1189.5486266104194 Explore P: 0.0521 SOC: 0.8296 Cumulative_SOC_deviation: 112.5683 Fuel Consumption: 63.8656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -898.2873642174548 Explore P: 0.0510 SOC: 0.7940 Cumulative_SOC_deviation: 83.8342 Fuel Consumption: 59.9457\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -844.405856498492 Explore P: 0.0498 SOC: 0.6714 Cumulative_SOC_deviation: 79.4395 Fuel Consumption: 50.0112\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -587.2405242949907 Explore P: 0.0488 SOC: 0.6896 Cumulative_SOC_deviation: 53.5847 Fuel Consumption: 51.3937\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -1163.2775223758067 Explore P: 0.0477 SOC: 0.5905 Cumulative_SOC_deviation: 111.9494 Fuel Consumption: 43.7839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -834.304014922685 Explore P: 0.0467 SOC: 0.6769 Cumulative_SOC_deviation: 78.3847 Fuel Consumption: 50.4567\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -634.3392680939619 Explore P: 0.0457 SOC: 0.6446 Cumulative_SOC_deviation: 58.6705 Fuel Consumption: 47.6344\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -824.8210418244382 Explore P: 0.0447 SOC: 0.6532 Cumulative_SOC_deviation: 77.6551 Fuel Consumption: 48.2704\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -793.2572832802123 Explore P: 0.0438 SOC: 0.6902 Cumulative_SOC_deviation: 74.2021 Fuel Consumption: 51.2361\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -824.4470012996854 Explore P: 0.0429 SOC: 0.6592 Cumulative_SOC_deviation: 77.5660 Fuel Consumption: 48.7868\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -893.7855231916901 Explore P: 0.0420 SOC: 0.7783 Cumulative_SOC_deviation: 83.4235 Fuel Consumption: 59.5509\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -1006.9231175544115 Explore P: 0.0411 SOC: 0.8279 Cumulative_SOC_deviation: 94.3680 Fuel Consumption: 63.2433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -674.353795208246 Explore P: 0.0403 SOC: 0.6461 Cumulative_SOC_deviation: 62.6393 Fuel Consumption: 47.9613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -1057.768336097001 Explore P: 0.0395 SOC: 0.7044 Cumulative_SOC_deviation: 100.4680 Fuel Consumption: 53.0884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -713.1313440512677 Explore P: 0.0387 SOC: 0.6633 Cumulative_SOC_deviation: 66.3398 Fuel Consumption: 49.7330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -1529.0870154846107 Explore P: 0.0379 SOC: 0.8000 Cumulative_SOC_deviation: 146.7506 Fuel Consumption: 61.5811\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -715.7217906433755 Explore P: 0.0371 SOC: 0.7081 Cumulative_SOC_deviation: 66.3880 Fuel Consumption: 51.8417\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -907.756131733284 Explore P: 0.0364 SOC: 0.6790 Cumulative_SOC_deviation: 85.7792 Fuel Consumption: 49.9646\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -388.21667381311033 Explore P: 0.0357 SOC: 0.6311 Cumulative_SOC_deviation: 34.2040 Fuel Consumption: 46.1764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -787.750310422636 Explore P: 0.0350 SOC: 0.6006 Cumulative_SOC_deviation: 74.3340 Fuel Consumption: 44.4108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -1422.9438071088696 Explore P: 0.0343 SOC: 0.7599 Cumulative_SOC_deviation: 136.4979 Fuel Consumption: 57.9647\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -442.1101218575463 Explore P: 0.0336 SOC: 0.5756 Cumulative_SOC_deviation: 40.0047 Fuel Consumption: 42.0629\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -2048.9374659023747 Explore P: 0.0330 SOC: 0.6096 Cumulative_SOC_deviation: 200.2396 Fuel Consumption: 46.5413\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -1519.1272800856373 Explore P: 0.0324 SOC: 0.8510 Cumulative_SOC_deviation: 145.4115 Fuel Consumption: 65.0120\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -1071.7091284976927 Explore P: 0.0318 SOC: 0.7621 Cumulative_SOC_deviation: 101.4465 Fuel Consumption: 57.2445\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -1937.0078724754972 Explore P: 0.0312 SOC: 0.5818 Cumulative_SOC_deviation: 189.2844 Fuel Consumption: 44.1635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -1413.5426688623077 Explore P: 0.0306 SOC: 0.6105 Cumulative_SOC_deviation: 136.8679 Fuel Consumption: 44.8633\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -1478.4402342426686 Explore P: 0.0301 SOC: 0.8907 Cumulative_SOC_deviation: 141.0889 Fuel Consumption: 67.5509\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -1418.95199296571 Explore P: 0.0295 SOC: 0.7255 Cumulative_SOC_deviation: 136.3612 Fuel Consumption: 55.3400\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -1623.7370102002667 Explore P: 0.0290 SOC: 0.9365 Cumulative_SOC_deviation: 155.1589 Fuel Consumption: 72.1480\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -1880.2506174207008 Explore P: 0.0285 SOC: 0.9411 Cumulative_SOC_deviation: 180.8790 Fuel Consumption: 71.4608\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -1356.3895046854607 Explore P: 0.0280 SOC: 0.5477 Cumulative_SOC_deviation: 131.5438 Fuel Consumption: 40.9515\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -1393.7899700483092 Explore P: 0.0275 SOC: 0.5651 Cumulative_SOC_deviation: 135.1872 Fuel Consumption: 41.9184\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -1277.8759234423765 Explore P: 0.0270 SOC: 0.5224 Cumulative_SOC_deviation: 123.8451 Fuel Consumption: 39.4251\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -2621.585446786896 Explore P: 0.0265 SOC: 0.4851 Cumulative_SOC_deviation: 258.5058 Fuel Consumption: 36.5273\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -1297.2476560569708 Explore P: 0.0261 SOC: 0.6230 Cumulative_SOC_deviation: 125.1161 Fuel Consumption: 46.0862\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -2280.1921731571447 Explore P: 0.0257 SOC: 0.9289 Cumulative_SOC_deviation: 220.8988 Fuel Consumption: 71.2038\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -1712.5749242677473 Explore P: 0.0252 SOC: 0.9175 Cumulative_SOC_deviation: 164.3188 Fuel Consumption: 69.3865\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -1053.9804843398913 Explore P: 0.0248 SOC: 0.8566 Cumulative_SOC_deviation: 98.9488 Fuel Consumption: 64.4925\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 154 Total reward: -2665.4522387683633 Explore P: 0.0244 SOC: 1.0000 Cumulative_SOC_deviation: 257.8474 Fuel Consumption: 86.9778\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -814.725674413493 Explore P: 0.0241 SOC: 0.5383 Cumulative_SOC_deviation: 77.5533 Fuel Consumption: 39.1930\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -1029.8296069857877 Explore P: 0.0237 SOC: 0.6650 Cumulative_SOC_deviation: 97.9714 Fuel Consumption: 50.1151\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -1721.7753908693742 Explore P: 0.0233 SOC: 0.6601 Cumulative_SOC_deviation: 167.2311 Fuel Consumption: 49.4644\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -3047.3805682670713 Explore P: 0.0229 SOC: 0.9933 Cumulative_SOC_deviation: 296.6527 Fuel Consumption: 80.8540\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -865.1871134469076 Explore P: 0.0226 SOC: 0.7110 Cumulative_SOC_deviation: 81.2893 Fuel Consumption: 52.2938\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -1575.0504133618967 Explore P: 0.0222 SOC: 0.5378 Cumulative_SOC_deviation: 153.5817 Fuel Consumption: 39.2330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -878.7703998242155 Explore P: 0.0219 SOC: 0.7739 Cumulative_SOC_deviation: 82.1656 Fuel Consumption: 57.1143\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -1063.8087664971129 Explore P: 0.0216 SOC: 0.8089 Cumulative_SOC_deviation: 100.3629 Fuel Consumption: 60.1793\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -668.7072904773134 Explore P: 0.0213 SOC: 0.6463 Cumulative_SOC_deviation: 62.0303 Fuel Consumption: 48.4042\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -2909.275479151582 Explore P: 0.0210 SOC: 1.0000 Cumulative_SOC_deviation: 282.6940 Fuel Consumption: 82.3353\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -1580.4991689118633 Explore P: 0.0207 SOC: 0.9253 Cumulative_SOC_deviation: 151.0036 Fuel Consumption: 70.4634\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -1529.518734478713 Explore P: 0.0204 SOC: 0.6264 Cumulative_SOC_deviation: 148.2945 Fuel Consumption: 46.5737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -1016.9143317119649 Explore P: 0.0201 SOC: 0.8281 Cumulative_SOC_deviation: 95.5253 Fuel Consumption: 61.6611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -1694.9584541992156 Explore P: 0.0198 SOC: 0.9431 Cumulative_SOC_deviation: 162.3314 Fuel Consumption: 71.6447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -1454.8285178797976 Explore P: 0.0196 SOC: 0.7567 Cumulative_SOC_deviation: 139.5920 Fuel Consumption: 58.9088\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -1287.4290842029131 Explore P: 0.0193 SOC: 0.6580 Cumulative_SOC_deviation: 123.8782 Fuel Consumption: 48.6475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -842.4449642192114 Explore P: 0.0191 SOC: 0.7709 Cumulative_SOC_deviation: 78.5755 Fuel Consumption: 56.6901\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -1191.552458180588 Explore P: 0.0188 SOC: 0.8555 Cumulative_SOC_deviation: 112.7763 Fuel Consumption: 63.7896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -834.3934602225709 Explore P: 0.0186 SOC: 0.5907 Cumulative_SOC_deviation: 79.1553 Fuel Consumption: 42.8403\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -1547.2138274821798 Explore P: 0.0183 SOC: 0.9152 Cumulative_SOC_deviation: 147.6923 Fuel Consumption: 70.2906\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -716.0471048399558 Explore P: 0.0181 SOC: 0.7281 Cumulative_SOC_deviation: 66.2472 Fuel Consumption: 53.5754\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -873.1800530130511 Explore P: 0.0179 SOC: 0.5165 Cumulative_SOC_deviation: 83.5648 Fuel Consumption: 37.5317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -2340.946248310666 Explore P: 0.0177 SOC: 1.0000 Cumulative_SOC_deviation: 226.3586 Fuel Consumption: 77.3607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -1549.1377842975312 Explore P: 0.0175 SOC: 0.5730 Cumulative_SOC_deviation: 150.5723 Fuel Consumption: 43.4144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -1606.178708068124 Explore P: 0.0173 SOC: 0.5233 Cumulative_SOC_deviation: 156.8173 Fuel Consumption: 38.0061\n",
      "Available condition is not avail... SOC: 0.996180032211222\n",
      "Episode: 180 Total reward: -2315.0003226231397 Explore P: 0.0171 SOC: 0.9962 Cumulative_SOC_deviation: 223.6063 Fuel Consumption: 78.9377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -1493.8562681290416 Explore P: 0.0169 SOC: 0.5664 Cumulative_SOC_deviation: 145.2513 Fuel Consumption: 41.3432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -1350.592322833436 Explore P: 0.0167 SOC: 0.8723 Cumulative_SOC_deviation: 128.5043 Fuel Consumption: 65.5492\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -1112.5996782603597 Explore P: 0.0165 SOC: 0.7155 Cumulative_SOC_deviation: 105.9215 Fuel Consumption: 53.3849\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -2607.238477939714 Explore P: 0.0163 SOC: 0.9943 Cumulative_SOC_deviation: 253.0360 Fuel Consumption: 76.8783\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -1442.109766604074 Explore P: 0.0162 SOC: 0.9206 Cumulative_SOC_deviation: 137.1268 Fuel Consumption: 70.8420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -720.7133936470788 Explore P: 0.0160 SOC: 0.6150 Cumulative_SOC_deviation: 67.6149 Fuel Consumption: 44.5648\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -2590.6555558438786 Explore P: 0.0158 SOC: 0.5340 Cumulative_SOC_deviation: 255.0646 Fuel Consumption: 40.0094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -1596.191625448327 Explore P: 0.0157 SOC: 0.8735 Cumulative_SOC_deviation: 153.0257 Fuel Consumption: 65.9343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -1812.2345680306362 Explore P: 0.0155 SOC: 0.9416 Cumulative_SOC_deviation: 173.9945 Fuel Consumption: 72.2894\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -1743.8275133760073 Explore P: 0.0154 SOC: 0.4894 Cumulative_SOC_deviation: 170.7518 Fuel Consumption: 36.3094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -2393.654886121757 Explore P: 0.0152 SOC: 0.5162 Cumulative_SOC_deviation: 235.5053 Fuel Consumption: 38.6020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -2053.8355016391065 Explore P: 0.0151 SOC: 0.5699 Cumulative_SOC_deviation: 201.1502 Fuel Consumption: 42.3340\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -1781.70268821534 Explore P: 0.0150 SOC: 0.9660 Cumulative_SOC_deviation: 170.8065 Fuel Consumption: 73.6374\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -1138.878223662017 Explore P: 0.0148 SOC: 0.8912 Cumulative_SOC_deviation: 107.1339 Fuel Consumption: 67.5393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -1268.3729072144795 Explore P: 0.0147 SOC: 0.8620 Cumulative_SOC_deviation: 120.3516 Fuel Consumption: 64.8569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -830.3449480820664 Explore P: 0.0146 SOC: 0.6348 Cumulative_SOC_deviation: 78.3787 Fuel Consumption: 46.5580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -936.3102822158921 Explore P: 0.0144 SOC: 0.6520 Cumulative_SOC_deviation: 88.7875 Fuel Consumption: 48.4356\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -1043.60721889644 Explore P: 0.0143 SOC: 0.6203 Cumulative_SOC_deviation: 99.8133 Fuel Consumption: 45.4740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -969.6829611769284 Explore P: 0.0142 SOC: 0.6270 Cumulative_SOC_deviation: 92.3708 Fuel Consumption: 45.9747\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -769.8102545595249 Explore P: 0.0141 SOC: 0.7326 Cumulative_SOC_deviation: 71.5773 Fuel Consumption: 54.0371\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
