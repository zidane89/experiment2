{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_pb_91_150.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 150\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    if not os.path.exists(root): \n",
    "        os.makedirs(root)\n",
    "    \n",
    "    actor_model.save_weights(\"./{}/actor_model.h5\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model.h5\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor.h5\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic.h5\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.090\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -5462.017640284397 SOC: 1.0000 Cumulative_SOC_deviation: 431.4512 Fuel Consumption: 1147.5059\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 20.894\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -5483.650291911684 SOC: 1.0000 Cumulative_SOC_deviation: 429.6326 Fuel Consumption: 1187.3242\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.074\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -5331.746805329948 SOC: 1.0000 Cumulative_SOC_deviation: 425.6030 Fuel Consumption: 1075.7172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.828\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -5248.238803606659 SOC: 1.0000 Cumulative_SOC_deviation: 417.3219 Fuel Consumption: 1075.0198\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 108.289\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -5190.254661642783 SOC: 1.0000 Cumulative_SOC_deviation: 416.0447 Fuel Consumption: 1029.8080\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.816\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -5221.738665014846 SOC: 1.0000 Cumulative_SOC_deviation: 420.0287 Fuel Consumption: 1021.4521\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 108.010\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -5013.371477619537 SOC: 1.0000 Cumulative_SOC_deviation: 402.6201 Fuel Consumption: 987.1703\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 97.818\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -4838.806663513056 SOC: 1.0000 Cumulative_SOC_deviation: 393.6115 Fuel Consumption: 902.6918\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.798\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -4963.855159088353 SOC: 1.0000 Cumulative_SOC_deviation: 401.1641 Fuel Consumption: 952.2144\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.287\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -4899.062857812667 SOC: 1.0000 Cumulative_SOC_deviation: 397.4061 Fuel Consumption: 925.0015\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.380\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -4768.9787768119995 SOC: 1.0000 Cumulative_SOC_deviation: 391.1397 Fuel Consumption: 857.5821\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.197\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -4672.314555254496 SOC: 1.0000 Cumulative_SOC_deviation: 383.2262 Fuel Consumption: 840.0530\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.866\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -4825.915204211252 SOC: 1.0000 Cumulative_SOC_deviation: 394.7637 Fuel Consumption: 878.2778\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.699\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -4472.824050260153 SOC: 1.0000 Cumulative_SOC_deviation: 368.8484 Fuel Consumption: 784.3401\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.999\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -4480.839760187771 SOC: 1.0000 Cumulative_SOC_deviation: 367.5620 Fuel Consumption: 805.2197\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.958\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -4344.363044999046 SOC: 1.0000 Cumulative_SOC_deviation: 358.9994 Fuel Consumption: 754.3691\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.500\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -4400.159697596982 SOC: 1.0000 Cumulative_SOC_deviation: 362.4870 Fuel Consumption: 775.2896\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.342\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -4432.811626846927 SOC: 1.0000 Cumulative_SOC_deviation: 366.9024 Fuel Consumption: 763.7874\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.100\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -3964.7512995095285 SOC: 1.0000 Cumulative_SOC_deviation: 329.9968 Fuel Consumption: 664.7831\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.164\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -4055.3668301297107 SOC: 1.0000 Cumulative_SOC_deviation: 337.9531 Fuel Consumption: 675.8358\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.308\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -3836.497263831449 SOC: 1.0000 Cumulative_SOC_deviation: 319.5944 Fuel Consumption: 640.5529\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.318\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -3905.8675985674304 SOC: 1.0000 Cumulative_SOC_deviation: 324.8644 Fuel Consumption: 657.2239\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.353\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -3969.4974326220545 SOC: 1.0000 Cumulative_SOC_deviation: 333.0313 Fuel Consumption: 639.1841\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.259\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -3626.9122897044745 SOC: 1.0000 Cumulative_SOC_deviation: 302.5789 Fuel Consumption: 601.1228\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.560\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -3831.86640527669 SOC: 1.0000 Cumulative_SOC_deviation: 318.5266 Fuel Consumption: 646.6003\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.345\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -3812.309156477953 SOC: 1.0000 Cumulative_SOC_deviation: 320.3954 Fuel Consumption: 608.3551\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.117\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -3601.4349456609284 SOC: 1.0000 Cumulative_SOC_deviation: 298.9668 Fuel Consumption: 611.7669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.474\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -3566.06842142781 SOC: 1.0000 Cumulative_SOC_deviation: 298.0840 Fuel Consumption: 585.2282\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.054\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -3493.1398758900996 SOC: 1.0000 Cumulative_SOC_deviation: 291.5634 Fuel Consumption: 577.5056\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.139\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -3152.8011023250647 SOC: 0.9850 Cumulative_SOC_deviation: 264.0365 Fuel Consumption: 512.4357\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.714\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -3230.4943767689356 SOC: 0.9955 Cumulative_SOC_deviation: 270.5596 Fuel Consumption: 524.8980\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.660\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -3096.4018176663167 SOC: 0.9877 Cumulative_SOC_deviation: 257.9022 Fuel Consumption: 517.3798\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.140\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -3133.8186962485224 SOC: 0.9858 Cumulative_SOC_deviation: 261.7174 Fuel Consumption: 516.6443\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.790\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2634.348202204066 SOC: 0.9267 Cumulative_SOC_deviation: 219.2723 Fuel Consumption: 441.6249\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.527\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -2702.3517800043833 SOC: 0.9351 Cumulative_SOC_deviation: 224.7856 Fuel Consumption: 454.4958\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.406\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -2729.840771530773 SOC: 0.9431 Cumulative_SOC_deviation: 226.6703 Fuel Consumption: 463.1378\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.332\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -2447.4070668380505 SOC: 0.9017 Cumulative_SOC_deviation: 203.4650 Fuel Consumption: 412.7571\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.838\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -2515.568448447718 SOC: 0.9041 Cumulative_SOC_deviation: 209.8398 Fuel Consumption: 417.1700\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.612\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -2620.6181450845525 SOC: 0.9257 Cumulative_SOC_deviation: 217.7890 Fuel Consumption: 442.7281\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.442\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -2395.5268570331787 SOC: 0.8910 Cumulative_SOC_deviation: 199.4721 Fuel Consumption: 400.8055\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.737\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -2373.874892069173 SOC: 0.8902 Cumulative_SOC_deviation: 197.3805 Fuel Consumption: 400.0700\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.745\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -2242.1837827295576 SOC: 0.8782 Cumulative_SOC_deviation: 185.7375 Fuel Consumption: 384.8087\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.751\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -2287.4244943395297 SOC: 0.8852 Cumulative_SOC_deviation: 189.4342 Fuel Consumption: 393.0829\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.353\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -2112.143895146092 SOC: 0.8743 Cumulative_SOC_deviation: 173.0461 Fuel Consumption: 381.6829\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.424\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -2293.390764213826 SOC: 0.8832 Cumulative_SOC_deviation: 190.2147 Fuel Consumption: 391.2442\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.929\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -1948.0618818973692 SOC: 0.8396 Cumulative_SOC_deviation: 160.9956 Fuel Consumption: 338.1055\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.330\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -1777.5106234805135 SOC: 0.8252 Cumulative_SOC_deviation: 145.4115 Fuel Consumption: 323.3958\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.528\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -1713.1227865238443 SOC: 0.8106 Cumulative_SOC_deviation: 140.9033 Fuel Consumption: 304.0893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.444\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -1872.4245645196447 SOC: 0.8268 Cumulative_SOC_deviation: 154.7374 Fuel Consumption: 325.0506\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.070\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -1816.4707388015468 SOC: 0.8188 Cumulative_SOC_deviation: 150.0981 Fuel Consumption: 315.4893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.833\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -1668.5973704905389 SOC: 0.8133 Cumulative_SOC_deviation: 136.1015 Fuel Consumption: 307.5829\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.297\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -1839.4586172195152 SOC: 0.8190 Cumulative_SOC_deviation: 152.3050 Fuel Consumption: 316.4087\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.291\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -1577.4139043748949 SOC: 0.7947 Cumulative_SOC_deviation: 129.0425 Fuel Consumption: 286.9893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.375\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -1740.0715726505516 SOC: 0.8156 Cumulative_SOC_deviation: 142.8995 Fuel Consumption: 311.0764\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.408\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -1700.2591091413592 SOC: 0.8091 Cumulative_SOC_deviation: 139.6721 Fuel Consumption: 303.5377\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.543\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -1512.0084535456215 SOC: 0.7869 Cumulative_SOC_deviation: 123.4948 Fuel Consumption: 277.0603\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.865\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -1258.9503664873619 SOC: 0.7582 Cumulative_SOC_deviation: 101.5906 Fuel Consumption: 243.0441\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.077\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -1416.2072864973732 SOC: 0.7854 Cumulative_SOC_deviation: 113.8963 Fuel Consumption: 277.2441\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.596\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -1411.7961035945982 SOC: 0.7769 Cumulative_SOC_deviation: 114.5768 Fuel Consumption: 266.0280\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.013\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -1208.9995442054785 SOC: 0.7574 Cumulative_SOC_deviation: 96.5220 Fuel Consumption: 243.7796\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.743\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -1402.0535819075299 SOC: 0.7750 Cumulative_SOC_deviation: 113.6945 Fuel Consumption: 265.1086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.782\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -1275.3816353838838 SOC: 0.7561 Cumulative_SOC_deviation: 103.3441 Fuel Consumption: 241.9409\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.809\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -1200.5742922388533 SOC: 0.7555 Cumulative_SOC_deviation: 95.9553 Fuel Consumption: 241.0215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.410\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -1123.9901191332724 SOC: 0.7478 Cumulative_SOC_deviation: 89.1978 Fuel Consumption: 232.0119\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.235\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -1172.4276192587065 SOC: 0.7502 Cumulative_SOC_deviation: 93.7106 Fuel Consumption: 235.3215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.086\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -1023.6113247042747 SOC: 0.7320 Cumulative_SOC_deviation: 81.1274 Fuel Consumption: 212.3377\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.326\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -1033.5400832682851 SOC: 0.7154 Cumulative_SOC_deviation: 83.8302 Fuel Consumption: 195.2376\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.839\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -779.8003616841131 SOC: 0.7115 Cumulative_SOC_deviation: 59.1366 Fuel Consumption: 188.4344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.058\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -936.901688638355 SOC: 0.7163 Cumulative_SOC_deviation: 74.3503 Fuel Consumption: 193.3989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.632\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -1024.6274422449462 SOC: 0.7266 Cumulative_SOC_deviation: 81.6519 Fuel Consumption: 208.1086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.125\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -947.278514325118 SOC: 0.7182 Cumulative_SOC_deviation: 75.0202 Fuel Consumption: 197.0764\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.730\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -951.373412531759 SOC: 0.7191 Cumulative_SOC_deviation: 75.2642 Fuel Consumption: 198.7312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.261\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -892.0790336395722 SOC: 0.7114 Cumulative_SOC_deviation: 70.3461 Fuel Consumption: 188.6183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.955\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -682.8565137668036 SOC: 0.6881 Cumulative_SOC_deviation: 52.1267 Fuel Consumption: 161.5892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.570\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -795.6954932522206 SOC: 0.6970 Cumulative_SOC_deviation: 62.3074 Fuel Consumption: 172.6215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.484\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -722.806797739235 SOC: 0.6959 Cumulative_SOC_deviation: 55.1656 Fuel Consumption: 171.1505\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.745\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -782.4539492773413 SOC: 0.7023 Cumulative_SOC_deviation: 60.3029 Fuel Consumption: 179.4247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.274\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -612.6554439559936 SOC: 0.6763 Cumulative_SOC_deviation: 46.5592 Fuel Consumption: 147.0634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.900\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -758.8594127658815 SOC: 0.6937 Cumulative_SOC_deviation: 59.0651 Fuel Consumption: 168.2086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.430\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -663.5138079592821 SOC: 0.6833 Cumulative_SOC_deviation: 50.7257 Fuel Consumption: 156.2570\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.745\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -739.6827323772253 SOC: 0.6891 Cumulative_SOC_deviation: 57.6255 Fuel Consumption: 163.4279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.656\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -589.7653163030589 SOC: 0.6814 Cumulative_SOC_deviation: 43.5531 Fuel Consumption: 154.2344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.979\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -610.6967169085262 SOC: 0.6756 Cumulative_SOC_deviation: 46.3266 Fuel Consumption: 147.4312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.119\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -563.1933101655624 SOC: 0.6779 Cumulative_SOC_deviation: 41.2820 Fuel Consumption: 150.3731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.973\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -468.4021562857057 SOC: 0.6699 Cumulative_SOC_deviation: 32.7406 Fuel Consumption: 140.9957\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.604\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -506.29036149961223 SOC: 0.6648 Cumulative_SOC_deviation: 37.0811 Fuel Consumption: 135.4795\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.941\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -581.8097262241813 SOC: 0.6756 Cumulative_SOC_deviation: 43.3827 Fuel Consumption: 147.9828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.948\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -597.8403254825616 SOC: 0.6748 Cumulative_SOC_deviation: 45.0593 Fuel Consumption: 147.2473\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.722\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -393.05411340929714 SOC: 0.6588 Cumulative_SOC_deviation: 26.4378 Fuel Consumption: 128.6763\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.696\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -491.44921637737167 SOC: 0.6674 Cumulative_SOC_deviation: 35.3212 Fuel Consumption: 138.2376\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.774\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -344.7343819317026 SOC: 0.6511 Cumulative_SOC_deviation: 22.5252 Fuel Consumption: 119.4828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.679\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -377.20890782682 SOC: 0.6499 Cumulative_SOC_deviation: 25.9197 Fuel Consumption: 118.0118\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.258\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -385.65325664878736 SOC: 0.6511 Cumulative_SOC_deviation: 26.6170 Fuel Consumption: 119.4828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.121\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -287.4571148793231 SOC: 0.6369 Cumulative_SOC_deviation: 18.4523 Fuel Consumption: 102.9344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.878\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -355.8819720730328 SOC: 0.6474 Cumulative_SOC_deviation: 24.0261 Fuel Consumption: 115.6215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.231\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -326.7068796472234 SOC: 0.6424 Cumulative_SOC_deviation: 21.7337 Fuel Consumption: 109.3699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.674\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -294.21507070156747 SOC: 0.6365 Cumulative_SOC_deviation: 19.1832 Fuel Consumption: 102.3828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.997\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -305.53890113249355 SOC: 0.6394 Cumulative_SOC_deviation: 19.9295 Fuel Consumption: 106.2440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.030\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -313.10694029627666 SOC: 0.6444 Cumulative_SOC_deviation: 20.1531 Fuel Consumption: 111.5763\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.241\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -333.586390737133 SOC: 0.6476 Cumulative_SOC_deviation: 21.8517 Fuel Consumption: 115.0699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.731\n",
      "Episode: 101 Exploration P: 0.0718 Total reward: -207.92833952759747 SOC: 0.6267 Cumulative_SOC_deviation: 11.6762 Fuel Consumption: 91.1666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.889\n",
      "Episode: 102 Exploration P: 0.0701 Total reward: -182.83450342947947 SOC: 0.6238 Cumulative_SOC_deviation: 9.4978 Fuel Consumption: 87.8569\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.789\n",
      "Episode: 103 Exploration P: 0.0685 Total reward: -240.85536141920625 SOC: 0.6302 Cumulative_SOC_deviation: 14.5460 Fuel Consumption: 95.3957\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.112\n",
      "Episode: 104 Exploration P: 0.0669 Total reward: -191.9488674251004 SOC: 0.6233 Cumulative_SOC_deviation: 10.4460 Fuel Consumption: 87.4892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.999\n",
      "Episode: 105 Exploration P: 0.0654 Total reward: -207.78374314152725 SOC: 0.6306 Cumulative_SOC_deviation: 11.2388 Fuel Consumption: 95.3957\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.867\n",
      "Episode: 106 Exploration P: 0.0639 Total reward: -258.9345088325552 SOC: 0.6356 Cumulative_SOC_deviation: 15.7471 Fuel Consumption: 101.4634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.183\n",
      "Episode: 107 Exploration P: 0.0624 Total reward: -282.2067920141404 SOC: 0.6354 Cumulative_SOC_deviation: 18.0743 Fuel Consumption: 101.4634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.982\n",
      "Episode: 108 Exploration P: 0.0610 Total reward: -211.77979220071174 SOC: 0.6237 Cumulative_SOC_deviation: 12.4658 Fuel Consumption: 87.1215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.303\n",
      "Episode: 109 Exploration P: 0.0596 Total reward: -316.1426899123758 SOC: 0.6414 Cumulative_SOC_deviation: 20.7508 Fuel Consumption: 108.6344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.028\n",
      "Episode: 110 Exploration P: 0.0583 Total reward: -194.89146984451418 SOC: 0.6260 Cumulative_SOC_deviation: 10.4093 Fuel Consumption: 90.7989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.329\n",
      "Episode: 111 Exploration P: 0.0570 Total reward: -163.49438852921273 SOC: 0.6168 Cumulative_SOC_deviation: 8.4096 Fuel Consumption: 79.3989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.539\n",
      "Episode: 112 Exploration P: 0.0557 Total reward: -250.61040428054733 SOC: 0.6344 Cumulative_SOC_deviation: 14.9882 Fuel Consumption: 100.7279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.274\n",
      "Episode: 113 Exploration P: 0.0545 Total reward: -286.5378063181492 SOC: 0.6386 Cumulative_SOC_deviation: 18.0662 Fuel Consumption: 105.8763\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.885\n",
      "Episode: 114 Exploration P: 0.0533 Total reward: -165.30236434238068 SOC: 0.6196 Cumulative_SOC_deviation: 8.2594 Fuel Consumption: 82.7085\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.076\n",
      "Episode: 115 Exploration P: 0.0521 Total reward: -189.80862931958734 SOC: 0.6270 Cumulative_SOC_deviation: 9.8274 Fuel Consumption: 91.5344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.349\n",
      "Episode: 116 Exploration P: 0.0510 Total reward: -178.53159456159153 SOC: 0.6214 Cumulative_SOC_deviation: 9.3984 Fuel Consumption: 84.5473\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.922\n",
      "Episode: 117 Exploration P: 0.0498 Total reward: -150.36635974387897 SOC: 0.6122 Cumulative_SOC_deviation: 7.5380 Fuel Consumption: 74.9860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.700\n",
      "Episode: 118 Exploration P: 0.0488 Total reward: -163.2078150873051 SOC: 0.6166 Cumulative_SOC_deviation: 8.4177 Fuel Consumption: 79.0311\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.730\n",
      "Episode: 119 Exploration P: 0.0477 Total reward: -130.8471484146925 SOC: 0.6113 Cumulative_SOC_deviation: 5.7516 Fuel Consumption: 73.3311\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.584\n",
      "Episode: 120 Exploration P: 0.0467 Total reward: -196.9144352481104 SOC: 0.6243 Cumulative_SOC_deviation: 10.8690 Fuel Consumption: 88.2247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.060\n",
      "Episode: 121 Exploration P: 0.0457 Total reward: -136.7481322147135 SOC: 0.6071 Cumulative_SOC_deviation: 6.7830 Fuel Consumption: 68.9182\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.007\n",
      "Episode: 122 Exploration P: 0.0447 Total reward: -162.01525372517918 SOC: 0.6174 Cumulative_SOC_deviation: 8.0962 Fuel Consumption: 81.0537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.878\n",
      "Episode: 123 Exploration P: 0.0438 Total reward: -166.90364271130375 SOC: 0.6027 Cumulative_SOC_deviation: 10.3134 Fuel Consumption: 63.7698\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.512\n",
      "Episode: 124 Exploration P: 0.0429 Total reward: -136.12525531238833 SOC: 0.6131 Cumulative_SOC_deviation: 6.0404 Fuel Consumption: 75.7214\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.734\n",
      "Episode: 125 Exploration P: 0.0420 Total reward: -111.82179180642 SOC: 0.6053 Cumulative_SOC_deviation: 4.5294 Fuel Consumption: 66.5279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.381\n",
      "Episode: 126 Exploration P: 0.0411 Total reward: -164.48802494384458 SOC: 0.6091 Cumulative_SOC_deviation: 9.3363 Fuel Consumption: 71.1247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.867\n",
      "Episode: 127 Exploration P: 0.0403 Total reward: -155.3211916819235 SOC: 0.6030 Cumulative_SOC_deviation: 9.1735 Fuel Consumption: 63.5860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.059\n",
      "Episode: 128 Exploration P: 0.0395 Total reward: -144.2253717210151 SOC: 0.6086 Cumulative_SOC_deviation: 7.3468 Fuel Consumption: 70.7569\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.916\n",
      "Episode: 129 Exploration P: 0.0387 Total reward: -118.36626591264461 SOC: 0.6091 Cumulative_SOC_deviation: 4.7242 Fuel Consumption: 71.1247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.209\n",
      "Episode: 130 Exploration P: 0.0379 Total reward: -148.29084036581762 SOC: 0.5974 Cumulative_SOC_deviation: 9.0956 Fuel Consumption: 57.3343\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.932\n",
      "Episode: 131 Exploration P: 0.0371 Total reward: -141.60315776318745 SOC: 0.5994 Cumulative_SOC_deviation: 8.1511 Fuel Consumption: 60.0924\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.812\n",
      "Episode: 132 Exploration P: 0.0364 Total reward: -115.56641087008629 SOC: 0.6092 Cumulative_SOC_deviation: 4.4258 Fuel Consumption: 71.3085\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.759\n",
      "Episode: 133 Exploration P: 0.0357 Total reward: -248.7202219828113 SOC: 0.5872 Cumulative_SOC_deviation: 20.3337 Fuel Consumption: 45.3827\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.942\n",
      "Episode: 134 Exploration P: 0.0350 Total reward: -165.72397968392264 SOC: 0.5986 Cumulative_SOC_deviation: 10.6183 Fuel Consumption: 59.5408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.831\n",
      "Episode: 135 Exploration P: 0.0343 Total reward: -144.09419681320037 SOC: 0.5987 Cumulative_SOC_deviation: 8.5105 Fuel Consumption: 58.9892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.098\n",
      "Episode: 136 Exploration P: 0.0336 Total reward: -135.8547089207651 SOC: 0.6022 Cumulative_SOC_deviation: 7.3188 Fuel Consumption: 62.6666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.866\n",
      "Episode: 137 Exploration P: 0.0330 Total reward: -185.75577505144597 SOC: 0.5967 Cumulative_SOC_deviation: 12.8973 Fuel Consumption: 56.7827\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.716\n",
      "Episode: 138 Exploration P: 0.0324 Total reward: -140.55216742671166 SOC: 0.6042 Cumulative_SOC_deviation: 7.4760 Fuel Consumption: 65.7924\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.121\n",
      "Episode: 139 Exploration P: 0.0318 Total reward: -147.03346893890134 SOC: 0.6017 Cumulative_SOC_deviation: 8.4735 Fuel Consumption: 62.2989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.838\n",
      "Episode: 140 Exploration P: 0.0312 Total reward: -120.98116046552252 SOC: 0.6051 Cumulative_SOC_deviation: 5.4821 Fuel Consumption: 66.1602\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.794\n",
      "Episode: 141 Exploration P: 0.0306 Total reward: -183.90186382933265 SOC: 0.5966 Cumulative_SOC_deviation: 12.7303 Fuel Consumption: 56.5989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.737\n",
      "Episode: 142 Exploration P: 0.0301 Total reward: -191.87004155242334 SOC: 0.6001 Cumulative_SOC_deviation: 13.1042 Fuel Consumption: 60.8279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.743\n",
      "Episode: 143 Exploration P: 0.0295 Total reward: -186.0947376467273 SOC: 0.5929 Cumulative_SOC_deviation: 13.3725 Fuel Consumption: 52.3698\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.803\n",
      "Episode: 144 Exploration P: 0.0290 Total reward: -149.6710996163614 SOC: 0.6012 Cumulative_SOC_deviation: 8.7740 Fuel Consumption: 61.9311\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.800\n",
      "Episode: 145 Exploration P: 0.0285 Total reward: -199.24115516259076 SOC: 0.5953 Cumulative_SOC_deviation: 14.3746 Fuel Consumption: 55.4956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.859\n",
      "Episode: 146 Exploration P: 0.0280 Total reward: -160.32163586044513 SOC: 0.6018 Cumulative_SOC_deviation: 9.7471 Fuel Consumption: 62.8505\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.799\n",
      "Episode: 147 Exploration P: 0.0275 Total reward: -173.58058027961778 SOC: 0.5981 Cumulative_SOC_deviation: 11.5143 Fuel Consumption: 58.4376\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.041\n",
      "Episode: 148 Exploration P: 0.0270 Total reward: -189.8993103093654 SOC: 0.5939 Cumulative_SOC_deviation: 13.6426 Fuel Consumption: 53.4730\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.351\n",
      "Episode: 149 Exploration P: 0.0265 Total reward: -163.5144275242063 SOC: 0.6002 Cumulative_SOC_deviation: 10.2135 Fuel Consumption: 61.3795\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.585\n",
      "Episode: 150 Exploration P: 0.0261 Total reward: -189.50124477464166 SOC: 0.5911 Cumulative_SOC_deviation: 13.8419 Fuel Consumption: 51.0827\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc1 in position 186: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36msave_v2\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1927\u001b[1;33m         shape_and_slices, tensors)\n\u001b[0m\u001b[0;32m   1928\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cc94108501b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DDPG2_trial{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_actor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_critic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     results_dict[trial + 1] = {\n",
      "\u001b[1;32m<ipython-input-12-f443134d2788>\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(actor_model, critic_model, target_actor, target_critic, root)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/critic_model_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtarget_actor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/target_actor_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtarget_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/target_critic_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model is saved..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1088\u001b[0m              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\n\u001b[0;32m   1089\u001b[0m             % (optimizer,))\n\u001b[1;32m-> 1090\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m       \u001b[1;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m       checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[1;32m-> 1155\u001b[1;33m         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\n\u001b[0m\u001b[0;32m   1156\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1102\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m# _SingleDeviceSaver will use the CPU device when necessary, but initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# read operations should be placed on the SaveableObject's device.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0msharded_saves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharded_saves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mtensor_slices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_slices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36msave_v2\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[0;32m   1931\u001b[0m         return save_v2_eager_fallback(\n\u001b[0;32m   1932\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_and_slices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1933\u001b[1;33m             ctx=_ctx)\n\u001b[0m\u001b[0;32m   1934\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1935\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36msave_v2_eager_fallback\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name, ctx)\u001b[0m\n\u001b[0;32m   1968\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"dtypes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m   _result = _execute.execute(b\"SaveV2\", 0, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m-> 1970\u001b[1;33m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m   1971\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc1 in position 186: invalid start byte"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 1\n",
    "reward_factor = 10\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "    \n",
    "    root = \"DDPG2_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
