{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_pb_91_150.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00005\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.089\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -5470.465242248691 SOC: 1.0000 Cumulative_SOC_deviation: 431.4399 Fuel Consumption: 1156.0661 Mean: 2.9702, STD: 9.3584\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 15.024\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -5388.67225125783 SOC: 1.0000 Cumulative_SOC_deviation: 424.9155 Fuel Consumption: 1139.5177 Mean: 2.9702, STD: 9.3652\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 50.642\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -5283.692164785715 SOC: 1.0000 Cumulative_SOC_deviation: 421.9781 Fuel Consumption: 1063.9113 Mean: 2.9702, STD: 9.3679\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.977\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -5283.6474566973175 SOC: 1.0000 Cumulative_SOC_deviation: 422.4747 Fuel Consumption: 1058.9005 Mean: 2.9702, STD: 9.3693\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.324\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -5222.313744117914 SOC: 1.0000 Cumulative_SOC_deviation: 415.8142 Fuel Consumption: 1064.1715 Mean: 2.9702, STD: 9.3702\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.794\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -5072.069739475623 SOC: 1.0000 Cumulative_SOC_deviation: 408.1120 Fuel Consumption: 990.9499 Mean: 2.9702, STD: 9.3708\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.950\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -4991.129292819356 SOC: 1.0000 Cumulative_SOC_deviation: 405.1990 Fuel Consumption: 939.1391 Mean: 2.9702, STD: 9.3713\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.241\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -4999.514537223656 SOC: 1.0000 Cumulative_SOC_deviation: 403.5573 Fuel Consumption: 963.9413 Mean: 2.9702, STD: 9.3717\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.558\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -4953.770305228556 SOC: 1.0000 Cumulative_SOC_deviation: 401.2404 Fuel Consumption: 941.3660 Mean: 2.9702, STD: 9.3719\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.597\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -4861.308652696225 SOC: 1.0000 Cumulative_SOC_deviation: 395.9352 Fuel Consumption: 901.9563 Mean: 2.9702, STD: 9.3722\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.403\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -4659.891624737941 SOC: 1.0000 Cumulative_SOC_deviation: 382.3434 Fuel Consumption: 836.4573 Mean: 2.9702, STD: 9.3724\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.502\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -4650.280550954533 SOC: 1.0000 Cumulative_SOC_deviation: 380.3506 Fuel Consumption: 846.7746 Mean: 2.9702, STD: 9.3725\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.357\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -4792.780023022341 SOC: 1.0000 Cumulative_SOC_deviation: 392.3512 Fuel Consumption: 869.2681 Mean: 2.9702, STD: 9.3727\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.275\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -4482.881878699995 SOC: 1.0000 Cumulative_SOC_deviation: 369.6601 Fuel Consumption: 786.2810 Mean: 2.9702, STD: 9.3728\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.366\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -4435.160296396001 SOC: 1.0000 Cumulative_SOC_deviation: 365.2843 Fuel Consumption: 782.3175 Mean: 2.9702, STD: 9.3729\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.478\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -4336.644924773247 SOC: 1.0000 Cumulative_SOC_deviation: 358.2460 Fuel Consumption: 754.1853 Mean: 2.9702, STD: 9.3730\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.811\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -4271.270850500055 SOC: 1.0000 Cumulative_SOC_deviation: 353.2265 Fuel Consumption: 739.0057 Mean: 2.9702, STD: 9.3731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.207\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -4064.331877887345 SOC: 1.0000 Cumulative_SOC_deviation: 334.9618 Fuel Consumption: 714.7143 Mean: 2.9702, STD: 9.3731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.879\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -4353.059796869661 SOC: 1.0000 Cumulative_SOC_deviation: 361.9693 Fuel Consumption: 733.3670 Mean: 2.9702, STD: 9.3732\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.337\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -4236.782654654024 SOC: 1.0000 Cumulative_SOC_deviation: 352.1905 Fuel Consumption: 714.8777 Mean: 2.9702, STD: 9.3733\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.148\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -3974.278478361097 SOC: 1.0000 Cumulative_SOC_deviation: 330.9209 Fuel Consumption: 665.0691 Mean: 2.9702, STD: 9.3733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.771\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -4088.525633582478 SOC: 1.0000 Cumulative_SOC_deviation: 343.4080 Fuel Consumption: 654.4454 Mean: 2.9702, STD: 9.3734\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.239\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -3774.8118224462596 SOC: 1.0000 Cumulative_SOC_deviation: 315.6875 Fuel Consumption: 617.9368 Mean: 2.9702, STD: 9.3734\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.870\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -3721.013545377402 SOC: 1.0000 Cumulative_SOC_deviation: 309.4047 Fuel Consumption: 626.9669 Mean: 2.9702, STD: 9.3735\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.877\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -3769.1163684771473 SOC: 1.0000 Cumulative_SOC_deviation: 312.6541 Fuel Consumption: 642.5755 Mean: 2.9702, STD: 9.3735\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.029\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -3243.755506390138 SOC: 1.0000 Cumulative_SOC_deviation: 268.5434 Fuel Consumption: 558.3217 Mean: 2.9702, STD: 9.3735\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.271\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -3609.1997094435374 SOC: 1.0000 Cumulative_SOC_deviation: 302.8507 Fuel Consumption: 580.6927 Mean: 2.9702, STD: 9.3736\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.764\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -3422.3662594202888 SOC: 1.0000 Cumulative_SOC_deviation: 284.7435 Fuel Consumption: 574.9314 Mean: 2.9702, STD: 9.3736\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.011\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -3486.8929565595136 SOC: 1.0000 Cumulative_SOC_deviation: 291.6068 Fuel Consumption: 570.8249 Mean: 2.9702, STD: 9.3736\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.696\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -3194.835370754397 SOC: 1.0000 Cumulative_SOC_deviation: 266.3951 Fuel Consumption: 530.8841 Mean: 2.9702, STD: 9.3737\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.540\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -2993.3798701954743 SOC: 0.9790 Cumulative_SOC_deviation: 248.8381 Fuel Consumption: 504.9991 Mean: 2.9702, STD: 9.3737\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.037\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -3081.1939350064354 SOC: 0.9814 Cumulative_SOC_deviation: 257.3375 Fuel Consumption: 507.8185 Mean: 2.9702, STD: 9.3737\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.932\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -3155.407272940107 SOC: 0.9846 Cumulative_SOC_deviation: 264.1031 Fuel Consumption: 514.3765 Mean: 2.9702, STD: 9.3737\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.032\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2927.842201151522 SOC: 0.9540 Cumulative_SOC_deviation: 245.3161 Fuel Consumption: 474.6808 Mean: 2.9702, STD: 9.3738\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.954\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -2723.1210299921877 SOC: 0.9391 Cumulative_SOC_deviation: 226.4764 Fuel Consumption: 458.3571 Mean: 2.9702, STD: 9.3738\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.209\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -2789.0889885439615 SOC: 0.9588 Cumulative_SOC_deviation: 230.7380 Fuel Consumption: 481.7088 Mean: 2.9702, STD: 9.3738\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.453\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -2285.8231224229635 SOC: 0.8932 Cumulative_SOC_deviation: 188.4650 Fuel Consumption: 401.1732 Mean: 2.9702, STD: 9.3738\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.048\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -2497.4276598877677 SOC: 0.9068 Cumulative_SOC_deviation: 207.7867 Fuel Consumption: 419.5603 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.345\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -2699.633511456247 SOC: 0.9326 Cumulative_SOC_deviation: 224.8325 Fuel Consumption: 451.3088 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.149\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -2492.542952544229 SOC: 0.9015 Cumulative_SOC_deviation: 207.8315 Fuel Consumption: 414.2281 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.750\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -2535.082467291735 SOC: 0.9173 Cumulative_SOC_deviation: 210.1364 Fuel Consumption: 433.7184 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.598\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -2210.5419927315515 SOC: 0.8780 Cumulative_SOC_deviation: 182.4814 Fuel Consumption: 385.7281 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.186\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -2270.4410501595785 SOC: 0.8860 Cumulative_SOC_deviation: 187.5336 Fuel Consumption: 395.1055 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.811\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -2197.9603560272426 SOC: 0.8777 Cumulative_SOC_deviation: 181.2416 Fuel Consumption: 385.5442 Mean: 2.9702, STD: 9.3739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.533\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -1997.9565739061675 SOC: 0.8541 Cumulative_SOC_deviation: 164.2567 Fuel Consumption: 355.3893 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.387\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -1975.5156614051452 SOC: 0.8419 Cumulative_SOC_deviation: 163.2813 Fuel Consumption: 342.7022 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.746\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -1848.855104875144 SOC: 0.8316 Cumulative_SOC_deviation: 152.1046 Fuel Consumption: 327.8087 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.899\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -1744.9266302492867 SOC: 0.8191 Cumulative_SOC_deviation: 142.9989 Fuel Consumption: 314.9377 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.902\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -1723.641292211511 SOC: 0.8264 Cumulative_SOC_deviation: 139.9510 Fuel Consumption: 324.1313 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.667\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -1656.1826288699808 SOC: 0.8132 Cumulative_SOC_deviation: 134.6393 Fuel Consumption: 309.7893 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.796\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -1645.0994570862672 SOC: 0.8195 Cumulative_SOC_deviation: 132.8139 Fuel Consumption: 316.9603 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.630\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -1643.7627100741768 SOC: 0.8168 Cumulative_SOC_deviation: 132.9377 Fuel Consumption: 314.3861 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.759\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -1549.6917503276195 SOC: 0.7936 Cumulative_SOC_deviation: 126.3438 Fuel Consumption: 286.2538 Mean: 2.9702, STD: 9.3740\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.080\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -1783.3233787676652 SOC: 0.8157 Cumulative_SOC_deviation: 147.1879 Fuel Consumption: 311.4442 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.988\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -1771.6396130536084 SOC: 0.8213 Cumulative_SOC_deviation: 145.2841 Fuel Consumption: 318.7990 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.067\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -1425.1111459359897 SOC: 0.7813 Cumulative_SOC_deviation: 115.2832 Fuel Consumption: 272.2796 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.361\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -1450.0699345832713 SOC: 0.7764 Cumulative_SOC_deviation: 118.3858 Fuel Consumption: 266.2119 Mean: 2.9702, STD: 9.3741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.343\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -1421.8183961993984 SOC: 0.7788 Cumulative_SOC_deviation: 115.2297 Fuel Consumption: 269.5216 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.170\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -1628.2313848036356 SOC: 0.7972 Cumulative_SOC_deviation: 133.8300 Fuel Consumption: 289.9312 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.806\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -1244.0007559219712 SOC: 0.7592 Cumulative_SOC_deviation: 99.9486 Fuel Consumption: 244.5151 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.521\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -1378.5289345699975 SOC: 0.7655 Cumulative_SOC_deviation: 112.5924 Fuel Consumption: 252.6054 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.058\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -1013.305892277964 SOC: 0.7400 Cumulative_SOC_deviation: 79.0671 Fuel Consumption: 222.6344 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.129\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -1189.1425806460227 SOC: 0.7507 Cumulative_SOC_deviation: 95.4005 Fuel Consumption: 235.1377 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.930\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -1295.4806969432173 SOC: 0.7589 Cumulative_SOC_deviation: 104.8208 Fuel Consumption: 247.2732 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.656\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -1274.3570147520134 SOC: 0.7511 Cumulative_SOC_deviation: 103.8668 Fuel Consumption: 235.6893 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.703\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -1181.3342559407545 SOC: 0.7530 Cumulative_SOC_deviation: 94.3806 Fuel Consumption: 237.5280 Mean: 2.9702, STD: 9.3741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.723\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -970.672908680789 SOC: 0.7196 Cumulative_SOC_deviation: 77.2126 Fuel Consumption: 198.5473 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.894\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -976.8897756010954 SOC: 0.7270 Cumulative_SOC_deviation: 76.8781 Fuel Consumption: 208.1086 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.804\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -742.412661470238 SOC: 0.7054 Cumulative_SOC_deviation: 55.9678 Fuel Consumption: 182.7344 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.362\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -883.6188022324737 SOC: 0.7149 Cumulative_SOC_deviation: 69.1139 Fuel Consumption: 192.4796 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.429\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -906.090674456849 SOC: 0.7158 Cumulative_SOC_deviation: 71.1956 Fuel Consumption: 194.1344 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.256\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -912.087534984652 SOC: 0.7153 Cumulative_SOC_deviation: 71.8137 Fuel Consumption: 193.9505 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.894\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -841.7287197494336 SOC: 0.7056 Cumulative_SOC_deviation: 65.9546 Fuel Consumption: 182.1828 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.778\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -918.2170857903275 SOC: 0.7246 Cumulative_SOC_deviation: 71.2683 Fuel Consumption: 205.5344 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.867\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -713.5563228350853 SOC: 0.6950 Cumulative_SOC_deviation: 54.4061 Fuel Consumption: 169.4957 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.912\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -720.5843381009549 SOC: 0.6926 Cumulative_SOC_deviation: 55.1456 Fuel Consumption: 169.1280 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.989\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -681.5158383614344 SOC: 0.6989 Cumulative_SOC_deviation: 50.5769 Fuel Consumption: 175.7473 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.333\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -755.3708583973728 SOC: 0.6928 Cumulative_SOC_deviation: 58.7714 Fuel Consumption: 167.6570 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.361\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -746.6007380912419 SOC: 0.6923 Cumulative_SOC_deviation: 57.7657 Fuel Consumption: 168.9441 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.536\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -584.9659555678655 SOC: 0.6863 Cumulative_SOC_deviation: 42.3928 Fuel Consumption: 161.0376 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.637\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -595.0648232520076 SOC: 0.6713 Cumulative_SOC_deviation: 45.2230 Fuel Consumption: 142.8344 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.184\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -629.7431541032727 SOC: 0.6776 Cumulative_SOC_deviation: 47.9738 Fuel Consumption: 150.0054 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.770\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -548.4028621838889 SOC: 0.6713 Cumulative_SOC_deviation: 40.5201 Fuel Consumption: 143.2021 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.400\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -642.5062664458646 SOC: 0.6789 Cumulative_SOC_deviation: 49.1030 Fuel Consumption: 151.4763 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.881\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -540.0590057789949 SOC: 0.6705 Cumulative_SOC_deviation: 39.7408 Fuel Consumption: 142.6505 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.855\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -462.8156731612535 SOC: 0.6622 Cumulative_SOC_deviation: 33.0830 Fuel Consumption: 131.9860 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.116\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -373.21025486159164 SOC: 0.6580 Cumulative_SOC_deviation: 24.5637 Fuel Consumption: 127.5731 Mean: 2.9702, STD: 9.3742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.578\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -464.59588247671854 SOC: 0.6618 Cumulative_SOC_deviation: 33.2978 Fuel Consumption: 131.6183 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.534\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -607.700528850846 SOC: 0.6732 Cumulative_SOC_deviation: 46.2292 Fuel Consumption: 145.4086 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.568\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -401.7488964081968 SOC: 0.6554 Cumulative_SOC_deviation: 27.7118 Fuel Consumption: 124.6312 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.661\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -439.5269003062844 SOC: 0.6567 Cumulative_SOC_deviation: 31.3425 Fuel Consumption: 126.1021 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.665\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -411.1230164083716 SOC: 0.6579 Cumulative_SOC_deviation: 28.3550 Fuel Consumption: 127.5731 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.539\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -436.5042932152103 SOC: 0.6594 Cumulative_SOC_deviation: 30.7828 Fuel Consumption: 128.6763 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.492\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -350.02281493867383 SOC: 0.6439 Cumulative_SOC_deviation: 23.8263 Fuel Consumption: 111.7602 Mean: 2.9702, STD: 9.3743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.444\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -350.11750963214024 SOC: 0.6458 Cumulative_SOC_deviation: 23.7806 Fuel Consumption: 112.3118 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.391\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -288.8406529574322 SOC: 0.6408 Cumulative_SOC_deviation: 18.2229 Fuel Consumption: 106.6118 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.352\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -302.792237002335 SOC: 0.6303 Cumulative_SOC_deviation: 20.8132 Fuel Consumption: 94.6602 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.474\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -364.7405349987488 SOC: 0.6572 Cumulative_SOC_deviation: 23.7535 Fuel Consumption: 127.2053 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.483\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -264.4672825047768 SOC: 0.6337 Cumulative_SOC_deviation: 16.5578 Fuel Consumption: 98.8892 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.437\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -295.0939147346917 SOC: 0.6391 Cumulative_SOC_deviation: 18.9401 Fuel Consumption: 105.6924 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.923\n",
      "Episode: 101 Exploration P: 0.0718 Total reward: -176.36066530386927 SOC: 0.6197 Cumulative_SOC_deviation: 9.3652 Fuel Consumption: 82.7085 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.825\n",
      "Episode: 102 Exploration P: 0.0701 Total reward: -253.11492212646758 SOC: 0.6334 Cumulative_SOC_deviation: 15.3674 Fuel Consumption: 99.4408 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.121\n",
      "Episode: 103 Exploration P: 0.0685 Total reward: -293.12479858910103 SOC: 0.6414 Cumulative_SOC_deviation: 18.5594 Fuel Consumption: 107.5311 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.855\n",
      "Episode: 104 Exploration P: 0.0669 Total reward: -216.7751748589908 SOC: 0.6310 Cumulative_SOC_deviation: 12.0460 Fuel Consumption: 96.3150 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.644\n",
      "Episode: 105 Exploration P: 0.0654 Total reward: -223.32077950178163 SOC: 0.6307 Cumulative_SOC_deviation: 12.7374 Fuel Consumption: 95.9473 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.802\n",
      "Episode: 106 Exploration P: 0.0639 Total reward: -248.28848617401178 SOC: 0.6392 Cumulative_SOC_deviation: 14.1861 Fuel Consumption: 106.4279 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.910\n",
      "Episode: 107 Exploration P: 0.0624 Total reward: -262.1058342793799 SOC: 0.6333 Cumulative_SOC_deviation: 16.1930 Fuel Consumption: 100.1763 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.992\n",
      "Episode: 108 Exploration P: 0.0610 Total reward: -212.65310358157433 SOC: 0.6242 Cumulative_SOC_deviation: 12.4612 Fuel Consumption: 88.0408 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.150\n",
      "Episode: 109 Exploration P: 0.0596 Total reward: -147.24432600869915 SOC: 0.6138 Cumulative_SOC_deviation: 7.1155 Fuel Consumption: 76.0892 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.233\n",
      "Episode: 110 Exploration P: 0.0583 Total reward: -166.45322810414808 SOC: 0.6191 Cumulative_SOC_deviation: 8.4112 Fuel Consumption: 82.3408 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.446\n",
      "Episode: 111 Exploration P: 0.0570 Total reward: -161.6746504590305 SOC: 0.6186 Cumulative_SOC_deviation: 7.9518 Fuel Consumption: 82.1569 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.685\n",
      "Episode: 112 Exploration P: 0.0557 Total reward: -203.69266462271338 SOC: 0.6260 Cumulative_SOC_deviation: 11.1974 Fuel Consumption: 91.7182 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.801\n",
      "Episode: 113 Exploration P: 0.0545 Total reward: -183.91029117585907 SOC: 0.6191 Cumulative_SOC_deviation: 10.1753 Fuel Consumption: 82.1569 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.423\n",
      "Episode: 114 Exploration P: 0.0533 Total reward: -177.04620293410605 SOC: 0.6244 Cumulative_SOC_deviation: 8.8822 Fuel Consumption: 88.2247 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.101\n",
      "Episode: 115 Exploration P: 0.0521 Total reward: -170.28672524599781 SOC: 0.6152 Cumulative_SOC_deviation: 9.1991 Fuel Consumption: 78.2956 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.889\n",
      "Episode: 116 Exploration P: 0.0510 Total reward: -165.68874115014734 SOC: 0.6179 Cumulative_SOC_deviation: 8.4451 Fuel Consumption: 81.2376 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.939\n",
      "Episode: 117 Exploration P: 0.0498 Total reward: -205.5610802975503 SOC: 0.6256 Cumulative_SOC_deviation: 11.5130 Fuel Consumption: 90.4311 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.608\n",
      "Episode: 118 Exploration P: 0.0488 Total reward: -151.22135482829256 SOC: 0.6173 Cumulative_SOC_deviation: 7.0535 Fuel Consumption: 80.6860 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.402\n",
      "Episode: 119 Exploration P: 0.0477 Total reward: -136.26372123738756 SOC: 0.6107 Cumulative_SOC_deviation: 6.4404 Fuel Consumption: 71.8602 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.971\n",
      "Episode: 120 Exploration P: 0.0467 Total reward: -180.26771531851531 SOC: 0.6166 Cumulative_SOC_deviation: 10.0133 Fuel Consumption: 80.1344 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.314\n",
      "Episode: 121 Exploration P: 0.0457 Total reward: -197.85633211266395 SOC: 0.6155 Cumulative_SOC_deviation: 11.9193 Fuel Consumption: 78.6634 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.425\n",
      "Episode: 122 Exploration P: 0.0447 Total reward: -177.51030662815126 SOC: 0.6141 Cumulative_SOC_deviation: 10.1421 Fuel Consumption: 76.0892 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.156\n",
      "Episode: 123 Exploration P: 0.0438 Total reward: -191.6453255912933 SOC: 0.6208 Cumulative_SOC_deviation: 10.6363 Fuel Consumption: 85.2827 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.297\n",
      "Episode: 124 Exploration P: 0.0429 Total reward: -147.50352601703779 SOC: 0.6163 Cumulative_SOC_deviation: 6.7737 Fuel Consumption: 79.7666 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.521\n",
      "Episode: 125 Exploration P: 0.0420 Total reward: -140.41839555073102 SOC: 0.6110 Cumulative_SOC_deviation: 6.7271 Fuel Consumption: 73.1473 Mean: 2.9702, STD: 9.3743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.918\n",
      "Episode: 126 Exploration P: 0.0411 Total reward: -124.98444376056169 SOC: 0.6053 Cumulative_SOC_deviation: 5.8089 Fuel Consumption: 66.8956 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.691\n",
      "Episode: 127 Exploration P: 0.0403 Total reward: -187.05435747629852 SOC: 0.5966 Cumulative_SOC_deviation: 13.0272 Fuel Consumption: 56.7827 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.667\n",
      "Episode: 128 Exploration P: 0.0395 Total reward: -161.4086020229164 SOC: 0.6191 Cumulative_SOC_deviation: 7.8332 Fuel Consumption: 83.0763 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.922\n",
      "Episode: 129 Exploration P: 0.0387 Total reward: -135.9798247985005 SOC: 0.6048 Cumulative_SOC_deviation: 7.0371 Fuel Consumption: 65.6085 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.004\n",
      "Episode: 130 Exploration P: 0.0379 Total reward: -109.27401716406247 SOC: 0.6016 Cumulative_SOC_deviation: 4.6607 Fuel Consumption: 62.6666 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.960\n",
      "Episode: 131 Exploration P: 0.0371 Total reward: -184.11779332136678 SOC: 0.5982 Cumulative_SOC_deviation: 12.5129 Fuel Consumption: 58.9892 Mean: 2.9702, STD: 9.3744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.916\n",
      "Episode: 132 Exploration P: 0.0364 Total reward: -168.98936889970557 SOC: 0.5948 Cumulative_SOC_deviation: 11.4965 Fuel Consumption: 54.0247 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.949\n",
      "Episode: 133 Exploration P: 0.0357 Total reward: -159.7623885371831 SOC: 0.6003 Cumulative_SOC_deviation: 9.9118 Fuel Consumption: 60.6440 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.282\n",
      "Episode: 134 Exploration P: 0.0350 Total reward: -168.01937467503018 SOC: 0.5966 Cumulative_SOC_deviation: 11.1421 Fuel Consumption: 56.5989 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.321\n",
      "Episode: 135 Exploration P: 0.0343 Total reward: -176.61555368191452 SOC: 0.6008 Cumulative_SOC_deviation: 11.5236 Fuel Consumption: 61.3795 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.239\n",
      "Episode: 136 Exploration P: 0.0336 Total reward: -170.9704363842497 SOC: 0.5988 Cumulative_SOC_deviation: 11.2165 Fuel Consumption: 58.8053 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.886\n",
      "Episode: 137 Exploration P: 0.0330 Total reward: -164.26485827826278 SOC: 0.5967 Cumulative_SOC_deviation: 10.7482 Fuel Consumption: 56.7827 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.434\n",
      "Episode: 138 Exploration P: 0.0324 Total reward: -129.3380357580039 SOC: 0.6076 Cumulative_SOC_deviation: 6.0052 Fuel Consumption: 69.2860 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.095\n",
      "Episode: 139 Exploration P: 0.0318 Total reward: -228.39568046989348 SOC: 0.5911 Cumulative_SOC_deviation: 17.8232 Fuel Consumption: 50.1634 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.292\n",
      "Episode: 140 Exploration P: 0.0312 Total reward: -225.9625020612833 SOC: 0.5943 Cumulative_SOC_deviation: 17.1938 Fuel Consumption: 54.0247 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.342\n",
      "Episode: 141 Exploration P: 0.0306 Total reward: -167.72827866520282 SOC: 0.5983 Cumulative_SOC_deviation: 10.9107 Fuel Consumption: 58.6214 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.836\n",
      "Episode: 142 Exploration P: 0.0301 Total reward: -170.07290471439597 SOC: 0.5935 Cumulative_SOC_deviation: 11.7151 Fuel Consumption: 52.9214 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.269\n",
      "Episode: 143 Exploration P: 0.0295 Total reward: -149.01194992918616 SOC: 0.5977 Cumulative_SOC_deviation: 9.0758 Fuel Consumption: 58.2537 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.206\n",
      "Episode: 144 Exploration P: 0.0290 Total reward: -210.69820077536227 SOC: 0.5882 Cumulative_SOC_deviation: 16.4028 Fuel Consumption: 46.6698 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.180\n",
      "Episode: 145 Exploration P: 0.0285 Total reward: -144.34697004029627 SOC: 0.5984 Cumulative_SOC_deviation: 8.5726 Fuel Consumption: 58.6214 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.808\n",
      "Episode: 146 Exploration P: 0.0280 Total reward: -187.75846731818146 SOC: 0.5960 Cumulative_SOC_deviation: 13.1895 Fuel Consumption: 55.8634 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 56.922\n",
      "Episode: 147 Exploration P: 0.0275 Total reward: -152.11522829828505 SOC: 0.6106 Cumulative_SOC_deviation: 7.8968 Fuel Consumption: 73.1473 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.159\n",
      "Episode: 148 Exploration P: 0.0270 Total reward: -123.38753176791302 SOC: 0.6013 Cumulative_SOC_deviation: 6.0721 Fuel Consumption: 62.6666 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.099\n",
      "Episode: 149 Exploration P: 0.0265 Total reward: -178.07411812394778 SOC: 0.5956 Cumulative_SOC_deviation: 12.2395 Fuel Consumption: 55.6795 Mean: 2.9702, STD: 9.3744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.192\n",
      "Episode: 150 Exploration P: 0.0261 Total reward: -248.69781491228804 SOC: 0.5854 Cumulative_SOC_deviation: 20.5338 Fuel Consumption: 43.3601 Mean: 2.9702, STD: 9.3744\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Mean: {:.4f}, STD: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
