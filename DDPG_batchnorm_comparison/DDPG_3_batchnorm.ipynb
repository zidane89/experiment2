{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_pb_91_150.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00005\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    if not os.path.exists(root): \n",
    "        os.makedirs(root)\n",
    "    \n",
    "    actor_model.save_weights(\"./{}/actor_model.h5\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model.h5\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor.h5\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic.h5\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 15.766\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -5477.871519100859 SOC: 1.0000 Cumulative_SOC_deviation: 430.0742 Fuel Consumption: 1177.1296\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 14.702\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -5449.891349226461 SOC: 1.0000 Cumulative_SOC_deviation: 427.6889 Fuel Consumption: 1173.0027\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.054\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -5354.2068716636695 SOC: 1.0000 Cumulative_SOC_deviation: 424.9692 Fuel Consumption: 1104.5145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.700\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -5251.616510175838 SOC: 1.0000 Cumulative_SOC_deviation: 419.6618 Fuel Consumption: 1054.9983\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.750\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -5132.318573834527 SOC: 1.0000 Cumulative_SOC_deviation: 411.6219 Fuel Consumption: 1016.0994\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.105\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -5080.577683550706 SOC: 1.0000 Cumulative_SOC_deviation: 408.7054 Fuel Consumption: 993.5241\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.403\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -5034.561568861862 SOC: 1.0000 Cumulative_SOC_deviation: 404.7902 Fuel Consumption: 986.6596\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.869\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -5041.010882903675 SOC: 1.0000 Cumulative_SOC_deviation: 406.9490 Fuel Consumption: 971.5209\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.016\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -4971.228481122643 SOC: 1.0000 Cumulative_SOC_deviation: 403.5522 Fuel Consumption: 935.7069\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.578\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -4814.5929594279605 SOC: 1.0000 Cumulative_SOC_deviation: 392.5385 Fuel Consumption: 889.2079\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.463\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -4745.543183967431 SOC: 1.0000 Cumulative_SOC_deviation: 386.6121 Fuel Consumption: 879.4219\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.468\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -4630.121352254591 SOC: 1.0000 Cumulative_SOC_deviation: 379.3541 Fuel Consumption: 836.5799\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.792\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -4565.95235794845 SOC: 1.0000 Cumulative_SOC_deviation: 373.4337 Fuel Consumption: 831.6154\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.139\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -4482.849228474419 SOC: 1.0000 Cumulative_SOC_deviation: 366.8048 Fuel Consumption: 814.8014\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.693\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -4465.460723217894 SOC: 1.0000 Cumulative_SOC_deviation: 367.1539 Fuel Consumption: 793.9218\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.499\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -4434.754753038921 SOC: 1.0000 Cumulative_SOC_deviation: 365.8321 Fuel Consumption: 776.4337\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.941\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -4378.237411687195 SOC: 1.0000 Cumulative_SOC_deviation: 361.2264 Fuel Consumption: 765.9734\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.140\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -4275.906983980114 SOC: 1.0000 Cumulative_SOC_deviation: 354.8138 Fuel Consumption: 727.7691\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.327\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -4257.41786392989 SOC: 1.0000 Cumulative_SOC_deviation: 355.2633 Fuel Consumption: 704.7852\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.272\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -3944.024634570268 SOC: 1.0000 Cumulative_SOC_deviation: 329.0315 Fuel Consumption: 653.7099\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.301\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -3973.161965456041 SOC: 1.0000 Cumulative_SOC_deviation: 330.0554 Fuel Consumption: 672.6078\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.021\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -4050.956962692676 SOC: 1.0000 Cumulative_SOC_deviation: 339.1710 Fuel Consumption: 659.2465\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.747\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -3687.306938124842 SOC: 1.0000 Cumulative_SOC_deviation: 306.7307 Fuel Consumption: 620.0002\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.710\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -3779.9412934885395 SOC: 1.0000 Cumulative_SOC_deviation: 315.5406 Fuel Consumption: 624.5357\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.935\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -3564.8111685322497 SOC: 1.0000 Cumulative_SOC_deviation: 297.4067 Fuel Consumption: 590.7443\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.568\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -3659.2536480196263 SOC: 1.0000 Cumulative_SOC_deviation: 303.7987 Fuel Consumption: 621.2669\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.523\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -3450.9818568614473 SOC: 1.0000 Cumulative_SOC_deviation: 287.2455 Fuel Consumption: 578.5271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.649\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -3619.1189916561016 SOC: 1.0000 Cumulative_SOC_deviation: 302.6352 Fuel Consumption: 592.7669\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.545\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -3277.0170301113862 SOC: 1.0000 Cumulative_SOC_deviation: 273.7757 Fuel Consumption: 539.2604\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.038\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -3435.917651325001 SOC: 1.0000 Cumulative_SOC_deviation: 285.5470 Fuel Consumption: 580.4475\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.542\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -3380.870762852125 SOC: 1.0000 Cumulative_SOC_deviation: 283.0966 Fuel Consumption: 549.9045\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.063\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -2949.1296626335925 SOC: 0.9662 Cumulative_SOC_deviation: 245.8432 Fuel Consumption: 490.6980\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.505\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -2999.6801289098485 SOC: 0.9511 Cumulative_SOC_deviation: 252.8084 Fuel Consumption: 471.5959\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.336\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2925.8565195840993 SOC: 0.9522 Cumulative_SOC_deviation: 245.0399 Fuel Consumption: 475.4572\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.505\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -2755.085439826925 SOC: 0.9504 Cumulative_SOC_deviation: 228.2611 Fuel Consumption: 472.4744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.971\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -2644.564902085268 SOC: 0.9311 Cumulative_SOC_deviation: 219.5401 Fuel Consumption: 449.1636\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.994\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -2764.592715274684 SOC: 0.9314 Cumulative_SOC_deviation: 231.5409 Fuel Consumption: 449.1840\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.355\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -2705.4568063317956 SOC: 0.9330 Cumulative_SOC_deviation: 225.5639 Fuel Consumption: 449.8174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.310\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -2370.565991904572 SOC: 0.8884 Cumulative_SOC_deviation: 197.3990 Fuel Consumption: 396.5765\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.317\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -2587.6814507216855 SOC: 0.9180 Cumulative_SOC_deviation: 215.3411 Fuel Consumption: 434.2700\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.333\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -2346.5651973718705 SOC: 0.9020 Cumulative_SOC_deviation: 193.1234 Fuel Consumption: 415.3313\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.295\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -2071.937271308397 SOC: 0.8656 Cumulative_SOC_deviation: 170.3677 Fuel Consumption: 368.2603\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.453\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -2195.269035884707 SOC: 0.8824 Cumulative_SOC_deviation: 180.6231 Fuel Consumption: 389.0377\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.572\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -2031.5867100676028 SOC: 0.8575 Cumulative_SOC_deviation: 167.1601 Fuel Consumption: 359.9861\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 85.455\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -1969.6146822913965 SOC: 0.8433 Cumulative_SOC_deviation: 162.4154 Fuel Consumption: 345.4603\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.070\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -2008.5174892481818 SOC: 0.8476 Cumulative_SOC_deviation: 165.8460 Fuel Consumption: 350.0571\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.776\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -1995.450242698448 SOC: 0.8450 Cumulative_SOC_deviation: 164.8151 Fuel Consumption: 347.2990\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.371\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -2026.125402549989 SOC: 0.8530 Cumulative_SOC_deviation: 167.0368 Fuel Consumption: 355.7571\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.060\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -1698.5088790395225 SOC: 0.8131 Cumulative_SOC_deviation: 139.0742 Fuel Consumption: 307.7667\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 85.301\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -1705.5649162758127 SOC: 0.8217 Cumulative_SOC_deviation: 138.6214 Fuel Consumption: 319.3506\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.527\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -1572.2989433949313 SOC: 0.7938 Cumulative_SOC_deviation: 128.8252 Fuel Consumption: 284.0474\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.406\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -1745.2721699949302 SOC: 0.8094 Cumulative_SOC_deviation: 144.1367 Fuel Consumption: 303.9054\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.440\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -1562.162589159704 SOC: 0.7956 Cumulative_SOC_deviation: 127.3518 Fuel Consumption: 288.6441\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.514\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -1503.7097813490818 SOC: 0.7891 Cumulative_SOC_deviation: 122.4075 Fuel Consumption: 279.6345\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.155\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -1755.4562460535994 SOC: 0.8170 Cumulative_SOC_deviation: 144.0335 Fuel Consumption: 315.1216\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.287\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -1283.6922083946265 SOC: 0.7717 Cumulative_SOC_deviation: 102.4284 Fuel Consumption: 259.4086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.084\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -1487.595825274169 SOC: 0.7856 Cumulative_SOC_deviation: 121.1271 Fuel Consumption: 276.3248\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.707\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -1413.3444529853653 SOC: 0.7775 Cumulative_SOC_deviation: 114.5845 Fuel Consumption: 267.4990\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.546\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -1657.720576918769 SOC: 0.8069 Cumulative_SOC_deviation: 135.6757 Fuel Consumption: 300.9635\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.593\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -1341.5343686028823 SOC: 0.7787 Cumulative_SOC_deviation: 107.3116 Fuel Consumption: 268.4183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.254\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -1312.7753522158616 SOC: 0.7669 Cumulative_SOC_deviation: 105.8699 Fuel Consumption: 254.0764\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.364\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -1218.1720253029919 SOC: 0.7469 Cumulative_SOC_deviation: 98.7263 Fuel Consumption: 230.9086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.727\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -1194.0165029339107 SOC: 0.7498 Cumulative_SOC_deviation: 95.9430 Fuel Consumption: 234.5861\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.846\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -1222.3674793509454 SOC: 0.7582 Cumulative_SOC_deviation: 97.6565 Fuel Consumption: 245.8022\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.559\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -1157.7261094535677 SOC: 0.7486 Cumulative_SOC_deviation: 92.4979 Fuel Consumption: 232.7473\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.383\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -1104.146983474455 SOC: 0.7409 Cumulative_SOC_deviation: 88.0225 Fuel Consumption: 223.9215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.406\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -1199.5870164187334 SOC: 0.7487 Cumulative_SOC_deviation: 96.6104 Fuel Consumption: 233.4828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.576\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -997.0202789276632 SOC: 0.7230 Cumulative_SOC_deviation: 79.5531 Fuel Consumption: 201.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.998\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -1050.2602314383532 SOC: 0.7351 Cumulative_SOC_deviation: 83.2774 Fuel Consumption: 217.4860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.796\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -858.4371633985139 SOC: 0.7182 Cumulative_SOC_deviation: 66.0809 Fuel Consumption: 197.6280\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.741\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -1115.8837384624328 SOC: 0.7313 Cumulative_SOC_deviation: 90.2811 Fuel Consumption: 213.0731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.152\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -842.7173602931116 SOC: 0.7074 Cumulative_SOC_deviation: 65.8328 Fuel Consumption: 184.3893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.779\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -928.1124992651014 SOC: 0.7141 Cumulative_SOC_deviation: 73.5817 Fuel Consumption: 192.2957\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.896\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -811.2712076557247 SOC: 0.6992 Cumulative_SOC_deviation: 63.5708 Fuel Consumption: 175.5634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.764\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -939.7179442583814 SOC: 0.7059 Cumulative_SOC_deviation: 75.6248 Fuel Consumption: 183.4699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.680\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -848.6962683579425 SOC: 0.7021 Cumulative_SOC_deviation: 67.1110 Fuel Consumption: 177.5860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.847\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -569.0312814113036 SOC: 0.6779 Cumulative_SOC_deviation: 41.9026 Fuel Consumption: 150.0054\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.811\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -509.37118848756523 SOC: 0.6721 Cumulative_SOC_deviation: 36.5801 Fuel Consumption: 143.5699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.365\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -541.5163025071688 SOC: 0.6729 Cumulative_SOC_deviation: 39.7027 Fuel Consumption: 144.4892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.299\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -664.2195694996701 SOC: 0.6823 Cumulative_SOC_deviation: 50.8330 Fuel Consumption: 155.8892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.823\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -739.7273340159713 SOC: 0.6867 Cumulative_SOC_deviation: 57.8874 Fuel Consumption: 160.8538\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.284\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -644.3168383416353 SOC: 0.6806 Cumulative_SOC_deviation: 49.0450 Fuel Consumption: 153.8667\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.141\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -473.80620007768107 SOC: 0.6665 Cumulative_SOC_deviation: 33.6488 Fuel Consumption: 137.3183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.440\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -420.78684890872563 SOC: 0.6571 Cumulative_SOC_deviation: 29.4869 Fuel Consumption: 125.9183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.557\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -626.5149916119964 SOC: 0.6786 Cumulative_SOC_deviation: 47.4303 Fuel Consumption: 152.2118\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.945\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -506.38314524010286 SOC: 0.6645 Cumulative_SOC_deviation: 37.1639 Fuel Consumption: 134.7441\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.044\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -414.54374305601124 SOC: 0.6550 Cumulative_SOC_deviation: 29.0464 Fuel Consumption: 124.0795\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.720\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -562.3062881265838 SOC: 0.6698 Cumulative_SOC_deviation: 42.1862 Fuel Consumption: 140.4441\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.355\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -585.9193673205368 SOC: 0.6763 Cumulative_SOC_deviation: 43.6649 Fuel Consumption: 149.2699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.475\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -409.3906410515347 SOC: 0.6596 Cumulative_SOC_deviation: 27.9795 Fuel Consumption: 129.5957\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.058\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -614.9497402587805 SOC: 0.6764 Cumulative_SOC_deviation: 46.4761 Fuel Consumption: 150.1892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.572\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -518.5259707459068 SOC: 0.6649 Cumulative_SOC_deviation: 38.3598 Fuel Consumption: 134.9279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.092\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -425.82515219197506 SOC: 0.6559 Cumulative_SOC_deviation: 30.0459 Fuel Consumption: 125.3666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.651\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -310.3913539655161 SOC: 0.6436 Cumulative_SOC_deviation: 19.9183 Fuel Consumption: 111.2086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.705\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -394.4675779715653 SOC: 0.6574 Cumulative_SOC_deviation: 26.8182 Fuel Consumption: 126.2860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.566\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -386.5164428867482 SOC: 0.6517 Cumulative_SOC_deviation: 26.6482 Fuel Consumption: 120.0344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.936\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -307.0303325628223 SOC: 0.6384 Cumulative_SOC_deviation: 20.2073 Fuel Consumption: 104.9569\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.775\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -426.5061143498404 SOC: 0.6484 Cumulative_SOC_deviation: 31.0149 Fuel Consumption: 116.3570\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.012\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -422.03664422751126 SOC: 0.6565 Cumulative_SOC_deviation: 29.4647 Fuel Consumption: 127.3892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.798\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -365.0777165015427 SOC: 0.6479 Cumulative_SOC_deviation: 24.9089 Fuel Consumption: 115.9892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.450\n",
      "Episode: 101 Exploration P: 0.0718 Total reward: -289.05961095573423 SOC: 0.6440 Cumulative_SOC_deviation: 17.8219 Fuel Consumption: 110.8408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.623\n",
      "Episode: 102 Exploration P: 0.0701 Total reward: -220.95514060198153 SOC: 0.6303 Cumulative_SOC_deviation: 12.5192 Fuel Consumption: 95.7634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.147\n",
      "Episode: 103 Exploration P: 0.0685 Total reward: -332.5951761406019 SOC: 0.6400 Cumulative_SOC_deviation: 22.6351 Fuel Consumption: 106.2440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.473\n",
      "Episode: 104 Exploration P: 0.0669 Total reward: -181.5166794379623 SOC: 0.6244 Cumulative_SOC_deviation: 9.2189 Fuel Consumption: 89.3279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.870\n",
      "Episode: 105 Exploration P: 0.0654 Total reward: -238.30033349214014 SOC: 0.6288 Cumulative_SOC_deviation: 14.4560 Fuel Consumption: 93.7408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.205\n",
      "Episode: 106 Exploration P: 0.0639 Total reward: -294.3915006692534 SOC: 0.6363 Cumulative_SOC_deviation: 19.1457 Fuel Consumption: 102.9344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.821\n",
      "Episode: 107 Exploration P: 0.0624 Total reward: -273.53657313945934 SOC: 0.6382 Cumulative_SOC_deviation: 16.8763 Fuel Consumption: 104.7731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.845\n",
      "Episode: 108 Exploration P: 0.0610 Total reward: -261.6570831420197 SOC: 0.6323 Cumulative_SOC_deviation: 16.3871 Fuel Consumption: 97.7860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.300\n",
      "Episode: 109 Exploration P: 0.0596 Total reward: -168.67455321748062 SOC: 0.6202 Cumulative_SOC_deviation: 8.5598 Fuel Consumption: 83.0763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.514\n",
      "Episode: 110 Exploration P: 0.0583 Total reward: -265.59948630079174 SOC: 0.6338 Cumulative_SOC_deviation: 16.5423 Fuel Consumption: 100.1763\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.387\n",
      "Episode: 111 Exploration P: 0.0570 Total reward: -175.49121089099683 SOC: 0.6191 Cumulative_SOC_deviation: 9.2967 Fuel Consumption: 82.5247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.244\n",
      "Episode: 112 Exploration P: 0.0557 Total reward: -195.2974943067721 SOC: 0.6221 Cumulative_SOC_deviation: 10.9463 Fuel Consumption: 85.8344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.399\n",
      "Episode: 113 Exploration P: 0.0545 Total reward: -230.49785907603138 SOC: 0.6317 Cumulative_SOC_deviation: 13.3263 Fuel Consumption: 97.2344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.263\n",
      "Episode: 114 Exploration P: 0.0533 Total reward: -214.07068896825737 SOC: 0.6284 Cumulative_SOC_deviation: 12.0146 Fuel Consumption: 93.9247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.172\n",
      "Episode: 115 Exploration P: 0.0521 Total reward: -145.1727260881085 SOC: 0.6156 Cumulative_SOC_deviation: 6.6877 Fuel Consumption: 78.2956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.396\n",
      "Episode: 116 Exploration P: 0.0510 Total reward: -139.3014086005241 SOC: 0.6134 Cumulative_SOC_deviation: 6.2844 Fuel Consumption: 76.4569\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.287\n",
      "Episode: 117 Exploration P: 0.0498 Total reward: -168.7218534889701 SOC: 0.6204 Cumulative_SOC_deviation: 8.4726 Fuel Consumption: 83.9956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.480\n",
      "Episode: 118 Exploration P: 0.0488 Total reward: -197.88077801115722 SOC: 0.6194 Cumulative_SOC_deviation: 11.5356 Fuel Consumption: 82.5247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.785\n",
      "Episode: 119 Exploration P: 0.0477 Total reward: -172.90427011584566 SOC: 0.6184 Cumulative_SOC_deviation: 9.1299 Fuel Consumption: 81.6053\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.009\n",
      "Episode: 120 Exploration P: 0.0467 Total reward: -185.2034738378822 SOC: 0.6183 Cumulative_SOC_deviation: 10.3598 Fuel Consumption: 81.6053\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.866\n",
      "Episode: 121 Exploration P: 0.0457 Total reward: -159.27463656431752 SOC: 0.6110 Cumulative_SOC_deviation: 8.5760 Fuel Consumption: 73.5150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.362\n",
      "Episode: 122 Exploration P: 0.0447 Total reward: -153.77487488831943 SOC: 0.6136 Cumulative_SOC_deviation: 7.7134 Fuel Consumption: 76.6408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.852\n",
      "Episode: 123 Exploration P: 0.0438 Total reward: -143.07037657622 SOC: 0.6026 Cumulative_SOC_deviation: 7.9668 Fuel Consumption: 63.4021\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.657\n",
      "Episode: 124 Exploration P: 0.0429 Total reward: -153.89912488704272 SOC: 0.6034 Cumulative_SOC_deviation: 8.9394 Fuel Consumption: 64.5053\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.071\n",
      "Episode: 125 Exploration P: 0.0420 Total reward: -172.616477983169 SOC: 0.6192 Cumulative_SOC_deviation: 8.9540 Fuel Consumption: 83.0763\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.494\n",
      "Episode: 126 Exploration P: 0.0411 Total reward: -144.85050827105022 SOC: 0.6088 Cumulative_SOC_deviation: 7.4645 Fuel Consumption: 70.2053\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.954\n",
      "Episode: 127 Exploration P: 0.0403 Total reward: -160.8901749782451 SOC: 0.6127 Cumulative_SOC_deviation: 8.5720 Fuel Consumption: 75.1698\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.998\n",
      "Episode: 128 Exploration P: 0.0395 Total reward: -148.01869094936018 SOC: 0.6104 Cumulative_SOC_deviation: 7.5055 Fuel Consumption: 72.9634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.733\n",
      "Episode: 129 Exploration P: 0.0387 Total reward: -132.1449711466571 SOC: 0.6069 Cumulative_SOC_deviation: 6.3962 Fuel Consumption: 68.1827\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.516\n",
      "Episode: 130 Exploration P: 0.0379 Total reward: -168.7410059783472 SOC: 0.6071 Cumulative_SOC_deviation: 10.0558 Fuel Consumption: 68.1827\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.799\n",
      "Episode: 131 Exploration P: 0.0371 Total reward: -130.68581745949083 SOC: 0.6112 Cumulative_SOC_deviation: 5.7539 Fuel Consumption: 73.1473\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.540\n",
      "Episode: 132 Exploration P: 0.0364 Total reward: -133.0091489006557 SOC: 0.6078 Cumulative_SOC_deviation: 6.3355 Fuel Consumption: 69.6537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.515\n",
      "Episode: 133 Exploration P: 0.0357 Total reward: -188.70604663698276 SOC: 0.5970 Cumulative_SOC_deviation: 13.1372 Fuel Consumption: 57.3343\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.797\n",
      "Episode: 134 Exploration P: 0.0350 Total reward: -177.02670070594118 SOC: 0.5964 Cumulative_SOC_deviation: 12.0796 Fuel Consumption: 56.2311\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.953\n",
      "Episode: 135 Exploration P: 0.0343 Total reward: -116.61211789236931 SOC: 0.6085 Cumulative_SOC_deviation: 4.6039 Fuel Consumption: 70.5731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.509\n",
      "Episode: 136 Exploration P: 0.0336 Total reward: -163.0900102146059 SOC: 0.6100 Cumulative_SOC_deviation: 9.1046 Fuel Consumption: 72.0440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.605\n",
      "Episode: 137 Exploration P: 0.0330 Total reward: -165.98767033142812 SOC: 0.5955 Cumulative_SOC_deviation: 10.9940 Fuel Consumption: 56.0472\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.367\n",
      "Episode: 138 Exploration P: 0.0324 Total reward: -159.10740987292468 SOC: 0.5989 Cumulative_SOC_deviation: 9.9750 Fuel Consumption: 59.3569\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.673\n",
      "Episode: 139 Exploration P: 0.0318 Total reward: -207.69707524616373 SOC: 0.5920 Cumulative_SOC_deviation: 15.6982 Fuel Consumption: 50.7150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.878\n",
      "Episode: 140 Exploration P: 0.0312 Total reward: -180.96164391464302 SOC: 0.5944 Cumulative_SOC_deviation: 12.7121 Fuel Consumption: 53.8408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.320\n",
      "Episode: 141 Exploration P: 0.0306 Total reward: -221.25433306808486 SOC: 0.5881 Cumulative_SOC_deviation: 17.4952 Fuel Consumption: 46.3021\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.801\n",
      "Episode: 142 Exploration P: 0.0301 Total reward: -246.46007720136961 SOC: 0.5895 Cumulative_SOC_deviation: 19.7768 Fuel Consumption: 48.6924\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.107\n",
      "Episode: 143 Exploration P: 0.0295 Total reward: -175.83958983267792 SOC: 0.6016 Cumulative_SOC_deviation: 11.3173 Fuel Consumption: 62.6666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ba92dd42db6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mDELAY_TRAINING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m                 \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_EPSILON\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mDECAY_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4679c004126e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mcritic_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mactor_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         actor_optimizer.apply_gradients(\n\u001b[0;32m     48\u001b[0m             \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1584\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1586\u001b[1;33m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6110\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6111\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6112\u001b[1;33m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[0;32m   6113\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6114\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5\n",
    "reward_factor = 10\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "    \n",
    "    root = \"DDPG3_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG3.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
