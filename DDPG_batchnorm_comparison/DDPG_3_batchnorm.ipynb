{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_pb_91_150.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 150\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    if not os.path.exists(root): \n",
    "        os.makedirs(root)\n",
    "    \n",
    "    actor_model.save_weights(\"./{}/actor_model.h5\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model.h5\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor.h5\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic.h5\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 21.767\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -5514.958166701528 SOC: 1.0000 Cumulative_SOC_deviation: 432.2159 Fuel Consumption: 1192.7995\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 21.637\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -5458.5529452413375 SOC: 1.0000 Cumulative_SOC_deviation: 428.9085 Fuel Consumption: 1169.4683\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.055\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -5418.1004514415345 SOC: 1.0000 Cumulative_SOC_deviation: 430.9270 Fuel Consumption: 1108.8309\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.314\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -5286.200079673862 SOC: 1.0000 Cumulative_SOC_deviation: 421.1385 Fuel Consumption: 1074.8155\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.281\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -5193.969825756128 SOC: 1.0000 Cumulative_SOC_deviation: 416.3038 Fuel Consumption: 1030.9316\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.065\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -5113.618375093542 SOC: 1.0000 Cumulative_SOC_deviation: 410.8102 Fuel Consumption: 1005.5166\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.367\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -4913.786178536406 SOC: 1.0000 Cumulative_SOC_deviation: 396.9110 Fuel Consumption: 944.6757\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.679\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -4841.8005188357265 SOC: 1.0000 Cumulative_SOC_deviation: 392.3132 Fuel Consumption: 918.6681\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.148\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -4852.807818667856 SOC: 1.0000 Cumulative_SOC_deviation: 393.8430 Fuel Consumption: 914.3778\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.103\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -4854.359691227201 SOC: 1.0000 Cumulative_SOC_deviation: 393.5344 Fuel Consumption: 919.0155\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.921\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -4605.852248689299 SOC: 1.0000 Cumulative_SOC_deviation: 376.8026 Fuel Consumption: 837.8262\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.710\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -4637.024377747037 SOC: 1.0000 Cumulative_SOC_deviation: 377.6521 Fuel Consumption: 860.5036\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.645\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -4660.477338801755 SOC: 1.0000 Cumulative_SOC_deviation: 383.1559 Fuel Consumption: 828.9186\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.265\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -4475.765708846766 SOC: 1.0000 Cumulative_SOC_deviation: 368.0414 Fuel Consumption: 795.3519\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.042\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -4387.469016705421 SOC: 1.0000 Cumulative_SOC_deviation: 360.8032 Fuel Consumption: 779.4369\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.915\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -4223.65870522243 SOC: 1.0000 Cumulative_SOC_deviation: 349.8995 Fuel Consumption: 724.6637\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.476\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -4229.910941085029 SOC: 1.0000 Cumulative_SOC_deviation: 348.7412 Fuel Consumption: 742.4992\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.957\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -4322.804671707606 SOC: 1.0000 Cumulative_SOC_deviation: 356.8211 Fuel Consumption: 754.5939\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.338\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -4110.701770959587 SOC: 1.0000 Cumulative_SOC_deviation: 338.0052 Fuel Consumption: 730.6498\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.195\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -4046.6966950276874 SOC: 1.0000 Cumulative_SOC_deviation: 334.0563 Fuel Consumption: 706.1336\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.158\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -4056.7497992418967 SOC: 1.0000 Cumulative_SOC_deviation: 337.3539 Fuel Consumption: 683.2110\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.315\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -3711.2682067076003 SOC: 1.0000 Cumulative_SOC_deviation: 308.0297 Fuel Consumption: 630.9712\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.301\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -4027.842773809884 SOC: 1.0000 Cumulative_SOC_deviation: 335.1639 Fuel Consumption: 676.2035\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.182\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -3836.896635512661 SOC: 1.0000 Cumulative_SOC_deviation: 319.3545 Fuel Consumption: 643.3519\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.746\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -3491.7146157829816 SOC: 1.0000 Cumulative_SOC_deviation: 289.2226 Fuel Consumption: 599.4884\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.600\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -3677.027668550273 SOC: 1.0000 Cumulative_SOC_deviation: 306.4648 Fuel Consumption: 612.3798\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.088\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -3613.4034129998254 SOC: 1.0000 Cumulative_SOC_deviation: 301.5427 Fuel Consumption: 597.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.803\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -3652.665738253758 SOC: 1.0000 Cumulative_SOC_deviation: 307.1176 Fuel Consumption: 581.4895\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.241\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -3337.875239775985 SOC: 1.0000 Cumulative_SOC_deviation: 278.9952 Fuel Consumption: 547.9228\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.245\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -3153.626213333785 SOC: 0.9845 Cumulative_SOC_deviation: 264.2294 Fuel Consumption: 511.3324\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.286\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -3141.65469026417 SOC: 0.9913 Cumulative_SOC_deviation: 262.0455 Fuel Consumption: 521.2002\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.343\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -3133.21449815107 SOC: 0.9827 Cumulative_SOC_deviation: 262.2822 Fuel Consumption: 510.3927\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.836\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -3062.34247072485 SOC: 0.9735 Cumulative_SOC_deviation: 256.1818 Fuel Consumption: 500.5249\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.933\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2825.201050499398 SOC: 0.9531 Cumulative_SOC_deviation: 235.0071 Fuel Consumption: 475.1303\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.781\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -2830.156783511147 SOC: 0.9528 Cumulative_SOC_deviation: 235.3229 Fuel Consumption: 476.9281\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.631\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -2529.788039620517 SOC: 0.9285 Cumulative_SOC_deviation: 208.2831 Fuel Consumption: 446.9571\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.646\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -2665.011925095065 SOC: 0.9278 Cumulative_SOC_deviation: 221.9342 Fuel Consumption: 445.6700\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.351\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -2443.5422093620355 SOC: 0.9071 Cumulative_SOC_deviation: 202.3798 Fuel Consumption: 419.7442\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.546\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -2607.430771117128 SOC: 0.9221 Cumulative_SOC_deviation: 216.8196 Fuel Consumption: 439.2346\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.794\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -2658.02181235055 SOC: 0.9179 Cumulative_SOC_deviation: 222.6142 Fuel Consumption: 431.8797\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.288\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -2453.4769935688596 SOC: 0.8982 Cumulative_SOC_deviation: 204.4581 Fuel Consumption: 408.8958\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.324\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -2481.803903846327 SOC: 0.9088 Cumulative_SOC_deviation: 205.9118 Fuel Consumption: 422.6862\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.571\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -2311.772384506226 SOC: 0.8908 Cumulative_SOC_deviation: 191.0231 Fuel Consumption: 401.5410\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.736\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -2322.3094454258517 SOC: 0.8968 Cumulative_SOC_deviation: 191.3781 Fuel Consumption: 408.5281\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.807\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -2009.1925006473766 SOC: 0.8556 Cumulative_SOC_deviation: 165.1781 Fuel Consumption: 357.4119\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.052\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -2234.8472498441547 SOC: 0.8613 Cumulative_SOC_deviation: 186.9897 Fuel Consumption: 364.9506\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.247\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -2149.2887694972196 SOC: 0.8655 Cumulative_SOC_deviation: 177.5880 Fuel Consumption: 373.4087\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.167\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -1920.9293185081822 SOC: 0.8444 Cumulative_SOC_deviation: 157.5653 Fuel Consumption: 345.2764\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.294\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -1943.9913618536345 SOC: 0.8483 Cumulative_SOC_deviation: 159.2463 Fuel Consumption: 351.5281\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.254\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -1776.636475269906 SOC: 0.8261 Cumulative_SOC_deviation: 145.2689 Fuel Consumption: 323.9474\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.046\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -1810.5433011038606 SOC: 0.8254 Cumulative_SOC_deviation: 148.9354 Fuel Consumption: 321.1893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.995\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -1719.1703800613725 SOC: 0.8193 Cumulative_SOC_deviation: 140.3681 Fuel Consumption: 315.4893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.484\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -1334.2841144617485 SOC: 0.7776 Cumulative_SOC_deviation: 106.8256 Fuel Consumption: 266.0280\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.197\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -1584.5585038255556 SOC: 0.8076 Cumulative_SOC_deviation: 128.2308 Fuel Consumption: 302.2506\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.640\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -1562.3620881574775 SOC: 0.7972 Cumulative_SOC_deviation: 127.2431 Fuel Consumption: 289.9312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.321\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -1441.888140943161 SOC: 0.7839 Cumulative_SOC_deviation: 116.7402 Fuel Consumption: 274.4861\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.374\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -1455.9110568204803 SOC: 0.7771 Cumulative_SOC_deviation: 118.9331 Fuel Consumption: 266.5796\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.575\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -1460.458144852072 SOC: 0.7858 Cumulative_SOC_deviation: 118.3398 Fuel Consumption: 277.0603\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.122\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -1261.6395771052953 SOC: 0.7700 Cumulative_SOC_deviation: 100.3518 Fuel Consumption: 258.1215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.252\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -1261.3041709178196 SOC: 0.7623 Cumulative_SOC_deviation: 101.3112 Fuel Consumption: 248.1925\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.600\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -1340.4626027698735 SOC: 0.7772 Cumulative_SOC_deviation: 107.2964 Fuel Consumption: 267.4990\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 94.866\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -1113.095833093147 SOC: 0.7346 Cumulative_SOC_deviation: 89.7816 Fuel Consumption: 215.2796\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 94.587\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -1055.6602734964038 SOC: 0.7413 Cumulative_SOC_deviation: 83.2290 Fuel Consumption: 223.3699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.919\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -1007.3865330765285 SOC: 0.7254 Cumulative_SOC_deviation: 80.2771 Fuel Consumption: 204.6151\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.911\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -1307.0448975534152 SOC: 0.7724 Cumulative_SOC_deviation: 104.5246 Fuel Consumption: 261.7990\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.373\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -1185.985482361984 SOC: 0.7460 Cumulative_SOC_deviation: 95.5445 Fuel Consumption: 230.5409\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.241\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -880.6185647471007 SOC: 0.7142 Cumulative_SOC_deviation: 68.7220 Fuel Consumption: 193.3989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.613\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -978.9964724650977 SOC: 0.7285 Cumulative_SOC_deviation: 77.0704 Fuel Consumption: 208.2925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.433\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -969.9061626243353 SOC: 0.7269 Cumulative_SOC_deviation: 76.2533 Fuel Consumption: 207.3731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.290\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -1033.9141612942922 SOC: 0.7326 Cumulative_SOC_deviation: 82.0106 Fuel Consumption: 213.8086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.350\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -1003.9281480115594 SOC: 0.7245 Cumulative_SOC_deviation: 79.9129 Fuel Consumption: 204.7989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.139\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -886.016575785162 SOC: 0.7127 Cumulative_SOC_deviation: 69.5927 Fuel Consumption: 190.0893\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.277\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -645.8575175110708 SOC: 0.6839 Cumulative_SOC_deviation: 48.8313 Fuel Consumption: 157.5441\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.186\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -774.534840372028 SOC: 0.6959 Cumulative_SOC_deviation: 60.3936 Fuel Consumption: 170.5989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.090\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -848.8636444524217 SOC: 0.7060 Cumulative_SOC_deviation: 66.5945 Fuel Consumption: 182.9183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.543\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -881.8095378025151 SOC: 0.7050 Cumulative_SOC_deviation: 69.9259 Fuel Consumption: 182.5505\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.460\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -693.65471276282 SOC: 0.6903 Cumulative_SOC_deviation: 52.8204 Fuel Consumption: 165.4505\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.671\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -678.1674963652741 SOC: 0.6876 Cumulative_SOC_deviation: 51.6578 Fuel Consumption: 161.5892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.159\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -753.3312229692079 SOC: 0.6933 Cumulative_SOC_deviation: 58.5123 Fuel Consumption: 168.2086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.411\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -624.7334865686103 SOC: 0.6846 Cumulative_SOC_deviation: 46.6822 Fuel Consumption: 157.9118\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.749\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -619.5794792393091 SOC: 0.6776 Cumulative_SOC_deviation: 46.8471 Fuel Consumption: 151.1086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.311\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -647.590537894021 SOC: 0.6718 Cumulative_SOC_deviation: 50.4572 Fuel Consumption: 143.0183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.309\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -527.4300043488126 SOC: 0.6745 Cumulative_SOC_deviation: 38.2021 Fuel Consumption: 145.4086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.269\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -616.4538080156489 SOC: 0.6802 Cumulative_SOC_deviation: 46.3507 Fuel Consumption: 152.9473\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.365\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -433.3822240467075 SOC: 0.6576 Cumulative_SOC_deviation: 30.6912 Fuel Consumption: 126.4699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.362\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -501.16994942847134 SOC: 0.6683 Cumulative_SOC_deviation: 36.1645 Fuel Consumption: 139.5247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.920\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -471.53927949957887 SOC: 0.6565 Cumulative_SOC_deviation: 34.6724 Fuel Consumption: 124.8150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.471\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -452.6413814532322 SOC: 0.6622 Cumulative_SOC_deviation: 32.0472 Fuel Consumption: 132.1699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.742\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -465.5864700411345 SOC: 0.6609 Cumulative_SOC_deviation: 33.4704 Fuel Consumption: 130.8828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.738\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -480.73772734906447 SOC: 0.6586 Cumulative_SOC_deviation: 35.3532 Fuel Consumption: 127.2053\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.138\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -357.3202290398075 SOC: 0.6461 Cumulative_SOC_deviation: 24.5008 Fuel Consumption: 112.3118\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.723\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -377.3732032891692 SOC: 0.6455 Cumulative_SOC_deviation: 26.4878 Fuel Consumption: 112.4957\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.627\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -274.7085261245991 SOC: 0.6395 Cumulative_SOC_deviation: 16.8281 Fuel Consumption: 106.4279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.439\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -452.58489119657605 SOC: 0.6556 Cumulative_SOC_deviation: 32.7770 Fuel Consumption: 124.8150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.664\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -390.6748998988339 SOC: 0.6563 Cumulative_SOC_deviation: 26.4573 Fuel Consumption: 126.1021\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.222\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -436.99233415122444 SOC: 0.6536 Cumulative_SOC_deviation: 31.4752 Fuel Consumption: 122.2408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.099\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -468.1270064339051 SOC: 0.6565 Cumulative_SOC_deviation: 34.2944 Fuel Consumption: 125.1828\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.271\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -317.0864570610789 SOC: 0.6443 Cumulative_SOC_deviation: 20.5878 Fuel Consumption: 111.2086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.607\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -242.69446583382012 SOC: 0.6326 Cumulative_SOC_deviation: 14.5460 Fuel Consumption: 97.2344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.497\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -328.84857039964345 SOC: 0.6408 Cumulative_SOC_deviation: 22.0214 Fuel Consumption: 108.6344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.106\n",
      "Episode: 101 Exploration P: 0.0718 Total reward: -315.22452146616706 SOC: 0.6443 Cumulative_SOC_deviation: 20.4016 Fuel Consumption: 111.2086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.604\n",
      "Episode: 102 Exploration P: 0.0701 Total reward: -348.16181300637277 SOC: 0.6473 Cumulative_SOC_deviation: 23.3092 Fuel Consumption: 115.0699\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.772\n",
      "Episode: 103 Exploration P: 0.0685 Total reward: -195.7013085189233 SOC: 0.6252 Cumulative_SOC_deviation: 10.6373 Fuel Consumption: 89.3279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.369\n",
      "Episode: 104 Exploration P: 0.0669 Total reward: -413.51810738756114 SOC: 0.6551 Cumulative_SOC_deviation: 28.8703 Fuel Consumption: 124.8150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.504\n",
      "Episode: 105 Exploration P: 0.0654 Total reward: -264.70994385185793 SOC: 0.6359 Cumulative_SOC_deviation: 16.2143 Fuel Consumption: 102.5666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.741\n",
      "Episode: 106 Exploration P: 0.0639 Total reward: -250.16490041594494 SOC: 0.6344 Cumulative_SOC_deviation: 14.9621 Fuel Consumption: 100.5440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.314\n",
      "Episode: 107 Exploration P: 0.0624 Total reward: -209.91363508230464 SOC: 0.6277 Cumulative_SOC_deviation: 11.7460 Fuel Consumption: 92.4537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.355\n",
      "Episode: 108 Exploration P: 0.0610 Total reward: -204.18703484697664 SOC: 0.6233 Cumulative_SOC_deviation: 11.7066 Fuel Consumption: 87.1215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.299\n",
      "Episode: 109 Exploration P: 0.0596 Total reward: -217.33347756950386 SOC: 0.6247 Cumulative_SOC_deviation: 12.8741 Fuel Consumption: 88.5924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.444\n",
      "Episode: 110 Exploration P: 0.0583 Total reward: -235.2718991483296 SOC: 0.6292 Cumulative_SOC_deviation: 14.0060 Fuel Consumption: 95.2118\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.415\n",
      "Episode: 111 Exploration P: 0.0570 Total reward: -172.7627888560541 SOC: 0.6204 Cumulative_SOC_deviation: 8.8767 Fuel Consumption: 83.9956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.233\n",
      "Episode: 112 Exploration P: 0.0557 Total reward: -275.470626143559 SOC: 0.6282 Cumulative_SOC_deviation: 18.3017 Fuel Consumption: 92.4537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.387\n",
      "Episode: 113 Exploration P: 0.0545 Total reward: -237.55406573928042 SOC: 0.6275 Cumulative_SOC_deviation: 14.5100 Fuel Consumption: 92.4537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.460\n",
      "Episode: 114 Exploration P: 0.0533 Total reward: -133.41881259298708 SOC: 0.6096 Cumulative_SOC_deviation: 6.2110 Fuel Consumption: 71.3085\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.342\n",
      "Episode: 115 Exploration P: 0.0521 Total reward: -122.24868413256075 SOC: 0.6080 Cumulative_SOC_deviation: 5.2227 Fuel Consumption: 70.0214\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.473\n",
      "Episode: 116 Exploration P: 0.0510 Total reward: -185.98080358292773 SOC: 0.6235 Cumulative_SOC_deviation: 9.7940 Fuel Consumption: 88.0408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.192\n",
      "Episode: 117 Exploration P: 0.0498 Total reward: -182.6184571488721 SOC: 0.6128 Cumulative_SOC_deviation: 10.7449 Fuel Consumption: 75.1698\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.201\n",
      "Episode: 118 Exploration P: 0.0488 Total reward: -151.04848707163663 SOC: 0.6158 Cumulative_SOC_deviation: 7.2753 Fuel Consumption: 78.2956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.440\n",
      "Episode: 119 Exploration P: 0.0477 Total reward: -147.8097450184117 SOC: 0.6056 Cumulative_SOC_deviation: 8.0730 Fuel Consumption: 67.0795\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.384\n",
      "Episode: 120 Exploration P: 0.0467 Total reward: -165.6734459185316 SOC: 0.6200 Cumulative_SOC_deviation: 8.1310 Fuel Consumption: 84.3634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.428\n",
      "Episode: 121 Exploration P: 0.0457 Total reward: -120.87456670003952 SOC: 0.6100 Cumulative_SOC_deviation: 4.8831 Fuel Consumption: 72.0440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.260\n",
      "Episode: 122 Exploration P: 0.0447 Total reward: -145.43526222815245 SOC: 0.6183 Cumulative_SOC_deviation: 6.4014 Fuel Consumption: 81.4215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.722\n",
      "Episode: 123 Exploration P: 0.0438 Total reward: -182.1996470316323 SOC: 0.6199 Cumulative_SOC_deviation: 9.8756 Fuel Consumption: 83.4440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.936\n",
      "Episode: 124 Exploration P: 0.0429 Total reward: -137.3480967507187 SOC: 0.6038 Cumulative_SOC_deviation: 7.3394 Fuel Consumption: 63.9537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.630\n",
      "Episode: 125 Exploration P: 0.0420 Total reward: -154.07178255376118 SOC: 0.6066 Cumulative_SOC_deviation: 8.6073 Fuel Consumption: 67.9989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.593\n",
      "Episode: 126 Exploration P: 0.0411 Total reward: -157.83216379040402 SOC: 0.6149 Cumulative_SOC_deviation: 7.9720 Fuel Consumption: 78.1118\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.508\n",
      "Episode: 127 Exploration P: 0.0403 Total reward: -179.28031714533378 SOC: 0.6035 Cumulative_SOC_deviation: 11.4407 Fuel Consumption: 64.8731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.594\n",
      "Episode: 128 Exploration P: 0.0395 Total reward: -122.17132172020555 SOC: 0.6082 Cumulative_SOC_deviation: 5.2334 Fuel Consumption: 69.8376\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.291\n",
      "Episode: 129 Exploration P: 0.0387 Total reward: -120.28148920572319 SOC: 0.6036 Cumulative_SOC_deviation: 5.5041 Fuel Consumption: 65.2408\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.047\n",
      "Episode: 130 Exploration P: 0.0379 Total reward: -177.64355861947146 SOC: 0.6005 Cumulative_SOC_deviation: 11.6448 Fuel Consumption: 61.1956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.199\n",
      "Episode: 131 Exploration P: 0.0371 Total reward: -131.65742222675172 SOC: 0.6049 Cumulative_SOC_deviation: 6.5497 Fuel Consumption: 66.1602\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.120\n",
      "Episode: 132 Exploration P: 0.0364 Total reward: -186.4958981258521 SOC: 0.6006 Cumulative_SOC_deviation: 12.5116 Fuel Consumption: 61.3795\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.995\n",
      "Episode: 133 Exploration P: 0.0357 Total reward: -167.17706618566757 SOC: 0.5973 Cumulative_SOC_deviation: 11.0210 Fuel Consumption: 56.9666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.243\n",
      "Episode: 134 Exploration P: 0.0350 Total reward: -139.03595334194208 SOC: 0.5992 Cumulative_SOC_deviation: 8.0047 Fuel Consumption: 58.9892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.153\n",
      "Episode: 135 Exploration P: 0.0343 Total reward: -175.08766774562034 SOC: 0.6042 Cumulative_SOC_deviation: 10.9663 Fuel Consumption: 65.4247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.111\n",
      "Episode: 136 Exploration P: 0.0336 Total reward: -173.13759316065142 SOC: 0.6064 Cumulative_SOC_deviation: 10.4955 Fuel Consumption: 68.1827\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.147\n",
      "Episode: 137 Exploration P: 0.0330 Total reward: -160.647648451141 SOC: 0.6001 Cumulative_SOC_deviation: 10.0004 Fuel Consumption: 60.6440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.266\n",
      "Episode: 138 Exploration P: 0.0324 Total reward: -163.49292865119145 SOC: 0.6011 Cumulative_SOC_deviation: 10.1930 Fuel Consumption: 61.5634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.102\n",
      "Episode: 139 Exploration P: 0.0318 Total reward: -121.72236198545964 SOC: 0.6012 Cumulative_SOC_deviation: 5.9240 Fuel Consumption: 62.4827\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.284\n",
      "Episode: 140 Exploration P: 0.0312 Total reward: -162.77330258452938 SOC: 0.5991 Cumulative_SOC_deviation: 10.3416 Fuel Consumption: 59.3569\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.178\n",
      "Episode: 141 Exploration P: 0.0306 Total reward: -201.35058260410054 SOC: 0.5909 Cumulative_SOC_deviation: 15.0819 Fuel Consumption: 50.5311\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.320\n",
      "Episode: 142 Exploration P: 0.0301 Total reward: -160.02735649127038 SOC: 0.6008 Cumulative_SOC_deviation: 9.8832 Fuel Consumption: 61.1956\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.120\n",
      "Episode: 143 Exploration P: 0.0295 Total reward: -170.22098278662963 SOC: 0.5968 Cumulative_SOC_deviation: 11.3071 Fuel Consumption: 57.1505\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.210\n",
      "Episode: 144 Exploration P: 0.0290 Total reward: -160.72354928888848 SOC: 0.6015 Cumulative_SOC_deviation: 9.8425 Fuel Consumption: 62.2989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.141\n",
      "Episode: 145 Exploration P: 0.0285 Total reward: -199.72349795277324 SOC: 0.5981 Cumulative_SOC_deviation: 14.1102 Fuel Consumption: 58.6214\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.150\n",
      "Episode: 146 Exploration P: 0.0280 Total reward: -195.57076229131383 SOC: 0.5963 Cumulative_SOC_deviation: 13.9156 Fuel Consumption: 56.4150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.355\n",
      "Episode: 147 Exploration P: 0.0275 Total reward: -205.2628162920401 SOC: 0.5905 Cumulative_SOC_deviation: 15.4916 Fuel Consumption: 50.3472\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.330\n",
      "Episode: 148 Exploration P: 0.0270 Total reward: -211.6755436433234 SOC: 0.5910 Cumulative_SOC_deviation: 16.1328 Fuel Consumption: 50.3472\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.042\n",
      "Episode: 149 Exploration P: 0.0265 Total reward: -134.5981794107749 SOC: 0.5971 Cumulative_SOC_deviation: 7.7632 Fuel Consumption: 56.9666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.075\n",
      "Episode: 150 Exploration P: 0.0261 Total reward: -158.2548918876432 SOC: 0.6041 Cumulative_SOC_deviation: 9.2830 Fuel Consumption: 65.4247\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc1 in position 186: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36msave_v2\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1927\u001b[1;33m         shape_and_slices, tensors)\n\u001b[0m\u001b[0;32m   1928\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ba92dd42db6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DDPG3_trial{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_actor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_critic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     results_dict[trial + 1] = {\n",
      "\u001b[1;32m<ipython-input-12-f443134d2788>\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(actor_model, critic_model, target_actor, target_critic, root)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_actor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_critic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/actor_model_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/critic_model_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtarget_actor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/target_actor_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtarget_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./{}/target_critic_checkpoint\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1088\u001b[0m              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\n\u001b[0;32m   1089\u001b[0m             % (optimizer,))\n\u001b[1;32m-> 1090\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m       \u001b[1;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m       checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[1;32m-> 1155\u001b[1;33m         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\n\u001b[0m\u001b[0;32m   1156\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1102\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m# _SingleDeviceSaver will use the CPU device when necessary, but initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# read operations should be placed on the SaveableObject's device.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0msharded_saves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharded_saves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mtensor_slices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_slices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36msave_v2\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[0;32m   1931\u001b[0m         return save_v2_eager_fallback(\n\u001b[0;32m   1932\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_and_slices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1933\u001b[1;33m             ctx=_ctx)\n\u001b[0m\u001b[0;32m   1934\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1935\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36msave_v2_eager_fallback\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name, ctx)\u001b[0m\n\u001b[0;32m   1968\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"dtypes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m   _result = _execute.execute(b\"SaveV2\", 0, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m-> 1970\u001b[1;33m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m   1971\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc1 in position 186: invalid start byte"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 1\n",
    "reward_factor = 10\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "    \n",
    "    root = \"DDPG3_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG3.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
